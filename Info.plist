Info.plistInfo.plistInfo.plistInfo.plistInfo.plist
Info.plist
Info.plist
Info.plist
Info.plistInfo.plist
Info.plistInfo.plistInfo.plistInfo.plist
Info.plistInfo.plist
Info.plist
Info.plistInfo.plist

Info.plist


Info.plistInfo.plistInfo.plistInfo.plistInfo.plistInfo.plistInfo.plist    Info.plist Info.plist Info.plist 

Info.plist

<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>cachesValidationUUID</key>
	<string>2ECBA4E8-6DF0-47BA-A9B8-B25EA6A572EC</string>
</dict>
</plist>

<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>com_apple_garageband_metadata_appid</key>
	<string>com.apple.mobilegarageband</string>
	<key>com_apple_garageband_metadata_artistName</key>
	<string></string>
	<key>com_apple_garageband_metadata_composer</key>
	<string></string>
	<key>com_apple_garageband_metadata_createdinappid</key>
	<string>com.apple.mobilegarageband</string>
	<key>com_apple_garageband_metadata_hasMasterRTPlugIn</key>
	<false/>
	<key>com_apple_garageband_metadata_hasMasterRTRegions</key>
	<false/>
	<key>com_apple_garageband_metadata_machine</key>
	<string>iPad7,11</string>
	<key>com_apple_garageband_metadata_mediaTrackType</key>
	<string>Undefined</string>
	<key>com_apple_garageband_metadata_numberOfArrangeAlchemyTracks</key>
	<integer>0</integer>
	<key>com_apple_garageband_metadata_numberOfArrangeDrummerTracks</key>
	<integer>0</integer>
	<key>com_apple_garageband_metadata_numberOfArrangeTracks</key>
	<integer>1</integer>
	<key>com_apple_garageband_metadata_numberOfAudioUnitTracks</key>
	<integer>0</integer>
	<key>com_apple_garageband_metadata_numberOfGridColumns</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfGridRows</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfRIClips</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfRIGridRows</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfRITracks</key>
	<integer>1</integer>
	<key>com_apple_garageband_metadata_numberOfSIClips</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfSIGridRows</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfSITracks</key>
	<integer>0</integer>
	<key>com_apple_garageband_metadata_numberOfVisibleArrangeTracks</key>
	<integer>1</integer>
	<key>com_apple_garageband_metadata_songDuration</key>
	<real>234</real>
	<key>com_apple_garageband_metadata_songGender</key>
	<string>major</string>
	<key>com_apple_garageband_metadata_songKey</key>
	<string>D</string>
	<key>com_apple_garageband_metadata_songSignatureDeNominator</key>
	<integer>4</integer>
	<key>com_apple_garageband_metadata_songSignatureNominator</key>
	<integer>4</integer>
	<key>com_apple_garageband_metadata_songTempo</key>
	<integer>120</integer>
	<key>com_apple_garageband_metadata_songTitle</key>
	<string>My Song 2</string>
	<key>com_apple_garageband_metadata_songVersionNumber</key>
	<integer>9</integer>
	<key>com_apple_garageband_metadata_system</key>
	<string>ios</string>
	<key>com_apple_mobileGarageBand_metadata_contentVersionNumber</key>
	<integer>2</integer>
	<key>com_apple_mobilegarageband_metadata_songVersionNumber</key>
	<integer>13</integer>
	<key>kDfMetaDataKey_numberOfRootRegions</key>
	<integer>1</integer>
	<key>plistVersion</key>
	<integer>1050</integer>
</dict>
</plist>

<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>cachesValidationUUID</key>
	<string>2ECBA4E8-6DF0-47BA-A9B8-B25EA6A572EC</string>
</dict>
</plist>
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>com_apple_garageband_metadata_appid</key>
	<string>com.apple.mobilegarageband</string>
	<key>com_apple_garageband_metadata_artistName</key>
	<string></string>
	<key>com_apple_garageband_metadata_composer</key>
	<string></string>
	<key>com_apple_garageband_metadata_createdinappid</key>
	<string>com.apple.mobilegarageband</string>
	<key>com_apple_garageband_metadata_hasMasterRTPlugIn</key>
	<false/>
	<key>com_apple_garageband_metadata_hasMasterRTRegions</key>
	<false/>
	<key>com_apple_garageband_metadata_machine</key>
	<string>iPad7,11</string>
	<key>com_apple_garageband_metadata_mediaTrackType</key>
	<string>Undefined</string>
	<key>com_apple_garageband_metadata_numberOfArrangeAlchemyTracks</key>
	<integer>0</integer>
	<key>com_apple_garageband_metadata_numberOfArrangeDrummerTracks</key>
	<integer>0</integer>
	<key>com_apple_garageband_metadata_numberOfArrangeTracks</key>
	<integer>1</integer>
	<key>com_apple_garageband_metadata_numberOfAudioUnitTracks</key>
	<integer>0</integer>
	<key>com_apple_garageband_metadata_numberOfGridColumns</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfGridRows</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfRIClips</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfRIGridRows</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfRITracks</key>
	<integer>1</integer>
	<key>com_apple_garageband_metadata_numberOfSIClips</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfSIGridRows</key>
	<integer>-1</integer>
	<key>com_apple_garageband_metadata_numberOfSITracks</key>
	<integer>0</integer>
	<key>com_apple_garageband_metadata_numberOfVisibleArrangeTracks</key>
	<integer>1</integer>
	<key>com_apple_garageband_metadata_songDuration</key>
	<real>234</real>
	<key>com_apple_garageband_metadata_songGender</key>
	<string>major</string>
	<key>com_apple_garageband_metadata_songKey</key>
	<string>D</string>
	<key>com_apple_garageband_metadata_songSignatureDeNominator</key>
	<integer>4</integer>
	<key>com_apple_garageband_metadata_songSignatureNominator</key>
	<integer>4</integer>
	<key>com_apple_garageband_metadata_songTempo</key>
	<integer>120</integer>
	<key>com_apple_garageband_metadata_songTitle</key>
	<string>My Song 2</string>
	<key>com_apple_garageband_metadata_songVersionNumber</key>
	<integer>9</integer>
	<key>com_apple_garageband_metadata_system</key>
	<string>ios</string>
	<key>com_apple_mobileGarageBand_metadata_contentVersionNumber</key>
	<integer>2</integer>
	<key>com_apple_mobilegarageband_metadata_songVersionNumber</key>
	<integer>13</integer>
	<key>kDfMetaDataKey_numberOfRootRegions</key>
	<integer>1</integer>
	<key>plistVersion</key>
	<integer>1050</integer>
</dict>
</plist>
PK
ï¿½ï¿½T" MIDI.band/Caches.nosync/_cacheInfoUT
ï¿½ï¿½bï¿½ï¿½bfcuxï¿½ï¿½Uï¿½Qkï¿½0ï¿½ï¿½ï¿½Hï¿½o][ï¿½%Mï¿½IA6a:Ø£$aï¿½Y
&ï¿½ï¿½ï¿½/ï¿½{ï¿½ï¿½ï¿½ï¿½Î¡ï¿½ï¿½kï¿½ï¿½ï¿½dï¿½q8ï¿½(\cï¿½9ï¿½nï¿½8ï¿½ï¿½Iï¿½ï¿½,ï¿½+ï¿½ï¿½ï¿½ï¿½ï¿½Lï¿½Yï¿½ï¿½ï¿½Taï¿½ï¿½kQTï¿½ï¿½kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½8gï¿½ï¿½<ï¿½ï¿½Bï¿½rï¿½.ï¿½ï¿½jï¿½ï¿½Ü­ï¿½2ï¿½ï¿½r
ï¿½ï¿½ï¿½ï¿½ï¿½Uï¿½t,xï¿½ï¿½ï¿½ï¿½d+/Ú¾ï¿½}ï¿½Zç¡¦)ï¿½ï¿½ï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½&ï¿½NOï¿½ï¿½hï¿½ï¿½-ï¿½ï¿½$ï¿½f)ï¿½7Lï¿½$ï¿½v1ï¿½?8ï¿½pï¿½Sï¿½mgï¿½PK ï¿½Jï¿½PK
ï¿½ï¿½Tï¿½ï¿½	 MIDI.band/projectDataUT
ï¿½ï¿½bï¿½ï¿½bfcuxï¿½ï¿½ï¿½ï¿½vï¿½È–(ï¿½\ï¿½+ï¿½İ£ï¿½|jï¿½Ö†;ï¿½ï¿½3"ï¿½$ 	ï¿½ï¿½F46_cFï¿½	pî•«ï¿½Jï¿½ï¿½e4ï¿½ï¿½"ï¿½ï¿½ï¿½{Zï¿½;Î¾ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6Û¸ï¿½ï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½nï¿½tï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ğ·®‰ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½ï¿½ï¿½ï¿½ï¿½+?ï¿½ï¿½ï¿½m+ï¿½ï¿½ï¿½ï¿½3^_ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½Aï¿½cwï¿½ï¿½İ®ï¿½ï¿½ï¿½kï¿½kï¿½ï¿½}ï¿½mï¿½ï¿½ ï¿½ï¿½ï¿½'Ã°ï¿½gï¿½C>ï¿½.ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½jvï¿½ï¿½ï¿½ï¿½ï¿½ãŸ¯ï¿½'ï¿½tï¿½"ï¿½ï¿½Oï¿½ßšï¿½gS~ï¿½m'Ë™{ï¿½ï¿½`ã¯¯1ï¿½?ï¿½ï¿½7ß¾ï¿½ï¿½ï¿½Xlï¿½ï¿½wï¿½ï¿½ï¿½ï¿½]ï¿½ï¿½ï¿½ï¿½3\ï¿½z;Yï¿½3oï¿½5ï¿½fï¿½ï¿½fï¿½wwï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½?ï¿½ï¿½ï¿½L;ï¿½ï¿½7>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½fï¿½ï¿½ï¿½?ï¿½ï¿½|ï¿½~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½tï¿½ï¿½ï¿½ï¿½Ûï¿½Gï¿½y/'ï¿½Û¯ZSï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½8ï¿½dï¿½'ï¿½M)}ï¿½ï¿½&ï¿½Qï¿½ï¿½ï¿½Ô­jØ£ï¿½Qï¿½~4
ï¿½ï¿½.ï¿½ï¿½ï¿½ï¿½
#ï¿½ï¿½ï¿½ï¿½ï¿½)!ï¿½Cï¿½e ï¿½Cï¿½.Ã„ï¿½0B^ï¿½ï¿½8ï¿½î³°ï¿½ï¿½ï¿½848)ï¿½}ï¿½kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½{nï¿½ï¿½Fpflï¿½ï¿½lï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½qï¿½Eï¿½M>ï¿½iï¿½ï¿½X:ï¿½ï¿½ï¿½)w3ï¿½ï¿½ï¿½3;Or{ï¿½Ü†ï¿½ï¿½dhï¿½dï¿½ï¿½ï¿½/2ï¿½s;F?ï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½Qï¿½ï¿½ï¿½Hï¿½ï¿½Oï¿½ ï¿½ï¿½zï¿½ï¿½#8rï¿½+|;N2
ï¿½ï¿½gï¿½ï¿½Ohï¿½ï¿½ï¿½&ï¿½Wï¿½+ï¿½cp5Nï¿½ï¿½ï¿½Iï¿½ï¿½hGï¿½Pï¿½ï¿½ï¿½;}ï¿½nï¿½Ã¯vï¿½ï¿½ï¿½Sï¿½8@b.ï¿½$ï¿½ï¿½yï¿½uï¿½'Ö€-ï¿½ï¿½Xï¿½rvï¿½ï¿½ï¿½Oï¿½=ï¿½ï¿½*{{Cï¿½ï¿½ï¿½ï¿½#ï¿½^ï¿½ï¿½ï¿½ï¿½~ï¿½xï¿½K@ï¿½ï¿½ï¿½.ï¿½ï¿½ï¿½Bï¿½oï¿½ï¿½'Dp*ï¿½Qï¿½yTjï¿½ï¿½cï¿½ï¿½!-uï¿½Û²ï¿½ï¿½V/0#ï¿½hï¿½ï¿½!ï¿½#ï¿½Â…ï¿½ï¿½Z
ï¿½h}ï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½L.<ï¿½ï¿½)Wï¿½ï¿½ï¿½ï¿½ï¿½iï¿½ï¿½=A<'ï¿½ï¿½ï¿½ï¿½
Öœï¿½ï¿½ï¿½ï¿½9ï¿½
ï¿½ï¿½ï¿½cï¿½ï¿½Aï¿½ï¿½w|]ï¿½#ï¿½}ï¿½ï¿½ï¿½ÏŠï¿½Vfï¿½twtï¿½4ï¿½ï¿½ï¿½^ï¿½ï¿½ï¿½tCï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½/ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½WWï¿½Qï¿½ï¿½|jï¿½xï¿½l<%ï¿½aï¿½ï¿½%ï¿½ï¿½-p"5ï¿½ï¿½Fï¿½Nb?Æ­?4Şï¿½/ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½ï¿½ï¿½1wï¿½\ï¿½ï¿½ï¿½
ï¿½ï¿½_/ï¿½ï¿½4mÏ¶ï¿½ï¿½D^1ï¿½X<ï¿½ï¿½ï¿½?0O|Yï¿½Nï¿½l^ï¿½ï¿½ï¿½Sï¿½ï¿½P;1+ï¿½(HLLï¿½CØšlzï¿½ï¿½tï¿½GwFUDï¿½à¦’_ï¿½Hï¿½
Iï¿½oï¿½Mï¿½È£ï¿½ï¿½Wï¿½ï¿½ï¿½;ï¿½q@ï¿½$^nlï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bï¿½A8rz-
&Eï¿½IH*yï¿½/gï¿½ï¿½ï¿½ï¿½ï¿½<ï¿½oï¿½ï¿½Kï¿½ï¿½ï¿½ï¿½ï¿½HLï¿½V(zfï¿½Q~"ï¿½]ï¿½ï¿½F(ï¿½qï¿½GYVï¿½'ï¿½]:]ç«¥Å¹ï¿½NI	LW@ï¿½ï¿½Z\2+uï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½tï¿½kJqz3Lï¿½<ï¿½ï¿½ï¿½TU_uï¿½ï¿½_ï¿½gï¿½ï¿½Siï¿½G}<

ï¿½ï¿½ï¿½``()ï¿½ï¿½dï¿½Zï¿½b|ï¿½=-ï¿½=EYvï¿½4ï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ò²ï¿½ï¿½ï¿½ï¿½Gï¿½Bï¿½ï¿½ï¿½ï¿½5ï¿½{~~Hï¿½Í«Opï¿½ï¿½ï¿½ĞŸï¿½&kqï¿½gHZï¿½ï¿½ï¿½ï¿½ï¿½Kï¿½;ï¿½ï¿½ï¿½pï¿½ï¿½ï¿½Yï¿½{8ï¿½ï¿½{ï¿½ï¿½(ï¿½x8ï¿½pï¿½ï¿½Cã¨ï¿½ï¿½ï¿½\Kï¿½ï¿½ï¿½ï¿½ï¿½Hï¿½'Sï¿½Je5ï¿½ï¿½ï¿½ï¿½f4ï¿½ï¿½E9Y!ï¿½ï¿½ï¿½ï¿½m\ï¿½
ï¿½Ç×¼ï¿½ï¿½KjFï¿½ï¿½x}ï¿½pï¿½ï¿½ï¿½tï¿½Çï¿½ï¿½=yEï¿½|.ï¿½fxï¿½_ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½cÂ‹ï¿½ï¿½q\ï¿½ï¿½ï¿½`tNï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1^ï¿½ï¿½ï¿½ï¿½#ï¿½sï¿½ï¿½Ç©ï¿½ï¿½,ï¿½ï¿½,rï¿½QMï¿½>Iï¿½ï¿½ï¿½|=ï¿½~ï¿½ï¿½ï¿½jï¿½ï¿½!ï¿½ï¿½tï¿½ï¿½ï¿½gIVï¿½Pwï¿½ï¿½ï¿½Mlë‚¤ï¿½q~rQ?+ï¿½/aï¿½ï¿½ï¿½Ù¬ï¿½ï¿½Cï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dCCï¿½ï¿½pJ"g\ï¿½'ï¿½ï¿½Qï¿½=Yï¿½
ï¿½Tï¿½ï¿½Ø®ï¿½Ü„z
[4ï¿½ï¿½VDï¿½ï¿½vï¿½ï¿½ï¿½Ç¶ï¿½ï¿½dï¿½]ï¿½Kï¿½ï¿½/ï¿½ï¿½Aï¿½ï¿½ï¿½ï¿½pì©¯ï¿½ï¿½ï¿½V}wï¿½ï¿½ï¿½EW4ï¿½Iï¿½ï¿½Î¿;dï¿½Aï¿½Ãƒ,-ï¿½
ï¿½ï¿½ï¿½ï¿½]^ï¿½ï¿½ï¿½Gxï¿½ulï¿½ï¿½ï¿½ï¿½ï¿½Jï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(-Ryï¿½,ï¿½â²¼ï¿½ï¿½Ô¸dï¿½ï¿½ï¿½ï¿½kï¿½ï¿½ï¿½#ï¿½Oï¿½7ï¿½ï¿½Gï¿½'QUï¿½ï¿½ï¿½ï¿½Eï¿½:ï¿½Vï¿½ï¿½ï¿½ï¿½ï¿½@ï¿½ï¿½Yï¿½6ï¿½vÙ§ï¿½:ï¿½.ï¿½ï¿½ï¿½`&8ï¿½Q:ï¿½!
ï¿½ï¿½ï¿½,3O)ï¿½ï¿½_	Pï¿½ï¿½ï¿½Wï¿½ï¿½vï¿½ï¿½ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½xï¿½ï¿½ï¿½0ï¿½81ï¿½ï¿½T'ï¿½ï¿½ï¿½:ï¿½ï¿½=!tï¿½ï¿½ï¿½ï¿½$ï¿½	ï¿½ï¿½Oï¿½?9:ï¿½ï¿½eï¿½ï¿½ï¿½O,ï¿½
H|Rrï¿½C8ï¿½ï¿½Úªï¿½ï¿½ï¿½ï¿½ï¿½ï¿½y.ï¿½SIï¿½ï¿½ï¿½ï¿½uï¿½hï¿½ï¿½!Cï¿½ï¿½Wvï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½7.ï¿½pl:n!ï¿½ï¿½vï¿½ï¿½Xï¿½&
ï¿½ï¿½ï¿½xk+ï¿½"ï¿½ï¿½ï¿½=ï¿½ï¿½ï¿½Q^-ï¿½Jï¿½ï¿½ï¿½/×²ï¿½ï¿½ÕµD~%k>ï¿½8Yï¿½1ï¿½ï¿½?ï¿½_ï¿½=ï¿½rï¿½ï¿½dï¿½Ke~ï¿½rï¿½6<Bï¿½ï¿½=ï¿½ï¿½sï¿½Rï¿½Â‰ï¿½ï¿½_ï¿½-ï¿½[Vï¿½ï¿½'Õ¬ï¿½ï¿½ï¿½=Wï¿½ï¿½ï¿½ï¿½ï¿½WNÜ´ï¿½ï¿½ï¿½ï¿½!ï¿½ï¿½ï¿½ï¿½ï¿½}ï¿½-ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½à¸«ï¿½Uï¿½ï¿½Oï¿½Ğ¢7ï¿½ï¿½ï¿½^ï¿½ï¿½Mï¿½Y:^Vï¿½Ò‘rtï¿½8ï¿½ï¿½ï¿½ï¿½<ï¿½umï¿½<!xPï¿½oï¿½ï¿½Kï¿½ï¿½ï¿½uï¿½ï¿½0Cï¿½ï¿½Zï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½Pï¿½vï¿½?ï¿½yï¿½ï¿½ï¿½-ï¿½ï¿½Gï¿½ï¿½ï¿½7(\>ï¿½ï¿½{gHtï¿½ï¿½Ö­Oï¿½ï¿½ï¿½1"%ï¿½!W:ï¿½ï¿½}"ï¿½ï¿½ï¿½Pï¿½ï¿½ï¿½3ï¿½qï¿½ï¿½hï¿½ï¿½Hï¿½rï¿½ï¿½>ï¿½ï¿½ï¿½!>*ï¿½^'ï¿½ï¿½q"ï¿½ï¿½&ï¿½dvï¿½1ï¿½ï¿½Vï¿½ï¿½ï¿½?ï¿½×´ï¿½ï¿½`rÆƒï¿½M&ï¿½Sï¿½P9ï¿½Nï¿½ï¿½x98ï¿½ï¿½ï¿½ï¿½[}ï¿½\{Vï¿½|
ï¿½Wï¿½ï¿½ï¿½RİŸï¿½	ï¿½lï¿½ï¿½ï¿½CØ›O$9ï¿½nbTï¿½ï¿½ï¿½ï¿½ï¿½9W8ï¿½^ï¿½O×¦gVï¿½ï¿½ï¿½nï¿½ï¿½ ï¿½!ÔŸq"E2[8ï¿½ï¿½ï¿½~ï¿½ï¿½ï¿½ï¿½Uï¿½b{Jï¿½ÔSï¿½ï¿½ï¿½ï¿½&ï¿½Rï¿½ï¿½ï¿½ï¿½ï¿½4ï¿½Wï¿½ï¿½Lï¿½=gï¿½}&Gï¿½ï¿½>\ï¿½8ï¿½ï¿½%Zï¿½ï¿½i;ï¿½k[Ü«Z;ï¿½;gEï¿½ï¿½ï¿½6ï¿½Wyï¿½x}9ï¿½ï¿½1^<6ï¿½ï¿½ï¿½i1-ï¿½	í‰¯/ï¿½kï¿½#oï¿½ï¿½>ï¿½ï¿½{TÕÖ½LO$ï¿½ï¿½6&ï¿½ï¿½ï¿½ï¿½U\ï¿½/_ï¿½ï¿½Sï¿½/9Cï¿½ï¿½ï¿½y\ï¿½cï¿½ï¿½ï¿½guoï¿½ï¿½ï¿½]ï¿½QDï¿½ï¿½ï¿½Kï¿½İ‹ï¿½ï¿½ï¿½
ï¿½ï¿½dï¿½ï¿½ï¿½ï¿½/Cï¿½]ï¿½Fï¿½ï¿½{Vï¿½ï¿½ï¿½É4ï¿½ï¿½R\ï¿½OeGï¿½Qï¿½ï¿½ÈRï¿½ï¿½ï¿½eï¿½<ï¿½ï¿½$.ï¿½#^ï¿½<Z[ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½Tï¿½ï¿½Eï¿½Úºï¿½ï¿½WFï¿½qï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Lï¿½yï¿½ï¿½ï¿½qq_ï¿½ï¿½ÉŒï¿½ï¿½?ï¿½wï¿½ï¿½ï¿½ï¿½ï¿½D\ï¿½ï¿½/ï¿½Xï¿½ï¿½Mï¿½ï¿½Jï¿½ï¿½?ï¿½Mï¿½9:bï¿½ï¿½;ï¿½ï¿½gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?Kï¿½ï¿½ï¿½2ï¿½Wï¿½ï¿½}ï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½;ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(ï¿½ï¿½.ï¿½mY~>ï¿½ï¿½ï¿½+lï¿½Dï¿½.ï¿½ï¿½~ï¿½?ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½Dï¿½Sï¿½[&ï¿½	ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½ï¿½ï¿½sï¿½ï¿½ï¿½y?ï¿½ï¿½ï¿½s9ï¿½t<ï¿½ï¿½Ã•ï¿½ï¿½X2ï¿½ï¿½ï¿½ï¿½~ï¿½ï¿½ï¿½os(ï¿½ï¿½ï¿½xï¿½ï¿½_ï¿½ï¿½ï¿½ï¿½ï¿½İŸwï¿½ï¿½ï¿½ï¿½ï¿½h<ï¿½ï¿½;ï¿½ï¿½]ï¿½%gï¿½)Zï¿½ï¿½xLï¿½ï¿½+ï¿½ï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½ï¿½ï¿½ï¿½gï¿½'zï¿½ï¿½>+ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½>^)ï¿½ï¿½ï¿½A*ï¿½
t*~Hï¿½ï¿½Şš3ï¿½ï¿½ï¿½ï¿½C!ï¿½ï¿½9ï¿½ï¿½ï¿½ï¿½1ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½y>ï¿½ï¿½ï¿½uï¿½Y;vï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½bï¿½sï¿½p|4ï¿½ï¿½ï¿½}nï¿½ï¿½ï¿½ï¿½Yï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½}ï¿½gxï¿½ï¿½gï¿½[>ï¿½{ï¿½ï¿½ï¿½Ó¼ï¿½_rï¿½+ï¿½ï¿½Oï¿½KWï¿½ï¿½ï¿½ï¿½ï¿½y"ï¿½ï¿½ï¿½ï¿½	>ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½YQŞ§ï¿½ï¿½Iï¿½?ï¿½ï¿½Y4Ş“ï¿½ï¿½ï¿½3ï¿½wAO]i{ï¿½Sï¿½'#ï¿½×‚?\ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½}ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½>ï¿½Iï¿½Iï¿½ï¿½Tï¿½~ï¿½ 3ï¿½-ï¿½ï¿½dï¿½dï¿½bGpï¿½ ï¿½ï¿½dSï¿½ßrï¿½ï¿½Dï¿½1ï¿½Jï¿½ï¿½2ï¿½lï¿½ï¿½@ï¿½ï¿½z&rï¿½(@ï¿½.ï¿½|ï¿½yBï¿½7)'~ï¿½Ï†ï¿½a	rï¿½{nï¿½ï¿½ï¿½25Tï¿½6Æ¶ï¿½=	p}"%ï¿½ï¿½ï¿½xRDï¿½sï¿½ï¿½qï¿½ï¿½Lï¿½\eï¿½Hï¿½Ç…pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Dpï¿½Å·Ø“ï¿½{ï¿½ï¿½ï¿½ï¿½0ï¿½ï¿½m4ï¿½?;ï¿½mï¿½N}K?ï¿½ï¿½ï¿½ï¿½PD1ï¿½ï¿½ï¿½iJï¿½|ï¿½nxZï¿½ï¿½+ï¿½ï¿½ï¿½ï¿½Nï¿½ï¿½Wï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½udï¿½
 ï¿½Gjï¿½ï¿½ï¿½ï¿½ï¿½ï¿½kHï¿½/o1L+ï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½np$8h]ï¿½ï¿½pï¿½_ï¿½ï¿½ï¿½Ifvxï¿½K{RJï¿½.ï¿½ï¿½xYï¿½~ftï¿½ï¿½ï¿½ï¿½
`ï¿½!ï¿½
ï¿½ ï¿½ï¿½#ï¿½ï¿½]ï¿½(ï¿½ï¿½ï¿½ï¿½ï¿½_ï¿½Òºï¿½*ï¿½Kï¿½ï¿½ï¿½KGï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½wÓ§ï¿½ï¿½^;ï¿½ï¿½8ï¿½?ï¿½ï¿½ï¿½ï¿½wZï¿½ï¿½ï¿½eï¿½itï¿½ï¿½bï¿½[ï¿½3ï¿½ï¿½ï¿½'ï¿½S$ï¿½ï¿½nï¿½odyï¿½ï¿½ï¿½ï¿½Zï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½@È£ï¿½oUï¿½ZVï¿½ï¿½ï¿½ï¿½qï¿½	ï¿½{ï¿½ï¿½ï¿½}ï¿½ï¿½Oï¿½>ï¿½wï¿½ï¿½|ï¿½([3~kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½nï¿½ï¿½Q|ï¿½ï¿½/okÉ„oï¿½ï¿½z:ï¿½,ï¿½oï¿½#!aÌ·ï¿½yË»ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ç«–ï¿½Õ›ï¿½=ï¿½n\Óï¿½ro4ï¿½/ï¿½7ï¿½ï¿½ï¿½Ïï¿½wï¿½ï¿½ï¿½ï¿½ï¿½Ü§qï¿½ï¿½ï¿½2ï¿½ï¿½2ï¿½ï¿½ï¿½ï¿½ï¿½Ñ›wï¿½pï¿½ï¿½ï¿½-ï¿½İ³ï¿½ï¿½;:ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½%ï¿½+ï¿½ï¿½ï¿½k:$ï¿½ï¿½rOps5Gï¿½ï¿½ï¿½ï¿½ï¿½97ï¿½ï¿½ï¿½=ï¿½GLï¿½ï¿½ï¿½[ï¿½gya&ï¿½wï¿½|Kï¿½Öï¿½ï¿½}ï¿½ï¿½,ï¿½Sx_ï¿½ï¿½u.\ï¿½ï¿½]bï¿½
qï¿½ï¿½ï¿½5ï¿½ï¿½ï¿½ï¿½Dï¿½>ï¿½_ï¿½F@ï¿½=	(ï¿½s]Lï¿½
wï¿½ï¿½;Jï¿½ï¿½ï¿½Jï¿½+ï¿½ï¿½ï¿½ï¿½tï¿½]=ï¿½ï¿½}&Wæ“£ï¿½ï¿½'ï¿½ï¿½tÆï¿½_ï¿½ï¿½ï¿½ï¿½3ï¿½ß—@ï¿½ï¿½ï¿½Ï‰3}ï¿½Î»ï¿½D!XJï¿½nB$9ï¿½ï¿½ï¿½ï¿½=ï¿½[!ï¿½ï¿½İ{Jï¿½ï¿½ï¿½/K7ï¿½_s-/ï¿½ï¿½ï¿½yp)ï¿½\ï¿½21ï¿½ï¿½x4ï¿½[ï¿½7ï¿½ï¿½ï¿½/21ï¿½ï¿½0ï¿½ï¿½8/{%1^4uï¿½sï¿½qï¿½<|rï¿½$ï¿½ï¿½ï¿½x_IB3}ï¿½Hï¿½ï¿½;ï¿½lu?ï¿½ï¿½kï¿½>ï¿½ï¿½Hxï¿½ï¿½;Yï¿½@>ï¿½ aï¿½ï¿½â•›|Sï¿½ï¿½ï¿½Pï¿½ZXï¿½Zï¿½ï¿½?ï¿½ï¿½nï¿½ï¿½Î¹Rï¿½Dï¿½ï¿½ï¿½JVIï¿½ï¿½oBï¿½ï¿½ï¿½Zï¿½}Hï¿½ï¿½Kï¿½Gï¿½iï¿½Pï¿½^Oï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½8]ï¿½Gï¿½ï¿½c,ï¿½@ï¿½ï¿½cï¿½ï¿½<ï¿½×…_~ï¿½cï¿½ï¿½ï¿½ï¿½yï¿½ï¿½ï¿½oï¿½]+Ô—ï¿½gï¿½NXï¿½4aï¿½Wï¿½cï¿½ï¿½?ï¿½wï¿½ï¿½yOï¿½\MRs#Şºï¿½ï¿½2ï¿½ï¿½ßŒï¿½Bï¿½ï¿½@ï¿½sCï¿½,ï¿½ï¿½ï¿½Bï¿½Lï¿½"d}ï¿½ï¿½}a"ï¿½,?Oï¿½9ï¿½,
ï¿½)iï¿½ï¿½m2ï¿½yBï¿½/'ï¿½Ó‘ï¿½ï¿½ï¿½JFï¿½×©ï¿½4ï¿½ï¿½H
Tï¿½ï¿½ï¿½54C?ï¿½İ„u$kï¿½ï¿½Wï¿½İŒ×‘ï¿½cx}ï¿½1ï¿½Pï¿½Mï¿½ï¿½7ï¿½3Ryï¿½0ï¿½ï¿½@Gï¿½ï¿½Ò¤Rï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½dï¿½Pï¿½qï¿½>ï¿½ï¿½ï¿½
ï¿½]ï¿½ï¿½'Uï¿½3+bnï¿½ï¿½Åï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Xï¿½34aï¿½Ä¯ï¿½ï¿½ï¿½ï¿½bï¿½ï¿½wï¿½ï¿½ï¿½ï¿½Vraï¿½Nï¿½Ş‘ï¿½ï¿½<ï¿½Rï¿½ ï¿½Rï¿½ï¿½5ZÃï¿½ï¿½ï¿½[\ï¿½|\ï¿½YvEï¿½ï¿½ï¿½ï¿½c0ï¿½ï¿½3ï¿½5ï¿½:rï¿½k#Q?ï¿½ï¿½Jï¿½ï¿½nwï¿½"ï¿½{Ò–ï¿½+ï¿½+6ï¿½8ï¿½ï¿½ï¿½\%ï¿½>kï¿½'ï¿½p'ï¿½ï¿½ï¿½ï¿½ï¿½;ï¿½0ï¿½"ï¿½ï¿½ï¿½9B7ï¿½_Ùp<4Nï¿½76ï¿½ï¿½ï¿½ï¿½Zï¿½'ï¿½ï¿½?ï¿½>\gï¿½Û©Xï¿½"ï¿½ï¿½ï¿½ï¿½-ï¿½ï¿½Gï¿½ï¿½ï¿½oÑŠWo%ï¿½ï¿½ï¿½Vdpï¿½]Trï¿½ï¿½ï¿½ï¿½yï¿½ï¿½Bï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½24ï¿½zï¿½vï¿½ï¿½/ï¿½ï¿½'0ï¿½4	ï¿½Ffï¿½ï¿½cß±ï¿½ï¿½ï¿½/iï¿½ï¿½ï¿½ï¿½1}eï¿½Õ½7ï¿½zzï¿½ï¿½ï¿½ï¿½<?>ï¿½#ï¿½1ï¿½ï¿½#ï¿½#ï¿½G|2ï¿½Y{yï¿½ï¿½
e<ï¿½ï¿½~ï¿½ï¿½}qï¿½Nï¿½|'ï¿½8ï¿½>ï¿½ï¿½ï¿½?,ï¿½Yï¿½ï¿½x=ï¿½[j;R?_ï¿½;vï¿½sG8ï¿½ï¿½\y7Xï¿½ï¿½ï¿½xLyï¿½A^ï¿½ï¿½[ï¿½ï¿½|ï¿½@~}}ï¿½_ï¿½ï¿½@7]xï¿½ï¿½ï¿½gï¿½s
â€ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ ï¿½Üï¿½ï¿½mï¿½ï¿½ñ°‡£ï¿½ï¿½Bï¿½ï¿½p8h]ï¿½eï¿½ï¿½Yï¿½ï¿½Rï¿½gï¿½.ï¿½t_]ï¿½_ï¿½, ï¿½fï¿½ï¿½ï¿½ï¿½ï¿½[ï¿½AJ$Z'ï¿½)By7ï¿½ï¿½Hï¿½S:W$C
â€%#ï¿½ï¿½ï¿½ï¿½ß¨ï¿½ï¿½ï¿½qcï¿½ï¿½ï¿½ï¿½:pï¿½Zï¿½cxï¿½.qXï¿½ï¿½ï¿½tï¿½ï¿½È¶Jï¿½cï¿½"ï¿½|ï¿½6ï¿½ï¿½0ï¿½ï¿½ï¿½ï¿½]ï¿½ï¿½>ï¿½!ï¿½gï¿½ï¿½ï¿½ï¿½Ø¥ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½NR{ï¿½ï¿½ï¿½gMHï¿½]ï¿½ï¿½ï¿½ï¿½bzbï¿½ï¿½ï¿½>Ô…=	?ï¿½ï¿½Cï¿½ï¿½ï¿½yfï¿½ï¿½Ù¢ï¿½Qï¿½ï¿½ï¿½t=ï¿½á¼¢ï¿½ï¿½Bï¿½ï¿½	ï¿½ï¿½ï¿½zï¿½;ï¿½ï¿½ï¿½=bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½gï¿½ï¿½ï¿½pï¿½ï¿½Tï¿½Ï‘Mï¿½vï¿½ï¿½ï¿½
ï¿½É³iï¿½ï¿½yï¿½ï¿½3xï¿½ï¿½4>ï¿½ï¿½"â£»ï¿½ï¿½ï¿½ï¿½ï¿½aOï¿½ï¿½9ï¿½Ibï¿½iï¿½ï¿½6ï¿½ï¿½oï¿½TEï¿½ï¿½~QØ›ï¿½fkn#ï¿½ï¿½ï¿½ï¿½ï¿½×·ë—©ï¿½|ï¿½?ï¿½:ï¿½ï¿½hï¿½ï¿½ï¿½3ï¿½ï¿½}×ƒsï¿½ï¿½ï¿½qï¿½ï¿½ï¿½ï¿½Eï¿½
7z'ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½î‰¡aï¿½yï¿½ï¿½Wï¿½qï¿½ï¿½ï¿½ï¿½"mï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½`
ï¿½Å•ï¿½ï¿½ï¿½pï¿½qfï¿½Jï¿½ï¿½Xçª±<ï¿½ï¿½!ï¿½u{ï¿½T:ï¿½ï¿½ï¿½f]J$7ï¿½,${vï¿½{ï¿½ï¿½Åï¿½yï¿½r5^<ï¿½ï¿½Å¥dï¿½Wï¿½ï¿½s?9c9/ï¿½ï¿½-ï¿½ï¿½ï¿½(ï¿½\yï¿½ï¿½:\!ï¿½7ï¿½7ï¿½ï¿½Q-ï¿½ï¿½)Ğ—ï¿½ï¿½ï¿½ï¿½oï¿½nï¿½ï¿½ï¿½ï¿½ï¿½ÌŸO?_ï¿½=ï¿½[ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½ï¿½^ï¿½#=ï¿½)ï¿½>pï¿½ï¿½8ï¿½_ï¿½ï¿½7ï¿½ï¿½ï¿½ÑHï¿½Éœï¿½nï¿½ï¿½wï¿½Ì…ï¿½"ï¿½cï¿½8Aï¿½{ï¿½iï¿½ï¿½ï¿½0'ï¿½ï¿½ï¿½<ojï¿½
bï¿½kOï¿½7ï¿½pï¿½ï¿½v)/^aï¿½x
ï¿½'o>ï¿½[Gs3ï¿½\Wï¿½8ï¿½fï¿½ï¿½ï¿½sï¿½ï¿½ï¿½uï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"ï¿½ï¿½8ï¿½]ï¿½ï¿½qï¿½cï¿½YRï¿½ï¿½4l2ï¿½<ï¿½ï¿½zcrï¿½ï¿½ï¿½ï¿½ï¿½\8ï¿½ï¿½ï¿½{0Yï¿½ï¿½lï¿½+cs>Oï¿½uï¿½æ¤’ï¿½]0ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½ï¿½fï¿½ï¿½qï¿½Z|Ï¤:]ï¿½ï¿½mvY'ï¿½Oï¿½ï¿½ï¿½ï¿½ï¿½Pï¿½ï¿½ï¿½ï¿½3ï¿½mï¿½3ï¿½gï¿½ï¿½Mï¿½ï¿½aï¿½o
ï¿½Cï¿½ï¿½é¥Ÿï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?`}aï¿½%ï¿½`G6ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿½^3ï¿½s""ï¿½1Ë·"ï¿½G
ï¿½ï¿½ï¿½ï¿½6ï¿½DÓ¹Nï¿½ï¿½Iï¿½[ï¿½ï¿½ï¿½ï¿½ï¿½fï¿½ŞÇ‡ï¿½ï¿½Kï¿½7XR
ï¿½ï¿½jï¿½ï¿½Sï¿½8,ï¿½×œ:cï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½>ï¿½,ï¿½ï¿½Tï¿½#ï¿½pï¿½ï¿½),ï¿½ï¿½{ï¿½'ï¿½ï¿½ï¿½ï¿½6+ï¿½ï¿½Qï¿½ï¿½ï¿½~ï¿½zï¿½ï¿½ï¿½ï¿½xï¿½Ì“ï¿½ï¿½ï¿½ï¿½<Bï¿½$sï¿½Wï¿½^Fï¿½ï¿½ï¿½ï¿½Lï¿½arï¿½Ìï¿½3gï¿½ï¿½vï¿½3Xï¿½g\ï¿½gï¿½sï¿½ï¿½{Fï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½i'~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dï¿½eï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½9iï¿½ï¿½ï¿½ï¿½ï¿½ï¿½g\ï¿½ï¿½ï¿½ï¿½rJï¿½q&Ï¸ï¿½NGï¿½rï¿½ï¿½6ï¿½9KMX'/ï¿½ï¿½+Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"ï¿½ï¿½ï¿½Ï¯ï¿½Rp	H8ï¿½ï¿½^Dï¿½ï¿½ï¿½ï¿½ï¿½mL
â€×—|ï¿½ï¿½ï¿½ï¿½oï¿½Aï¿½_q`}ß£ï¿½xï¿½R>
ï¿½gï¿½n
ï¿½5/\pï¿½ï¿½Úˆï¿½zpï¿½ï¿½1ï¿½Zï¿½`ï¿½Zï¿½ï¿½hï¿½zï¿½ï¿½zï¿½7=ï¿½Ö·G?ï¿½e|ï¿½ï¿½ï¿½	,ï¿½É¼ï¿½ï¿½ï¿½yï¿½ï¿½5?K×—ï¿½ï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½=Ø”=ï¿½ï¿½ï¿½{6ï¿½ï¿½>Kï¿½@Oï¿½Vï¿½ï¿½ï¿½Iï¿½Jï¿½ï¿½Oï¿½2ï¿½ï¿½yï¿½ï¿½ï¿½_ï¿½ï¿½Sypï¿½6LNï¿½i:t3ï¿½Kï¿½ï¿½rÓ¶JNÛ§ï¿½^fï¿½ï¿½k{$ï¿½ï¿½Pï¿½ï¿½Zï¿½ï¿½ï¿½cEï¿½ï¿½ï¿½ï¿½p)ï¿½ï¿½ï¿½@>e[ï¿½ï¿½Mï¿½ï¿½ï¿½ï¿½ï¿½!Bh7xï¿½ï¿½a/ï¿½'ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ØƒVï¿½ï¿½ï¿½@>ï¿½"ï¿½ï¿½ï¿½=ï¿½_ï¿½ï¿½X=n44<zï¿½Mï¿½2ï¿½ï¿½ï¿½<?sï¿½ï¿½cï¿½\Rï¿½ï¿½ï¿½PÛ¡c{ï¿½mï¿½ï¿½Kï¿½_ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½`ï¿½ï¿½fï¿½ï¿½ï¿½ï¿½ï¿½ï¿½iï¿½>ï¿½_ï¿½wï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½;ï¿½ï¿½2Nï¿½ï¿½ï¿½ï¿½5OVzï¿½ sï¿½ï¿½	\~ï¿½ï¿½Ó’3ï¿½i+rï¿½xï¿½ï¿½oUzï¿½ï¿½ï¿½gï¿½#Nï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½ï¿½ï¿½'ï¿½`Ï±3ï¿½ï¿½ï¿½7ï¿½OKï¿½Mï¿½m	ï¿½&rï¿½ï¿½uï¿½ï¿½ï¿½ï¿½nï¿½ï¿½J^Kï¿½2ï¿½ï¿½e?{ï¿½ï¿½ï¿½xOï¿½ï¿½ï¿½ï¿½!Lï¿½ï¿½ï¿½ï¿½ï¿½Y_Vï¿½ï¿½kFï¿½ï¿½ï¿½ï¿½qUï¿½Iï¿½ï¿½Kï¿½ï¿½4Gï¿½(ï¿½{ï¿½ï¿½,Ø€ï¿½a<ï¿½{;ï¿½Sï¿½ï¿½ï¿½ï¿½ï¿½ï¿½&|ï¿½ï¿½ï¿½Rï¿½ï¿½Fï¿½-ï¿½zï¿½ï¿½ï¿½ï¿½9ï¿½#ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½x&ï¿½Î¨ZPzï¿½ï¿½ï¿½fï¿½aXï¿½j'ï¿½ï¿½ï¿½ï¿½tØ¨ï¿½*oï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½$yÎ§Ç£ï¿½ï¿½ï¿½t<Dï¿½ï¿½EQgï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½sï¿½vï¿½ï¿½mï¿½ï¿½_ï¿½ï¿½Ò—ï¿½ï¿½ï¿½Oï¿½=ï¿½ï¿½×¯ï¿½F{Zhï¿½oï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½B\Yï¿½ï¿½Aï¿½9ï¿½GÅ›è†ï¿½ï¿½ï¿½_ï¿½ï¿½ï¿½Ü“\sï¿½3ï¿½ï¿½ï¿½Uï¿½ï¿½Pï¿½xï¿½/ï¿½=ï¿½ï¿½%ï¿½\ï¿½ZFï¿½3ï¿½ï¿½%ï¿½~ï¿½ï¿½3Mï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½5ï¿½dï¿½:ï¿½ï¿½Tï¿½ï¿½ï¿½[ï¿½ï¿½qï¿½+ï¿½@ï¿½<yï¿½ï¿½ï¿½kï¿½ï¿½ï¿½7i[ï¿½ï¿½ï¿½ï¿½ï¿½3yï¿½,ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½5Xï¿½ï¿½ï¿½ï¿½~ï¿½kï¿½ï¿½ï¿½ï¿½Ê£ï¿½ï¿½ï¿½\=ï¿½ï¿½O&E_ï¿½ï¿½ï¿½hLï¿½ï¿½Õ§(ï¿½zDï¿½ï¿½Jï¿½{ï¿½É§ï¿½ï¿½ï¿½ï¿½G4Î£/ï¿½ï¿½hï¿½ï¿½ï¿½ï¿½ï¿½Ñ•ï¿½/ï¿½ï¿½g/Fï¿½ï¿½_Jï¿½Xï¿½ï¿½Tï¿½sQï¿½ï¿½Ş£ï¿½ï¿½Eï¿½Kï¿½ï¿½kï¿½Sï¿½0ï¿½~\Hï¿½<ï¿½Ú£ï¿½Æ©ï¿½oBï¿½P/ï¿½ï¿½ï¿½R$ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½#ï¿½ï¿½'ï¿½ï¿½xï¿½ï¿½ï¿½ï¿½kï¿½ï¿½ï¿½Bï¿½"_ï¿½7>ï¿½ó¡¦Oï¿½ï¿½7ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Æ©|ï¿½}Nï¿½ï¿½3zï¿½ï¿½ï¿½Ì¡qï¿½ï¿½[ï¿½ï¿½Oï¿½ï¿½ï¿½<ï¿½hï¿½ï¿½Wï¿½
ï¿½ï¿½Ó·ï¿½|ï¿½ï¿½.ï¿½1ï¿½ï¿½ï¿½pÖŸï¿½ï¿½ï¿½3ZCï¿½ï¿½ï¿½ï¿½ï¿½ï¿½p?ï¿½ï¿½}ï¿½yï¿½É½ï¿½ï¿½p
ï¿½Oï¿½éˆ¾ï¿½hlï¿½szFï¿½ï¿½zğ±ï¿½ï¿½1_ï¿½ï¿½|ï¿½ï¿½4Oï¿½=]7ï¿½:ï¿½qï¿½gé–¾4~ï¿½Yï¿½#ï¿½ï¿½ï¿½ï¿½tï¿½?)ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½È·Gy2ï¿½ï¿½ï¿½Iï¿½ï¿½ï¿½ï¿½Uï¿½ï¿½\Nï¿½ï¿½Dï¿½ï¿½hL3hï¿½2Ú‡ï¿½ï¿½tD_ï¿½<ï¿½T"ï¿½9ï¿½ï¿½ï¿½iï¿½ï¿½Kï¿½qï¿½gï¿½Xï¿½cï¿½ï¿½ï¿½ï¿½Êoï¿½xNï¿½Û§ï¿½ttï¿½'ï¿½ï¿½Ş‘ï¿½$?Aï¿½ï¿½ï¿½ï¿½d~ï¿½ï¿½ï¿½ï¿½Oï¿½Oï¿½È°ï¿½ï¿½>ï¿½kï¿½ï¿½ï¿½ï¿½ZŞµï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Eï¿½GOï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½#_+ï¿½Kå ’ï¿½ï¿½eï¿½Z>+ï¿½i|ï¿½ï¿½8ï¿½6ï¿½ï¿½7èŒ¶Vvï¿½y4ï¿½]ï¿½}Sp?ï¿½nï¿½Cï¿½ï¿½ï¿½ï¿½\Lcï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'11ï¿½'Hï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bï¿½ï¿½>9uï¿½ï¿½cï¿½ï¿½ï¿½Ó¾Vï¿½hï¿½zï¿½ï¿½ï¿½ï¿½ï¿½@_6^ï¿½ï¿½Oï¿½ï¿½1ï¿½~Gï¿½Kï¿½ï¿½qï¿½0Kï¿½î§¾Vvï¿½ï¿½z:ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½k1ï¿½ï¿½3ï¿½\ï¿½ï¿½ï¿½Iï¿½ÛŸï¿½Zï¿½ï¿½'ï¿½ï¿½ï¿½ï¿½hLï¿½ï¿½ï¿½o*~ï¿½ï¿½ï¿½Eï¿½/ï¿½cï¿½>ï¿½ï¿½hï¿½ï¿½ï¿½Z4}Kï¿½	ï¿½e8ï¿½/?ï¿½Bï¿½%ï¿½ï¿½A<ï¿½<ï¿½^Û¿ge8É¿ï¿½ï¿½ï¿½ï¿½ï¿½p1ï¿½i
ï¿½)Nï¿½Uï¿½ï¿½tÒ³ï¿½}Q>ï¿½aL,ï¿½ï¿½ï¿½C_Kï¿½ï¿½ï¿½ï¿½Vï¿½ï¿½ï¿½Xï¿½ï¿½Jñœ•ï¿½dï¿½?ï¿½"8ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Srï¿½ï¿½ï¿½ï¿½Cï¿½ï¿½Eï¿½ï¿½ï¿½ï¿½ï¿½ï¿½UHï¿½ï¿½ï¿½7ï¿½ï¿½+;\\ï¿½}ï¿½ï¿½ï¿½ï¿½@ï¿½=ï¿½lqï¿½~aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½xNï¿½ï¿½ï¿½@ï¿½ï¿½Lï¿½ï¿½ï¿½ï¿½Wï¿½ï¿½w,ï¿½ï¿½ï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½oï¿½?Qï¿½ï¿½HOï¿½tMï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½ï¿½Tï¿½ï¿½uï¿½?ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½cï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½/ï¿½?ï¿½hï¿½ï¿½ï¿½Xï¿½ï¿½ï¿½ï¿½ï¿½ï¿½311;ï¿½ï¿½Ï¨?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ yï¿½oï¿½?ï¿½ï¿½5ï¿½ï¿½ï¿½@ï¿½hï¿½ï¿½ï¿½mï¿½8ï¿½Ç¿X8'{e~{ï¿½!ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 7ï¿½ï¿½suï¿½hOï¿½ï¿½ï¿½yï¿½ï¿½Xï¿½bï¿½ï¿½ï¿½7ï¿½ï¿½ï¿½_Rï¿½ï¿½ï¿½Ù±xFï¿½ï¿½ï¿½L^ï¿½Oï¿½?ï¿½É½ï¿½ï¿½ï¿½ï¿½ï¿½uï¿½ï¿½Qsï¿½pnï¿½×‰ï¿½ï¿½ï¿½ï¿½Yï¿½ï¿½ï¿½ï¿½ï¿½É½ï¿½ï¿½ï¿½ï¿½×‰ï¿½ï¿½Uï¿½ï¿½6<<ï¿½á¸¾ï¿½ï¿½ï¿½ç¢¼ï¿½oï¿½?ï¿½ï¿½Mï¿½v`ï¿½oï¿½?Hï¿½~Xï¿½ï¿½:q4ï¿½_P8ï¿½$ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½ï¿½uï¿½ï¿½ï¿½Sï¿½ï¿½;ï¿½ï¿½?ï¿½ï¿½uï¿½?P8ï¿½ï¿½Zï¿½ï¿½ï¿½0ï¿½ï¿½ï¿½oï¿½?ï¿½ï¿½ï¿½Ä¿ï¿½ï¿½ =ï¿½ï¿½æŸ²ï¿½ï¿½Q^ï¿½Oï¿½?Lï¿½'ï¿½ï¿½ï¿½Nï¿½Ä¿ï¿½ï¿½@ï¿½Òï¿½?ï¿½ï¿½:q8ï¿½?Zï¿½ï¿½ï¿½;ï¿½ï¿½ï¿½	ï¿½ï¿½ï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½_'ï¿½ï¿½eï¿½A~'ï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½?\ï¿½ï¿½Zï¿½ï¿½ï¿½!ï¿½ï¿½ï¿½ï¿½Fï¿½;ï¿½rï¿½wï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½gï¿½ZBï¿½aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½uï¿½?ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½:ï¿½ï¿½?\ï¿½ï¿½ï¿½Qï¿½ï¿½#ï¿½ï¿½wï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½@gï¿½Hï¿½ï¿½7Fï¿½ï¿½r/ï¿½ï¿½'ï¿½ï¿½ä‰¿ï¿½ï¿½ï¿½ï¿½ï¿½[ï¿½Jï¿½ß­?pï¿½uï¿½hï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½yrï¿½oï¿½?pï¿½ubï¿½'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ñ±rh,ï¿½ï¿½ï¿½pï¿½ï¿½kï¿½ï¿½ï¿½$r_1[X0~/bï¿½ï¿½ï¿½7ï¿½8ï¿½ï¿½Aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dï¿½Lï¿½qï¿½Wqï¿½ï¿½ï¿½ï¿½ï¿½N6ï¿½<ï¿½ï¿½gpï¿½ï¿½ï¿½ï¿½Mï¿½Oï¿½3ï¿½ï¿½ï¿½?@*2Cß‚ï¿½ï¿½ï¿½ï¿½ï¿½zSï¿½tï¿½ï¿½Qï¿½ï¿½;ï¿½Cï¿½ï¿½ï¿½yï¿½ï¿½zï¿½pï¿½qï¿½hï¿½7ï¿½Cï¿½ï¿½ï¿½xIÈ©?pï¿½_ï¿½2ï¿½ï¿½@ï¿½Ş›ï¿½ï¿½ï¿½_|ï¿½Ngï¿½ï¿½6ï¿½'ï¿½geÖ›_Héµ¼ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½è›‘ï¿½'y:ï¿½ï¿½ï¿½ì¥¿ï¿½ï¿½ï¿½ï¿½%ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½t
/ï¿½+Zoï¿½ï¿½0ï¿½ï¿½lï¿½=ï¿½ï¿½yï¿½}?ï¿½M-9ë½Šï¿½ï¿½g ï¿½??ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½;4ï¿½[ï¿½ï¿½ï¿½!oï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½Fï¿½#ï¿½,?ï¿½Tï¿½gmï¿½Rï¿½ï¿½Wï¿½ï¿½>f]ï¿½%-ï¿½ï¿½xÉ¬ï¿½/ï¿½a.mï¿½ï¿½|yî…‹ï¿½Hï¿½ï¿½|ï¿½Ì”ï¿½ï¿½hï¿½Ú‡V2~ï¿½ï¿½ï¿½Fï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½3ï¿½"ï¿½_cï¿½ï¿½ï¿½A}ï¿½I|ï¿½ï¿½Íˆï¿½Rï¿½I}ï¿½Kï¿½ï¿½F~ï¿½ï¿½:ï¿½#ï¿½Cï¿½ï¿½ï¿½/Y|Ú¿ï¿½ï¿½8ï¿½&ï¿½Ã©ï¿½-jï¿½D.ï¿½ï¿½3Oï¿½Rï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½ï¿½>oFQJï¿½j`zï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½Ip&bUï¿½9qgO:v}ï¿½^[57ï¿½}ï¿½ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½å« ï¿½"$ï¿½pï¿½wJPï¿½Gï¿½1ï¿½ï¿½ï¿½Rï¿½ï¿½ï¿½A><eï¿½u?ï¿½eï¿½ï¿½U7ï¿½\!ï¿½ï¿½lï¿½0ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½Ux=[ï¿½bï¿½ï¿½ï¿½ï¿½ERï¿½ï¿½.ï¿½ï¿½ï¿½ï¿½aszï¿½"ï¿½ï¿½Itï¿½sï¿½ï¿½ï¿½.|.Q>KŞ§ï¿½ï¿½ï¿½.ï¿½ï¿½8gï¿½'ï¿½ï¿½oï¿½\?2uï¿½ï¿½%
	8ï¿½ï¿½ï¿½'ï¿½ï¿½Hï¿½|	ï¿½ï¿½H{ï¿½ï¿½`ï¿½Yï¿½{ï¿½'ï¿½ï¿½ï¿½a7ï¿½Â¸ï¿½ï¿½ï¿½}tjQpï¿½ï¿½/zï¿½ï¿½ï¿½YFï¿½ï¿½ï¿½ï¿½ï¿½=[ï¿½ï¿½=Qï¿½pï¿½ï¿½ï¿½Ï Myï¿½	ï¿½bï¿½ï¿½Í“ï¿½oŞ†ï¿½ï¿½3ï¿½ß¼ï¿½ï¿½ONyï¿½Mï¿½ï¿½ï¿½h}ï¿½ï¿½&|Mï¿½Lï¿½ï¿½ï¿½$
ï¿½ÂŠï¿½ï¿½t
ï¿½0f#ï¿½g`ï¿½Hï¿½lkdï¿½Dvï¿½wï¿½>#æ“±'ï¿½Kï¿½ï¿½=/Eï¿½%ï¿½ï¿½/ï¿½ï¿½ï¿½ï¿½7Jï¿½ï¿½$<ï¿½ï¿½ï¿½4ï¿½xï¿½zï¿½=ï¿½ï¿½ï¿½=ï¿½ï¿½Yï¿½ï¿½+ï¿½ï¿½ï¿½pï¿½zï¿½@ï¿½Bï¿½ï¿½ï¿½ï¿½ï¿½Ä†ï¿½]p)8ï¿½'ï¿½3ï¿½{ï¿½Ä·$k1ï¿½ï¿½ï¿½\bï¿½<+ï¿½+6ã¹Šnï¿½ï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½ï¿½{ï¿½cAï¿½)mï¿½{ï¿½;%ï¿½ï¿½ï¿½Frï¿½9~va=ï¿½9Ì•j"ï¿½ï¿½ï¿½ï¿½<ï¿½ï¿½(ï¿½ï¿½=ï¿½ï¿½>É½ï¿½Eï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½cï¿½ï¿½kgspï¿½Y8ï¿½ï¿½ï¿½ï¿½9ï¿½?uï¿½ï¿½Sp^ï¿½*.ï¿½ï¿½Î¨4ï¿½ï¿½ï¿½nU?ï¿½ï¿½Wï¿½]&7oj%pï¿½ï¿½uï¿½`Z<bï¿½ï¿½ï¿½ï¿½x`Zï¿½ï¿½ï¿½fï¿½ï¿½ï¿½ï¿½%ï¿½ï¿½ï¿½\ sUï¿½[ï¿½#ï¿½ï¿½ï¿½ï¿½ï¿½5[H]ZUï¿½ï¿½ï¿½Krï¿½>ï¿½ï¿½ï¿½Iï¿½s{ï¿½ï¿½CpÙ«0Ê’@ï¿½ï¿½ï¿½u:ï¿½ï¿½oEï¿½ï¿½_ï¿½ï¿½2ï¿½ï¿½C3pR=ï¿½tï¿½ï¿½r|Æ¢ï¿½ï¿½'fï¿½eï¿½ï¿½>É³sï¿½ï¿½ï¿½_Q'`cï¿½\4ï¿½ppï¿½ï¿½(/Yï¿½ï¿½)ï¿½ï¿½
![ï¿½6ï¿½ï¿½+ï¿½!ï¿½0}O={ï¿½/ï¿½
ï¿½fLï¿½ï¿½ï¿½ï¿½ï¿½FİˆOï¿½:Cï¿½ï¿½ï¿½4ß‘ï¿½ï¿½wï¿½ï¿½ï¿½Fï¿½ï¿½*|Fï¿½`ï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½>Cï¿½ÜGfï¿½ï¿½ï¿½J:.ï¿½_ï¿½ï¿½cï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1g	ï¿½<ï¿½ï¿½g1ï¿½cÃ„ï¿½ï¿½$Ë±&ï¿½T.>ï¿½ï¿½ï¿½\ï¿½~Iï¿½5ï¿½ï¿½ï¿½ï¿½Oï¿½È³ï¿½ï¿½ï¿½'kï¿½ï¿½ï¿½_ï¿½ï¿½Îº9wï¿½ï¿½rï¿½ï¿½x7ï¿½ï¿½.ï¿½ï¿½yï¿½ï¿½Ñ°sï¿½
ï¿½qï¿½z*ï¿½@Egï¿½F5ï¿½&ï¿½ï¿½lï¿½~<ï¿½KÏ›ï¿½Qyï¿½L	ï¿½pï¿½sTyï¿½}7ï¿½ï¿½Gï¿½$ï¿½ï¿½ï¿½|7][gï¿½ï¿½&Uï¿½Rs1^[Ë©ï¿½ï¿½/Qï¿½ï¿½ï¿½0ï¿½R}ï¿½ï¿½=ï¿½ï¿½ï¿½xHï¿½sï¿½Ú­Æ¶^ï¿½Mxï¿½^ï¿½Cï¿½Qï¿½ï¿½ï¿½*ï¿½3ï¿½ï¿½ï¿½Ûµï¿½}ï¿½_;wï¿½ï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½Gï¿½ï¿½vlï¿½&Rï¿½ï¿½ï¿½{"s*ï¿½ï¿½ï¿½ï¿½yNï¿½ï¿½+ï¿½ï¿½ï¿½~kKï¿½ï¿½Æ³ï¿½7ß½ï¿½-Uï¿½ï¿½'ï¿½eï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½Ì«ï¿½>ï¿½ï¿½ï¿½Í†ï¿½xï¿½kß“ï¿½ï¿½9"srï¿½ï¿½Ê¡ï¿½ï¿½ï¿½	=e bï¿½Oï¿½ï¿½Wï¿½#"lï¿½Oï¿½ï¿½2ï¿½qj}ï¿½ï¿½ï¿½Ü«}(ï¿½ï¿½PjPï¿½@*ï¿½ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½8ï¿½@ï¿½\ï¿½ï¿½5ï¿½ï¿½ï¿½ï¿½Gï¿½ï¿½ß®lï¿½ï¿½UKÍ•Yï¿½JSï¿½ï¿½Uï¿½ï¿½'ï¿½{ï¿½hØ£yï¿½Iï¿½ï¿½
ï¿½ï¿½Dï¿½FY7kï¿½0ï¿½ï¿½3J'Tï¿½3S=Oï¿½ï¿½ï¿½hï¿½ï¿½Nï¿½ï¿½İ·{3
ï¿½oTï¿½ï¿½Èİ?ï¿½>3ï¿½ï¿½Ş”ï¿½ï¿½ï¿½"bï¿½ï¿½ï¿½ï¿½Å‹ï¿½*ï¿½]ï¿½&ï¿½é‡€ï¿½8Yï¿½	ï¿½ï¿½ï¿½ï¿½Éµï¿½'ï¿½Ó¥ï¿½5ï¿½/ï¿½nï¿½Gï¿½ï¿½É¼7:MzUï¿½Êˆ)ï¿½ï¿½ï¿½k_ï¿½ï¿½ï¿½mï¿½[ï¿½Û±Ë¾...more

PK
ï¿½ï¿½T" MIDI.band/Caches.nosync/_cacheInfoUT
ï¿½ï¿½bï¿½ï¿½bfcuxï¿½ï¿½Uï¿½Qkï¿½0ï¿½ï¿½ï¿½Hï¿½o][ï¿½%Mï¿½IA6a:Ø£$aï¿½Y
&ï¿½ï¿½ï¿½/ï¿½{ï¿½ï¿½ï¿½ï¿½Î¡ï¿½ï¿½kï¿½ï¿½ï¿½dï¿½q8ï¿½(\cï¿½9ï¿½nï¿½8ï¿½ï¿½Iï¿½ï¿½,ï¿½+ï¿½ï¿½ï¿½ï¿½ï¿½Lï¿½Yï¿½ï¿½ï¿½Taï¿½ï¿½kQTï¿½ï¿½kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½8gï¿½ï¿½<ï¿½ï¿½Bï¿½rï¿½.ï¿½ï¿½jï¿½ï¿½Ü­ï¿½2ï¿½ï¿½r
ï¿½ï¿½ï¿½ï¿½ï¿½Uï¿½t,xï¿½ï¿½ï¿½ï¿½d+/Ú¾ï¿½}ï¿½Zç¡¦)ï¿½ï¿½ï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½&ï¿½NOï¿½ï¿½hï¿½ï¿½-ï¿½ï¿½$ï¿½f)ï¿½7Lï¿½$ï¿½v1ï¿½?8ï¿½pï¿½Sï¿½mgï¿½PK ï¿½Jï¿½PK
ï¿½ï¿½Tï¿½ï¿½	 MIDI.band/projectDataUT
ï¿½ï¿½bï¿½ï¿½bfcuxï¿½ï¿½ï¿½ï¿½vï¿½È–(ï¿½\ï¿½+ï¿½İ£ï¿½|jï¿½Ö†;ï¿½ï¿½3"ï¿½$ 	ï¿½ï¿½F46_cFï¿½	pî•«ï¿½Jï¿½ï¿½e4ï¿½ï¿½"ï¿½ï¿½ï¿½{Zï¿½;Î¾ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6Û¸ï¿½ï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½nï¿½tï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ğ·®‰ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½ï¿½ï¿½ï¿½ï¿½+?ï¿½ï¿½ï¿½m+ï¿½ï¿½ï¿½ï¿½3^_ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½Aï¿½cwï¿½ï¿½İ®ï¿½ï¿½ï¿½kï¿½kï¿½ï¿½}ï¿½mï¿½ï¿½ ï¿½ï¿½ï¿½'Ã°ï¿½gï¿½C>ï¿½.ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½jvï¿½ï¿½ï¿½ï¿½ï¿½ãŸ¯ï¿½'ï¿½tï¿½"ï¿½ï¿½Oï¿½ßšï¿½gS~ï¿½m'Ë™{ï¿½ï¿½`ã¯¯1ï¿½?ï¿½ï¿½7ß¾ï¿½ï¿½ï¿½Xlï¿½ï¿½wï¿½ï¿½ï¿½ï¿½]ï¿½ï¿½ï¿½ï¿½3\ï¿½z;Yï¿½3oï¿½5ï¿½fï¿½ï¿½fï¿½wwï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½?ï¿½ï¿½ï¿½L;ï¿½ï¿½7>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½fï¿½ï¿½ï¿½?ï¿½ï¿½|ï¿½~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½tï¿½ï¿½ï¿½ï¿½Ûï¿½Gï¿½y/'ï¿½Û¯ZSï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½8ï¿½dï¿½'ï¿½M)}ï¿½ï¿½&ï¿½Qï¿½ï¿½ï¿½Ô­jØ£ï¿½Qï¿½~4
ï¿½ï¿½.ï¿½ï¿½ï¿½ï¿½
#ï¿½ï¿½ï¿½ï¿½ï¿½)!ï¿½Cï¿½e ï¿½Cï¿½.Ã„ï¿½0B^ï¿½ï¿½8ï¿½î³°ï¿½ï¿½ï¿½848)ï¿½}ï¿½kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½{nï¿½ï¿½Fpflï¿½ï¿½lï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½qï¿½Eï¿½M>ï¿½iï¿½ï¿½X:ï¿½ï¿½ï¿½)w3ï¿½ï¿½ï¿½3;Or{ï¿½Ü†ï¿½ï¿½dhï¿½dï¿½ï¿½ï¿½/2ï¿½s;F?ï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½Qï¿½ï¿½ï¿½Hï¿½ï¿½Oï¿½ ï¿½ï¿½zï¿½ï¿½#8rï¿½+|;N2
ï¿½ï¿½gï¿½ï¿½Ohï¿½ï¿½ï¿½&ï¿½Wï¿½+ï¿½cp5Nï¿½ï¿½ï¿½Iï¿½ï¿½hGï¿½Pï¿½ï¿½ï¿½;}ï¿½nï¿½Ã¯vï¿½ï¿½ï¿½Sï¿½8@b.ï¿½$ï¿½ï¿½yï¿½uï¿½'Ö€-ï¿½ï¿½Xï¿½rvï¿½ï¿½ï¿½Oï¿½=ï¿½ï¿½*{{Cï¿½ï¿½ï¿½ï¿½#ï¿½^ï¿½ï¿½ï¿½ï¿½~ï¿½xï¿½K@ï¿½ï¿½ï¿½.ï¿½ï¿½ï¿½Bï¿½oï¿½ï¿½'Dp*ï¿½Qï¿½yTjï¿½ï¿½cï¿½ï¿½!-uï¿½Û²ï¿½ï¿½V/0#ï¿½hï¿½ï¿½!ï¿½#ï¿½Â…ï¿½ï¿½Z
ï¿½h}ï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½L.<ï¿½ï¿½)Wï¿½ï¿½ï¿½ï¿½ï¿½iï¿½ï¿½=A<'ï¿½ï¿½ï¿½ï¿½
Öœï¿½ï¿½ï¿½ï¿½9ï¿½
ï¿½ï¿½ï¿½cï¿½ï¿½Aï¿½ï¿½w|]ï¿½#ï¿½}ï¿½ï¿½ï¿½ÏŠï¿½Vfï¿½twtï¿½4ï¿½ï¿½ï¿½^ï¿½ï¿½ï¿½tCï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½/ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½WWï¿½Qï¿½ï¿½|jï¿½xï¿½l<%ï¿½aï¿½ï¿½%ï¿½ï¿½-p"5ï¿½ï¿½Fï¿½Nb?Æ­?4Şï¿½/ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½ï¿½ï¿½1wï¿½\ï¿½ï¿½ï¿½
ï¿½ï¿½_/ï¿½ï¿½4mÏ¶ï¿½ï¿½D^1ï¿½X<ï¿½ï¿½ï¿½?0O|Yï¿½Nï¿½l^ï¿½ï¿½ï¿½Sï¿½ï¿½P;1+ï¿½(HLLï¿½CØšlzï¿½ï¿½tï¿½GwFUDï¿½à¦’_ï¿½Hï¿½
Iï¿½oï¿½Mï¿½È£ï¿½ï¿½Wï¿½ï¿½ï¿½;ï¿½q@ï¿½$^nlï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bï¿½A8rz-
&Eï¿½IH*yï¿½/gï¿½ï¿½ï¿½ï¿½ï¿½<ï¿½oï¿½ï¿½Kï¿½ï¿½ï¿½ï¿½ï¿½HLï¿½V(zfï¿½Q~"ï¿½]ï¿½ï¿½F(ï¿½qï¿½GYVï¿½'ï¿½]:]ç«¥Å¹ï¿½NI	LW@ï¿½ï¿½Z\2+uï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½tï¿½kJqz3Lï¿½<ï¿½ï¿½ï¿½TU_uï¿½ï¿½_ï¿½gï¿½ï¿½Siï¿½G}<

ï¿½ï¿½ï¿½``()ï¿½ï¿½dï¿½Zï¿½b|ï¿½=-ï¿½=EYvï¿½4ï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ò²ï¿½ï¿½ï¿½ï¿½Gï¿½Bï¿½ï¿½ï¿½ï¿½5ï¿½{~~Hï¿½Í«Opï¿½ï¿½ï¿½ĞŸï¿½&kqï¿½gHZï¿½ï¿½ï¿½ï¿½ï¿½Kï¿½;ï¿½ï¿½ï¿½pï¿½ï¿½ï¿½Yï¿½{8ï¿½ï¿½{ï¿½ï¿½(ï¿½x8ï¿½pï¿½ï¿½Cã¨ï¿½ï¿½ï¿½\Kï¿½ï¿½ï¿½ï¿½ï¿½Hï¿½'Sï¿½Je5ï¿½ï¿½ï¿½ï¿½f4ï¿½ï¿½E9Y!ï¿½ï¿½ï¿½ï¿½m\ï¿½
ï¿½Ç×¼ï¿½ï¿½KjFï¿½ï¿½x}ï¿½pï¿½ï¿½ï¿½tï¿½Çï¿½ï¿½=yEï¿½|.ï¿½fxï¿½_ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½cÂ‹ï¿½ï¿½q\ï¿½ï¿½ï¿½`tNï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1^ï¿½ï¿½ï¿½ï¿½#ï¿½sï¿½ï¿½Ç©ï¿½ï¿½,ï¿½ï¿½,rï¿½QMï¿½>Iï¿½ï¿½ï¿½|=ï¿½~ï¿½ï¿½ï¿½jï¿½ï¿½!ï¿½ï¿½tï¿½ï¿½ï¿½gIVï¿½Pwï¿½ï¿½ï¿½Mlë‚¤ï¿½q~rQ?+ï¿½/aï¿½ï¿½ï¿½Ù¬ï¿½ï¿½Cï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dCCï¿½ï¿½pJ"g\ï¿½'ï¿½ï¿½Qï¿½=Yï¿½
ï¿½Tï¿½ï¿½Ø®ï¿½Ü„z
[4ï¿½ï¿½VDï¿½ï¿½vï¿½ï¿½ï¿½Ç¶ï¿½ï¿½dï¿½]ï¿½Kï¿½ï¿½/ï¿½ï¿½Aï¿½ï¿½ï¿½ï¿½pì©¯ï¿½ï¿½ï¿½V}wï¿½ï¿½ï¿½EW4ï¿½Iï¿½ï¿½Î¿;dï¿½Aï¿½Ãƒ,-ï¿½
ï¿½ï¿½ï¿½ï¿½]^ï¿½ï¿½ï¿½Gxï¿½ulï¿½ï¿½ï¿½ï¿½ï¿½Jï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(-Ryï¿½,ï¿½â²¼ï¿½ï¿½Ô¸dï¿½ï¿½ï¿½ï¿½kï¿½ï¿½ï¿½#ï¿½Oï¿½7ï¿½ï¿½Gï¿½'QUï¿½ï¿½ï¿½ï¿½Eï¿½:ï¿½Vï¿½ï¿½ï¿½ï¿½ï¿½@ï¿½ï¿½Yï¿½6ï¿½vÙ§ï¿½:ï¿½.ï¿½ï¿½ï¿½`&8ï¿½Q:ï¿½!
ï¿½ï¿½ï¿½,3O)ï¿½ï¿½_	Pï¿½ï¿½ï¿½Wï¿½ï¿½vï¿½ï¿½ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½xï¿½ï¿½ï¿½0ï¿½81ï¿½ï¿½T'ï¿½ï¿½ï¿½:ï¿½ï¿½=!tï¿½ï¿½ï¿½ï¿½$ï¿½	ï¿½ï¿½Oï¿½?9:ï¿½ï¿½eï¿½ï¿½ï¿½O,ï¿½
H|Rrï¿½C8ï¿½ï¿½Úªï¿½ï¿½ï¿½ï¿½ï¿½ï¿½y.ï¿½SIï¿½ï¿½ï¿½ï¿½uï¿½hï¿½ï¿½!Cï¿½ï¿½Wvï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½7.ï¿½pl:n!ï¿½ï¿½vï¿½ï¿½Xï¿½&
ï¿½ï¿½ï¿½xk+ï¿½"ï¿½ï¿½ï¿½=ï¿½ï¿½ï¿½Q^-ï¿½Jï¿½ï¿½ï¿½/×²ï¿½ï¿½ÕµD~%k>ï¿½8Yï¿½1ï¿½ï¿½?ï¿½_ï¿½=ï¿½rï¿½ï¿½dï¿½Ke~ï¿½rï¿½6<Bï¿½ï¿½=ï¿½ï¿½sï¿½Rï¿½Â‰ï¿½ï¿½_ï¿½-ï¿½[Vï¿½ï¿½'Õ¬ï¿½ï¿½ï¿½=Wï¿½ï¿½ï¿½ï¿½ï¿½WNÜ´ï¿½ï¿½ï¿½ï¿½!ï¿½ï¿½ï¿½ï¿½ï¿½}ï¿½-ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½à¸«ï¿½Uï¿½ï¿½Oï¿½Ğ¢7ï¿½ï¿½ï¿½^ï¿½ï¿½Mï¿½Y:^Vï¿½Ò‘rtï¿½8ï¿½ï¿½ï¿½ï¿½<ï¿½umï¿½<!xPï¿½oï¿½ï¿½Kï¿½ï¿½ï¿½uï¿½ï¿½0Cï¿½ï¿½Zï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½Pï¿½vï¿½?ï¿½yï¿½ï¿½ï¿½-ï¿½ï¿½Gï¿½ï¿½ï¿½7(\>ï¿½ï¿½{gHtï¿½ï¿½Ö­Oï¿½ï¿½ï¿½1"%ï¿½!W:ï¿½ï¿½}"ï¿½ï¿½ï¿½Pï¿½ï¿½ï¿½3ï¿½qï¿½ï¿½hï¿½ï¿½Hï¿½rï¿½ï¿½>ï¿½ï¿½ï¿½!>*ï¿½^'ï¿½ï¿½q"ï¿½ï¿½&ï¿½dvï¿½1ï¿½ï¿½Vï¿½ï¿½ï¿½?ï¿½×´ï¿½ï¿½`rÆƒï¿½M&ï¿½Sï¿½P9ï¿½Nï¿½ï¿½x98ï¿½ï¿½ï¿½ï¿½[}ï¿½\{Vï¿½|
ï¿½Wï¿½ï¿½ï¿½RİŸï¿½	ï¿½lï¿½ï¿½ï¿½CØ›O$9ï¿½nbTï¿½ï¿½ï¿½ï¿½ï¿½9W8ï¿½^ï¿½O×¦gVï¿½ï¿½ï¿½nï¿½ï¿½ ï¿½!ÔŸq"E2[8ï¿½ï¿½ï¿½~ï¿½ï¿½ï¿½ï¿½Uï¿½b{Jï¿½ÔSï¿½ï¿½ï¿½ï¿½&ï¿½Rï¿½ï¿½ï¿½ï¿½ï¿½4ï¿½Wï¿½ï¿½Lï¿½=gï¿½}&Gï¿½ï¿½>\ï¿½8ï¿½ï¿½%Zï¿½ï¿½i;ï¿½k[Ü«Z;ï¿½;gEï¿½ï¿½ï¿½6ï¿½Wyï¿½x}9ï¿½ï¿½1^<6ï¿½ï¿½ï¿½i1-ï¿½	í‰¯/ï¿½kï¿½#oï¿½ï¿½>ï¿½ï¿½{TÕÖ½LO$ï¿½ï¿½6&ï¿½ï¿½ï¿½ï¿½U\ï¿½/_ï¿½ï¿½Sï¿½/9Cï¿½ï¿½ï¿½y\ï¿½cï¿½ï¿½ï¿½guoï¿½ï¿½ï¿½]ï¿½QDï¿½ï¿½ï¿½Kï¿½İ‹ï¿½ï¿½ï¿½
ï¿½ï¿½dï¿½ï¿½ï¿½ï¿½/Cï¿½]ï¿½Fï¿½ï¿½{Vï¿½ï¿½ï¿½É4ï¿½ï¿½R\ï¿½OeGï¿½Qï¿½ï¿½ÈRï¿½ï¿½ï¿½eï¿½<ï¿½ï¿½$.ï¿½#^ï¿½<Z[ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½Tï¿½ï¿½Eï¿½Úºï¿½ï¿½WFï¿½qï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Lï¿½yï¿½ï¿½ï¿½qq_ï¿½ï¿½ÉŒï¿½ï¿½?ï¿½wï¿½ï¿½ï¿½ï¿½ï¿½D\ï¿½ï¿½/ï¿½Xï¿½ï¿½Mï¿½ï¿½Jï¿½ï¿½?ï¿½Mï¿½9:bï¿½ï¿½;ï¿½ï¿½gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?Kï¿½ï¿½ï¿½2ï¿½Wï¿½ï¿½}ï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½;ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(ï¿½ï¿½.ï¿½mY~>ï¿½ï¿½ï¿½+lï¿½Dï¿½.ï¿½ï¿½~ï¿½?ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½Dï¿½Sï¿½[&ï¿½	ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½ï¿½ï¿½sï¿½ï¿½ï¿½y?ï¿½ï¿½ï¿½s9ï¿½t<ï¿½ï¿½Ã•ï¿½ï¿½X2ï¿½ï¿½ï¿½ï¿½~ï¿½ï¿½ï¿½os(ï¿½ï¿½ï¿½xï¿½ï¿½_ï¿½ï¿½ï¿½ï¿½ï¿½İŸwï¿½ï¿½ï¿½ï¿½ï¿½h<ï¿½ï¿½;ï¿½ï¿½]ï¿½%gï¿½)Zï¿½ï¿½xLï¿½ï¿½+ï¿½ï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½ï¿½ï¿½ï¿½gï¿½'zï¿½ï¿½>+ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½>^)ï¿½ï¿½ï¿½A*ï¿½
t*~Hï¿½ï¿½Şš3ï¿½ï¿½ï¿½ï¿½C!ï¿½ï¿½9ï¿½ï¿½ï¿½ï¿½1ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½y>ï¿½ï¿½ï¿½uï¿½Y;vï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½bï¿½sï¿½p|4ï¿½ï¿½ï¿½}nï¿½ï¿½ï¿½ï¿½Yï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½}ï¿½gxï¿½ï¿½gï¿½[>ï¿½{ï¿½ï¿½ï¿½Ó¼ï¿½_rï¿½+ï¿½ï¿½Oï¿½KWï¿½ï¿½ï¿½ï¿½ï¿½y"ï¿½ï¿½ï¿½ï¿½	>ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½YQŞ§ï¿½ï¿½Iï¿½?ï¿½ï¿½Y4Ş“ï¿½ï¿½ï¿½3ï¿½wAO]i{ï¿½Sï¿½'#ï¿½×‚?\ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½}ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½>ï¿½Iï¿½Iï¿½ï¿½Tï¿½~ï¿½ 3ï¿½-ï¿½ï¿½dï¿½dï¿½bGpï¿½ ï¿½ï¿½dSï¿½ßrï¿½ï¿½Dï¿½1ï¿½Jï¿½ï¿½2ï¿½lï¿½ï¿½@ï¿½ï¿½z&rï¿½(@ï¿½.ï¿½|ï¿½yBï¿½7)'~ï¿½Ï†ï¿½a	rï¿½{nï¿½ï¿½ï¿½25Tï¿½6Æ¶ï¿½=	p}"%ï¿½ï¿½ï¿½xRDï¿½sï¿½ï¿½qï¿½ï¿½Lï¿½\eï¿½Hï¿½Ç…pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Dpï¿½Å·Ø“ï¿½{ï¿½ï¿½ï¿½ï¿½0ï¿½ï¿½m4ï¿½?;ï¿½mï¿½N}K?ï¿½ï¿½ï¿½ï¿½PD1ï¿½ï¿½ï¿½iJï¿½|ï¿½nxZï¿½ï¿½+ï¿½ï¿½ï¿½ï¿½Nï¿½ï¿½Wï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½udï¿½
 ï¿½Gjï¿½ï¿½ï¿½ï¿½ï¿½ï¿½kHï¿½/o1L+ï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½np$8h]ï¿½ï¿½pï¿½_ï¿½ï¿½ï¿½Ifvxï¿½K{RJï¿½.ï¿½ï¿½xYï¿½~ftï¿½ï¿½ï¿½ï¿½
`ï¿½!ï¿½
ï¿½ ï¿½ï¿½#ï¿½ï¿½]ï¿½(ï¿½ï¿½ï¿½ï¿½ï¿½_ï¿½Òºï¿½*ï¿½Kï¿½ï¿½ï¿½KGï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½wÓ§ï¿½ï¿½^;ï¿½ï¿½8ï¿½?ï¿½ï¿½ï¿½ï¿½wZï¿½ï¿½ï¿½eï¿½itï¿½ï¿½bï¿½[ï¿½3ï¿½ï¿½ï¿½'ï¿½S$ï¿½ï¿½nï¿½odyï¿½ï¿½ï¿½ï¿½Zï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½@È£ï¿½oUï¿½ZVï¿½ï¿½ï¿½ï¿½qï¿½	ï¿½{ï¿½ï¿½ï¿½}ï¿½ï¿½Oï¿½>ï¿½wï¿½ï¿½|ï¿½([3~kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½nï¿½ï¿½Q|ï¿½ï¿½/okÉ„oï¿½ï¿½z:ï¿½,ï¿½oï¿½#!aÌ·ï¿½yË»ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ç«–ï¿½Õ›ï¿½=ï¿½n\Óï¿½ro4ï¿½/ï¿½7ï¿½ï¿½ï¿½Ïï¿½wï¿½ï¿½ï¿½ï¿½ï¿½Ü§qï¿½ï¿½ï¿½2ï¿½ï¿½2ï¿½ï¿½ï¿½ï¿½ï¿½Ñ›wï¿½pï¿½ï¿½ï¿½-ï¿½İ³ï¿½ï¿½;:ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½%ï¿½+ï¿½ï¿½ï¿½k:$ï¿½ï¿½rOps5Gï¿½ï¿½ï¿½ï¿½ï¿½97ï¿½ï¿½ï¿½=ï¿½GLï¿½ï¿½ï¿½[ï¿½gya&ï¿½wï¿½|Kï¿½Öï¿½ï¿½}ï¿½ï¿½,ï¿½Sx_ï¿½ï¿½u.\ï¿½ï¿½]bï¿½
qï¿½ï¿½ï¿½5ï¿½ï¿½ï¿½ï¿½Dï¿½>ï¿½_ï¿½F@ï¿½=	(ï¿½s]Lï¿½
wï¿½ï¿½;Jï¿½ï¿½ï¿½Jï¿½+ï¿½ï¿½ï¿½ï¿½tï¿½]=ï¿½ï¿½}&Wæ“£ï¿½ï¿½'ï¿½ï¿½tÆï¿½_ï¿½ï¿½ï¿½ï¿½3ï¿½ß—@ï¿½ï¿½ï¿½Ï‰3}ï¿½Î»ï¿½D!XJï¿½nB$9ï¿½ï¿½ï¿½ï¿½=ï¿½[!ï¿½ï¿½İ{Jï¿½ï¿½ï¿½/K7ï¿½_s-/ï¿½ï¿½ï¿½yp)ï¿½\ï¿½21ï¿½ï¿½x4ï¿½[ï¿½7ï¿½ï¿½ï¿½/21ï¿½ï¿½0ï¿½ï¿½8/{%1^4uï¿½sï¿½qï¿½<|rï¿½$ï¿½ï¿½ï¿½x_IB3}ï¿½Hï¿½ï¿½;ï¿½lu?ï¿½ï¿½kï¿½>ï¿½ï¿½Hxï¿½ï¿½;Yï¿½@>ï¿½ aï¿½ï¿½â•›|Sï¿½ï¿½ï¿½Pï¿½ZXï¿½Zï¿½ï¿½?ï¿½ï¿½nï¿½ï¿½Î¹Rï¿½Dï¿½ï¿½ï¿½JVIï¿½ï¿½oBï¿½ï¿½ï¿½Zï¿½}Hï¿½ï¿½Kï¿½Gï¿½iï¿½Pï¿½^Oï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½8]ï¿½Gï¿½ï¿½c,ï¿½@ï¿½ï¿½cï¿½ï¿½<ï¿½×…_~ï¿½cï¿½ï¿½ï¿½ï¿½yï¿½ï¿½ï¿½oï¿½]+Ô—ï¿½gï¿½NXï¿½4aï¿½Wï¿½cï¿½ï¿½?ï¿½wï¿½ï¿½yOï¿½\MRs#Şºï¿½ï¿½2ï¿½ï¿½ßŒï¿½Bï¿½ï¿½@ï¿½sCï¿½,ï¿½ï¿½ï¿½Bï¿½Lï¿½"d}ï¿½ï¿½}a"ï¿½,?Oï¿½9ï¿½,
ï¿½)iï¿½ï¿½m2ï¿½yBï¿½/'ï¿½Ó‘ï¿½ï¿½ï¿½JFï¿½×©ï¿½4ï¿½ï¿½H
Tï¿½ï¿½ï¿½54C?ï¿½İ„u$kï¿½ï¿½Wï¿½İŒ×‘ï¿½cx}ï¿½1ï¿½Pï¿½Mï¿½ï¿½7ï¿½3Ryï¿½0ï¿½ï¿½@Gï¿½ï¿½Ò¤Rï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½dï¿½Pï¿½qï¿½>ï¿½ï¿½ï¿½
ï¿½]ï¿½ï¿½'Uï¿½3+bnï¿½ï¿½Åï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Xï¿½34aï¿½Ä¯ï¿½ï¿½ï¿½ï¿½bï¿½ï¿½wï¿½ï¿½ï¿½ï¿½Vraï¿½Nï¿½Ş‘ï¿½ï¿½<ï¿½Rï¿½ ï¿½Rï¿½ï¿½5ZÃï¿½ï¿½ï¿½[\ï¿½|\ï¿½YvEï¿½ï¿½ï¿½ï¿½c0ï¿½ï¿½3ï¿½5ï¿½:rï¿½k#Q?ï¿½ï¿½Jï¿½ï¿½nwï¿½"ï¿½{Ò–ï¿½+ï¿½+6ï¿½8ï¿½ï¿½ï¿½\%ï¿½>kï¿½'ï¿½p'ï¿½ï¿½ï¿½ï¿½ï¿½;ï¿½0ï¿½"ï¿½ï¿½ï¿½9B7ï¿½_Ùp<4Nï¿½76ï¿½ï¿½ï¿½ï¿½Zï¿½'ï¿½ï¿½?ï¿½>\gï¿½Û©Xï¿½"ï¿½ï¿½ï¿½ï¿½-ï¿½ï¿½Gï¿½ï¿½ï¿½oÑŠWo%ï¿½ï¿½ï¿½Vdpï¿½]Trï¿½ï¿½ï¿½ï¿½yï¿½ï¿½Bï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½24ï¿½zï¿½vï¿½ï¿½/ï¿½ï¿½'0ï¿½4	ï¿½Ffï¿½ï¿½cß±ï¿½ï¿½ï¿½/iï¿½ï¿½ï¿½ï¿½1}eï¿½Õ½7ï¿½zzï¿½ï¿½ï¿½ï¿½<?>ï¿½#ï¿½1ï¿½ï¿½#ï¿½#ï¿½G|2ï¿½Y{yï¿½ï¿½
e<ï¿½ï¿½~ï¿½ï¿½}qï¿½Nï¿½|'ï¿½8ï¿½>ï¿½ï¿½ï¿½?,ï¿½Yï¿½ï¿½x=ï¿½[j;R?_ï¿½;vï¿½sG8ï¿½ï¿½\y7Xï¿½ï¿½ï¿½xLyï¿½A^ï¿½ï¿½[ï¿½ï¿½|ï¿½@~}}ï¿½_ï¿½ï¿½@7]xï¿½ï¿½ï¿½gï¿½s
â€ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ ï¿½Üï¿½ï¿½mï¿½ï¿½ñ°‡£ï¿½ï¿½Bï¿½ï¿½p8h]ï¿½eï¿½ï¿½Yï¿½ï¿½Rï¿½gï¿½.ï¿½t_]ï¿½_ï¿½, ï¿½fï¿½ï¿½ï¿½ï¿½ï¿½[ï¿½AJ$Z'ï¿½)By7ï¿½ï¿½Hï¿½S:W$C
â€%#ï¿½ï¿½ï¿½ï¿½ß¨ï¿½ï¿½ï¿½qcï¿½ï¿½ï¿½ï¿½:pï¿½Zï¿½cxï¿½.qXï¿½ï¿½ï¿½tï¿½ï¿½È¶Jï¿½cï¿½"ï¿½|ï¿½6ï¿½ï¿½0ï¿½ï¿½ï¿½ï¿½]ï¿½ï¿½>ï¿½!ï¿½gï¿½ï¿½ï¿½ï¿½Ø¥ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½NR{ï¿½ï¿½ï¿½gMHï¿½]ï¿½ï¿½ï¿½ï¿½bzbï¿½ï¿½ï¿½>Ô…=	?ï¿½ï¿½Cï¿½ï¿½ï¿½yfï¿½ï¿½Ù¢ï¿½Qï¿½ï¿½ï¿½t=ï¿½á¼¢ï¿½ï¿½Bï¿½ï¿½	ï¿½ï¿½ï¿½zï¿½;ï¿½ï¿½ï¿½=bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½gï¿½ï¿½ï¿½pï¿½ï¿½Tï¿½Ï‘Mï¿½vï¿½ï¿½ï¿½
ï¿½É³iï¿½ï¿½yï¿½ï¿½3xï¿½ï¿½4>ï¿½ï¿½"â£»ï¿½ï¿½ï¿½ï¿½ï¿½aOï¿½ï¿½9ï¿½Ibï¿½iï¿½ï¿½6ï¿½ï¿½oï¿½TEï¿½ï¿½~QØ›ï¿½fkn#ï¿½ï¿½ï¿½ï¿½ï¿½×·ë—©ï¿½|ï¿½?ï¿½:ï¿½ï¿½hï¿½ï¿½ï¿½3ï¿½ï¿½}×ƒsï¿½ï¿½ï¿½qï¿½ï¿½ï¿½ï¿½Eï¿½
7z'ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½î‰¡aï¿½yï¿½ï¿½Wï¿½qï¿½ï¿½ï¿½ï¿½"mï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½`
ï¿½Å•ï¿½ï¿½ï¿½pï¿½qfï¿½Jï¿½ï¿½Xçª±<ï¿½ï¿½!ï¿½u{ï¿½T:ï¿½ï¿½ï¿½f]J$7ï¿½,${vï¿½{ï¿½ï¿½Åï¿½yï¿½r5^<ï¿½ï¿½Å¥dï¿½Wï¿½ï¿½s?9c9/ï¿½ï¿½-ï¿½ï¿½ï¿½(ï¿½\yï¿½ï¿½:\!ï¿½7ï¿½7ï¿½ï¿½Q-ï¿½ï¿½)Ğ—ï¿½ï¿½ï¿½ï¿½oï¿½nï¿½ï¿½ï¿½ï¿½ï¿½ÌŸO?_ï¿½=ï¿½[ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½ï¿½^ï¿½#=ï¿½)ï¿½>pï¿½ï¿½8ï¿½_ï¿½ï¿½7ï¿½ï¿½ï¿½ÑHï¿½Éœï¿½nï¿½ï¿½wï¿½Ì…ï¿½"ï¿½cï¿½8Aï¿½{ï¿½iï¿½ï¿½ï¿½0'ï¿½ï¿½ï¿½<ojï¿½
bï¿½kOï¿½7ï¿½pï¿½ï¿½v)/^aï¿½x
ï¿½'o>ï¿½[Gs3ï¿½\Wï¿½8ï¿½fï¿½ï¿½ï¿½sï¿½ï¿½ï¿½uï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"ï¿½ï¿½8ï¿½]ï¿½ï¿½qï¿½cï¿½YRï¿½ï¿½4l2ï¿½<ï¿½ï¿½zcrï¿½ï¿½ï¿½ï¿½ï¿½\8ï¿½ï¿½ï¿½{0Yï¿½ï¿½lï¿½+cs>Oï¿½uï¿½æ¤’ï¿½]0ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½ï¿½fï¿½ï¿½qï¿½Z|Ï¤:]ï¿½ï¿½mvY'ï¿½Oï¿½ï¿½ï¿½ï¿½ï¿½Pï¿½ï¿½ï¿½ï¿½3ï¿½mï¿½3ï¿½gï¿½ï¿½Mï¿½ï¿½aï¿½o
ï¿½Cï¿½ï¿½é¥Ÿï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?`}aï¿½%ï¿½`G6ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿½^3ï¿½s""ï¿½1Ë·"ï¿½G
ï¿½ï¿½ï¿½ï¿½6ï¿½DÓ¹Nï¿½ï¿½Iï¿½[ï¿½ï¿½ï¿½ï¿½ï¿½fï¿½ŞÇ‡ï¿½ï¿½Kï¿½7XR
ï¿½ï¿½jï¿½ï¿½Sï¿½8,ï¿½×œ:cï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½>ï¿½,ï¿½ï¿½Tï¿½#ï¿½pï¿½ï¿½),ï¿½ï¿½{ï¿½'ï¿½ï¿½ï¿½ï¿½6+ï¿½ï¿½Qï¿½ï¿½ï¿½~ï¿½zï¿½ï¿½ï¿½ï¿½xï¿½Ì“ï¿½ï¿½ï¿½ï¿½<Bï¿½$sï¿½Wï¿½^Fï¿½ï¿½ï¿½ï¿½Lï¿½arï¿½Ìï¿½3gï¿½ï¿½vï¿½3Xï¿½g\ï¿½gï¿½sï¿½ï¿½{Fï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½i'~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dï¿½eï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½9iï¿½ï¿½ï¿½ï¿½ï¿½ï¿½g\ï¿½ï¿½ï¿½ï¿½rJï¿½q&Ï¸ï¿½NGï¿½rï¿½ï¿½6ï¿½9KMX'/ï¿½ï¿½+Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"ï¿½ï¿½ï¿½Ï¯ï¿½Rp	H8ï¿½ï¿½^Dï¿½ï¿½ï¿½ï¿½ï¿½mL
â€×—|ï¿½ï¿½ï¿½ï¿½oï¿½Aï¿½_q`}ß£ï¿½xï¿½R>
ï¿½gï¿½n
ï¿½5/\pï¿½ï¿½Úˆï¿½zpï¿½ï¿½1ï¿½Zï¿½`ï¿½Zï¿½ï¿½hï¿½zï¿½ï¿½zï¿½7=ï¿½Ö·G?ï¿½e|ï¿½ï¿½ï¿½	,ï¿½É¼ï¿½ï¿½ï¿½yï¿½ï¿½5?K×—ï¿½ï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½=Ø”=ï¿½ï¿½ï¿½{6ï¿½ï¿½>Kï¿½@Oï¿½Vï¿½ï¿½ï¿½Iï¿½Jï¿½ï¿½Oï¿½2ï¿½ï¿½yï¿½ï¿½ï¿½_ï¿½ï¿½Sypï¿½6LNï¿½i:t3ï¿½Kï¿½ï¿½rÓ¶JNÛ§ï¿½^fï¿½ï¿½k{$ï¿½ï¿½Pï¿½ï¿½Zï¿½ï¿½ï¿½cEï¿½ï¿½ï¿½ï¿½p)ï¿½ï¿½ï¿½@>e[ï¿½ï¿½Mï¿½ï¿½ï¿½ï¿½ï¿½!Bh7xï¿½ï¿½a/ï¿½'ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ØƒVï¿½ï¿½ï¿½@>ï¿½"ï¿½ï¿½ï¿½=ï¿½_ï¿½ï¿½X=n44<zï¿½Mï¿½2ï¿½ï¿½ï¿½<?sï¿½ï¿½cï¿½\Rï¿½ï¿½ï¿½PÛ¡c{ï¿½mï¿½ï¿½Kï¿½_ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½`ï¿½ï¿½fï¿½ï¿½ï¿½ï¿½ï¿½ï¿½iï¿½>ï¿½_ï¿½wï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½;ï¿½ï¿½2Nï¿½ï¿½ï¿½ï¿½5OVzï¿½ sï¿½ï¿½	\~ï¿½ï¿½Ó’3ï¿½i+rï¿½xï¿½ï¿½oUzï¿½ï¿½ï¿½gï¿½#Nï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½ï¿½ï¿½'ï¿½`Ï±3ï¿½ï¿½ï¿½7ï¿½OKï¿½Mï¿½m	ï¿½&rï¿½ï¿½uï¿½ï¿½ï¿½ï¿½nï¿½ï¿½J^Kï¿½2ï¿½ï¿½e?{ï¿½ï¿½ï¿½xOï¿½ï¿½ï¿½ï¿½!Lï¿½ï¿½ï¿½ï¿½ï¿½Y_Vï¿½ï¿½kFï¿½ï¿½ï¿½ï¿½qUï¿½Iï¿½ï¿½Kï¿½ï¿½4Gï¿½(ï¿½{ï¿½ï¿½,Ø€ï¿½a<ï¿½{;ï¿½Sï¿½ï¿½ï¿½ï¿½ï¿½ï¿½&|ï¿½ï¿½ï¿½Rï¿½ï¿½Fï¿½-ï¿½zï¿½ï¿½ï¿½ï¿½9ï¿½#ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½x&ï¿½Î¨ZPzï¿½ï¿½ï¿½fï¿½aXï¿½j'ï¿½ï¿½ï¿½ï¿½tØ¨ï¿½*oï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½$yÎ§Ç£ï¿½ï¿½ï¿½t<Dï¿½ï¿½EQgï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½sï¿½vï¿½ï¿½mï¿½ï¿½_ï¿½ï¿½Ò—ï¿½ï¿½ï¿½Oï¿½=ï¿½ï¿½×¯ï¿½F{Zhï¿½oï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½B\Yï¿½ï¿½Aï¿½9ï¿½GÅ›è†ï¿½ï¿½ï¿½_ï¿½ï¿½ï¿½Ü“\sï¿½3ï¿½ï¿½ï¿½Uï¿½ï¿½Pï¿½xï¿½/ï¿½=ï¿½ï¿½%ï¿½\ï¿½ZFï¿½3ï¿½ï¿½%ï¿½~ï¿½ï¿½3Mï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½5ï¿½dï¿½:ï¿½ï¿½Tï¿½ï¿½ï¿½[ï¿½ï¿½qï¿½+ï¿½@ï¿½<yï¿½ï¿½ï¿½kï¿½ï¿½ï¿½7i[ï¿½ï¿½ï¿½ï¿½ï¿½3yï¿½,ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½5Xï¿½ï¿½ï¿½ï¿½~ï¿½kï¿½ï¿½ï¿½ï¿½Ê£ï¿½ï¿½ï¿½\=ï¿½ï¿½O&E_ï¿½ï¿½ï¿½hLï¿½ï¿½Õ§(ï¿½zDï¿½ï¿½Jï¿½{ï¿½É§ï¿½ï¿½ï¿½ï¿½G4Î£/ï¿½ï¿½hï¿½ï¿½ï¿½ï¿½ï¿½Ñ•ï¿½/ï¿½ï¿½g/Fï¿½ï¿½_Jï¿½Xï¿½ï¿½Tï¿½sQï¿½ï¿½Ş£ï¿½ï¿½Eï¿½Kï¿½ï¿½kï¿½Sï¿½0ï¿½~\Hï¿½<ï¿½Ú£ï¿½Æ©ï¿½oBï¿½P/ï¿½ï¿½ï¿½R$ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½#ï¿½ï¿½'ï¿½ï¿½xï¿½ï¿½ï¿½ï¿½kï¿½ï¿½ï¿½Bï¿½"_ï¿½7>ï¿½ó¡¦Oï¿½ï¿½7ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Æ©|ï¿½}Nï¿½ï¿½3zï¿½ï¿½ï¿½Ì¡qï¿½ï¿½[ï¿½ï¿½Oï¿½ï¿½ï¿½<ï¿½hï¿½ï¿½Wï¿½
ï¿½ï¿½Ó·ï¿½|ï¿½ï¿½.ï¿½1ï¿½ï¿½ï¿½pÖŸï¿½ï¿½ï¿½3ZCï¿½ï¿½ï¿½ï¿½ï¿½ï¿½p?ï¿½ï¿½}ï¿½yï¿½É½ï¿½ï¿½p
ï¿½Oï¿½éˆ¾ï¿½hlï¿½szFï¿½ï¿½zğ±ï¿½ï¿½1_ï¿½ï¿½|ï¿½ï¿½4Oï¿½=]7ï¿½:ï¿½qï¿½gé–¾4~ï¿½Yï¿½#ï¿½ï¿½ï¿½ï¿½tï¿½?)ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½È·Gy2ï¿½ï¿½ï¿½Iï¿½ï¿½ï¿½ï¿½Uï¿½ï¿½\Nï¿½ï¿½Dï¿½ï¿½hL3hï¿½2Ú‡ï¿½ï¿½tD_ï¿½<ï¿½T"ï¿½9ï¿½ï¿½ï¿½iï¿½ï¿½Kï¿½qï¿½gï¿½Xï¿½cï¿½ï¿½ï¿½ï¿½Êoï¿½xNï¿½Û§ï¿½ttï¿½'ï¿½ï¿½Ş‘ï¿½$?Aï¿½ï¿½ï¿½ï¿½d~ï¿½ï¿½ï¿½ï¿½Oï¿½Oï¿½È°ï¿½ï¿½>ï¿½kï¿½ï¿½ï¿½ï¿½ZŞµï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Eï¿½GOï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½#_+ï¿½Kå ’ï¿½ï¿½eï¿½Z>+ï¿½i|ï¿½ï¿½8ï¿½6ï¿½ï¿½7èŒ¶Vvï¿½y4ï¿½]ï¿½}Sp?ï¿½nï¿½Cï¿½ï¿½ï¿½ï¿½\Lcï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'11ï¿½'Hï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bï¿½ï¿½>9uï¿½ï¿½cï¿½ï¿½ï¿½Ó¾Vï¿½hï¿½zï¿½ï¿½ï¿½ï¿½ï¿½@_6^ï¿½ï¿½Oï¿½ï¿½1ï¿½~Gï¿½Kï¿½ï¿½qï¿½0Kï¿½î§¾Vvï¿½ï¿½z:ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½k1ï¿½ï¿½3ï¿½\ï¿½ï¿½ï¿½Iï¿½ÛŸï¿½Zï¿½ï¿½'ï¿½ï¿½ï¿½ï¿½hLï¿½ï¿½ï¿½o*~ï¿½ï¿½ï¿½Eï¿½/ï¿½cï¿½>ï¿½ï¿½hï¿½ï¿½ï¿½Z4}Kï¿½	ï¿½e8ï¿½/?ï¿½Bï¿½%ï¿½ï¿½A<ï¿½<ï¿½^Û¿ge8É¿ï¿½ï¿½ï¿½ï¿½ï¿½p1ï¿½i
ï¿½)Nï¿½Uï¿½ï¿½tÒ³ï¿½}Q>ï¿½aL,ï¿½ï¿½ï¿½C_Kï¿½ï¿½ï¿½ï¿½Vï¿½ï¿½ï¿½Xï¿½ï¿½Jñœ•ï¿½dï¿½?ï¿½"8ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Srï¿½ï¿½ï¿½ï¿½Cï¿½ï¿½Eï¿½ï¿½ï¿½ï¿½ï¿½ï¿½UHï¿½ï¿½ï¿½7ï¿½ï¿½+;\\ï¿½}ï¿½ï¿½ï¿½ï¿½@ï¿½=ï¿½lqï¿½~aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½xNï¿½ï¿½ï¿½@ï¿½ï¿½Lï¿½ï¿½ï¿½ï¿½Wï¿½ï¿½w,ï¿½ï¿½ï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½oï¿½?Qï¿½ï¿½HOï¿½tMï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½ï¿½Tï¿½ï¿½uï¿½?ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½cï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½/ï¿½?ï¿½hï¿½ï¿½ï¿½Xï¿½ï¿½ï¿½ï¿½ï¿½ï¿½311;ï¿½ï¿½Ï¨?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ yï¿½oï¿½?ï¿½ï¿½5ï¿½ï¿½ï¿½@ï¿½hï¿½ï¿½ï¿½mï¿½8ï¿½Ç¿X8'{e~{ï¿½!ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 7ï¿½ï¿½suï¿½hOï¿½ï¿½ï¿½yï¿½ï¿½Xï¿½bï¿½ï¿½ï¿½7ï¿½ï¿½ï¿½_Rï¿½ï¿½ï¿½Ù±xFï¿½ï¿½ï¿½L^ï¿½Oï¿½?ï¿½É½ï¿½ï¿½ï¿½ï¿½ï¿½uï¿½ï¿½Qsï¿½pnï¿½×‰ï¿½ï¿½ï¿½ï¿½Yï¿½ï¿½ï¿½ï¿½ï¿½É½ï¿½ï¿½ï¿½ï¿½×‰ï¿½ï¿½Uï¿½ï¿½6<<ï¿½á¸¾ï¿½ï¿½ï¿½ç¢¼ï¿½oï¿½?ï¿½ï¿½Mï¿½v`ï¿½oï¿½?Hï¿½~Xï¿½ï¿½:q4ï¿½_P8ï¿½$ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½ï¿½uï¿½ï¿½ï¿½Sï¿½ï¿½;ï¿½ï¿½?ï¿½ï¿½uï¿½?P8ï¿½ï¿½Zï¿½ï¿½ï¿½0ï¿½ï¿½ï¿½oï¿½?ï¿½ï¿½ï¿½Ä¿ï¿½ï¿½ =ï¿½ï¿½æŸ²ï¿½ï¿½Q^ï¿½Oï¿½?Lï¿½'ï¿½ï¿½ï¿½Nï¿½Ä¿ï¿½ï¿½@ï¿½Òï¿½?ï¿½ï¿½:q8ï¿½?Zï¿½ï¿½ï¿½;ï¿½ï¿½ï¿½	ï¿½ï¿½ï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½_'ï¿½ï¿½eï¿½A~'ï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½?\ï¿½ï¿½Zï¿½ï¿½ï¿½!ï¿½ï¿½ï¿½ï¿½Fï¿½;ï¿½rï¿½wï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½gï¿½ZBï¿½aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½uï¿½?ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½:ï¿½ï¿½?\ï¿½ï¿½ï¿½Qï¿½ï¿½#ï¿½ï¿½wï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½@gï¿½Hï¿½ï¿½7Fï¿½ï¿½r/ï¿½ï¿½'ï¿½ï¿½ä‰¿ï¿½ï¿½ï¿½ï¿½ï¿½[ï¿½Jï¿½ß­?pï¿½uï¿½hï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½yrï¿½oï¿½?pï¿½ubï¿½'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ñ±rh,ï¿½ï¿½ï¿½pï¿½ï¿½kï¿½ï¿½ï¿½$r_1[X0~/bï¿½ï¿½ï¿½7ï¿½8ï¿½ï¿½Aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dï¿½Lï¿½qï¿½Wqï¿½ï¿½ï¿½ï¿½ï¿½N6ï¿½<ï¿½ï¿½gpï¿½ï¿½ï¿½ï¿½Mï¿½Oï¿½3ï¿½ï¿½ï¿½?@*2Cß‚ï¿½ï¿½ï¿½ï¿½ï¿½zSï¿½tï¿½ï¿½Qï¿½ï¿½;ï¿½Cï¿½ï¿½ï¿½yï¿½ï¿½zï¿½pï¿½qï¿½hï¿½7ï¿½Cï¿½ï¿½ï¿½xIÈ©?pï¿½_ï¿½2ï¿½ï¿½@ï¿½Ş›ï¿½ï¿½ï¿½_|ï¿½Ngï¿½ï¿½6ï¿½'ï¿½geÖ›_Héµ¼ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½è›‘ï¿½'y:ï¿½ï¿½ï¿½ì¥¿ï¿½ï¿½ï¿½ï¿½%ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½t
/ï¿½+Zoï¿½ï¿½0ï¿½ï¿½lï¿½=ï¿½ï¿½yï¿½}?ï¿½M-9ë½Šï¿½ï¿½g ï¿½??ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½;4ï¿½[ï¿½ï¿½ï¿½!oï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½Fï¿½#ï¿½,?ï¿½Tï¿½gmï¿½Rï¿½ï¿½Wï¿½ï¿½>f]ï¿½%-ï¿½ï¿½xÉ¬ï¿½/ï¿½a.mï¿½ï¿½|yî…‹ï¿½Hï¿½ï¿½|ï¿½Ì”ï¿½ï¿½hï¿½Ú‡V2~ï¿½ï¿½ï¿½Fï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½3ï¿½"ï¿½_cï¿½ï¿½ï¿½A}ï¿½I|ï¿½ï¿½Íˆï¿½Rï¿½I}ï¿½Kï¿½ï¿½F~ï¿½ï¿½:ï¿½#ï¿½Cï¿½ï¿½ï¿½/Y|Ú¿ï¿½ï¿½8ï¿½&ï¿½Ã©ï¿½-jï¿½D.ï¿½ï¿½3Oï¿½Rï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½ï¿½>oFQJï¿½j`zï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½Ip&bUï¿½9qgO:v}ï¿½^[57ï¿½}ï¿½ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½å« ï¿½"$ï¿½pï¿½wJPï¿½Gï¿½1ï¿½ï¿½ï¿½Rï¿½ï¿½ï¿½A><eï¿½u?ï¿½eï¿½ï¿½U7ï¿½\!ï¿½ï¿½lï¿½0ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½Ux=[ï¿½bï¿½ï¿½ï¿½ï¿½ERï¿½ï¿½.ï¿½ï¿½ï¿½ï¿½aszï¿½"ï¿½ï¿½Itï¿½sï¿½ï¿½ï¿½.|.Q>KŞ§ï¿½ï¿½ï¿½.ï¿½ï¿½8gï¿½'ï¿½ï¿½oï¿½\?2uï¿½ï¿½%
	8ï¿½ï¿½ï¿½'ï¿½ï¿½Hï¿½|	ï¿½ï¿½H{ï¿½ï¿½`ï¿½Yï¿½{ï¿½'ï¿½ï¿½ï¿½a7ï¿½Â¸ï¿½ï¿½ï¿½}tjQpï¿½ï¿½/zï¿½ï¿½ï¿½YFï¿½ï¿½ï¿½ï¿½ï¿½=[ï¿½ï¿½=Qï¿½pï¿½ï¿½ï¿½Ï Myï¿½	ï¿½bï¿½ï¿½Í“ï¿½oŞ†ï¿½ï¿½3ï¿½ß¼ï¿½ï¿½ONyï¿½Mï¿½ï¿½ï¿½h}ï¿½ï¿½&|Mï¿½Lï¿½ï¿½ï¿½$
ï¿½ÂŠï¿½ï¿½t
ï¿½0f#ï¿½g`ï¿½Hï¿½lkdï¿½Dvï¿½wï¿½>#æ“±'ï¿½Kï¿½ï¿½=/Eï¿½%ï¿½ï¿½/ï¿½ï¿½ï¿½ï¿½7Jï¿½ï¿½$<ï¿½ï¿½ï¿½4ï¿½xï¿½zï¿½=ï¿½ï¿½ï¿½=ï¿½ï¿½Yï¿½ï¿½+ï¿½ï¿½ï¿½pï¿½zï¿½@ï¿½Bï¿½ï¿½ï¿½ï¿½ï¿½Ä†ï¿½]p)8ï¿½'ï¿½3ï¿½{ï¿½Ä·$k1ï¿½ï¿½ï¿½\bï¿½<+ï¿½+6ã¹Šnï¿½ï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½ï¿½{ï¿½cAï¿½)mï¿½{ï¿½;%ï¿½ï¿½ï¿½Frï¿½9~va=ï¿½9Ì•j"ï¿½ï¿½ï¿½ï¿½<ï¿½ï¿½(ï¿½ï¿½=ï¿½ï¿½>É½ï¿½Eï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½cï¿½ï¿½kgspï¿½Y8ï¿½ï¿½ï¿½ï¿½9ï¿½?uï¿½ï¿½Sp^ï¿½*.ï¿½ï¿½Î¨4ï¿½ï¿½ï¿½nU?ï¿½ï¿½Wï¿½]&7oj%pï¿½ï¿½uï¿½`Z<bï¿½ï¿½ï¿½ï¿½x`Zï¿½ï¿½ï¿½fï¿½ï¿½ï¿½ï¿½%ï¿½ï¿½ï¿½\ sUï¿½[ï¿½#ï¿½ï¿½ï¿½ï¿½ï¿½5[H]ZUï¿½ï¿½ï¿½Krï¿½>ï¿½ï¿½ï¿½Iï¿½s{ï¿½ï¿½CpÙ«0Ê’@ï¿½ï¿½ï¿½u:ï¿½ï¿½oEï¿½ï¿½_ï¿½ï¿½2ï¿½ï¿½C3pR=ï¿½tï¿½ï¿½r|Æ¢ï¿½ï¿½'fï¿½eï¿½ï¿½>É³sï¿½ï¿½ï¿½_Q'`cï¿½\4ï¿½ppï¿½ï¿½(/Yï¿½ï¿½)ï¿½ï¿½
![ï¿½6ï¿½ï¿½+ï¿½!ï¿½0}O={ï¿½/ï¿½
ï¿½fLï¿½ï¿½ï¿½ï¿½ï¿½FİˆOï¿½:Cï¿½ï¿½ï¿½4ß‘ï¿½ï¿½wï¿½ï¿½ï¿½Fï¿½ï¿½*|Fï¿½`ï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½>Cï¿½ÜGfï¿½ï¿½ï¿½J:.ï¿½_ï¿½ï¿½cï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1g	ï¿½<ï¿½ï¿½g1ï¿½cÃ„ï¿½ï¿½$Ë±&ï¿½T.>ï¿½ï¿½ï¿½\ï¿½~Iï¿½5ï¿½ï¿½ï¿½ï¿½Oï¿½È³ï¿½ï¿½ï¿½'kï¿½ï¿½ï¿½_ï¿½ï¿½Îº9wï¿½ï¿½rï¿½ï¿½x7ï¿½ï¿½.ï¿½ï¿½yï¿½ï¿½Ñ°sï¿½
ï¿½qï¿½z*ï¿½@Egï¿½F5ï¿½&ï¿½ï¿½lï¿½~<ï¿½KÏ›ï¿½Qyï¿½L	ï¿½pï¿½sTyï¿½}7ï¿½ï¿½Gï¿½$ï¿½ï¿½ï¿½|7][gï¿½ï¿½&Uï¿½Rs1^[Ë©ï¿½ï¿½/Qï¿½ï¿½ï¿½0ï¿½R}ï¿½ï¿½=ï¿½ï¿½ï¿½xHï¿½sï¿½Ú­Æ¶^ï¿½Mxï¿½^ï¿½Cï¿½Qï¿½ï¿½ï¿½*ï¿½3ï¿½ï¿½ï¿½Ûµï¿½}ï¿½_;wï¿½ï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½Gï¿½ï¿½vlï¿½&Rï¿½ï¿½ï¿½{"s*ï¿½ï¿½ï¿½ï¿½yNï¿½ï¿½+ï¿½ï¿½ï¿½~kKï¿½ï¿½Æ³ï¿½7ß½ï¿½-Uï¿½ï¿½'ï¿½eï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½Ì«ï¿½>ï¿½ï¿½ï¿½Í†ï¿½xï¿½kß“ï¿½ï¿½9"srï¿½ï¿½Ê¡ï¿½ï¿½ï¿½	=e bï¿½Oï¿½ï¿½Wï¿½#"lï¿½Oï¿½ï¿½2ï¿½qj}ï¿½ï¿½ï¿½Ü«}(ï¿½ï¿½PjPï¿½@*ï¿½ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½8ï¿½@ï¿½\ï¿½ï¿½5ï¿½ï¿½ï¿½ï¿½Gï¿½ï¿½ß®lï¿½ï¿½UKÍ•Yï¿½JSï¿½ï¿½Uï¿½ï¿½'ï¿½{ï¿½hØ£yï¿½Iï¿½ï¿½
ï¿½ï¿½Dï¿½FY7kï¿½0ï¿½ï¿½3J'Tï¿½3S=Oï¿½ï¿½ï¿½hï¿½ï¿½Nï¿½ï¿½İ·{3
ï¿½oTï¿½ï¿½Èİ?ï¿½>3ï¿½ï¿½Ş”ï¿½ï¿½ï¿½"bï¿½ï¿½ï¿½ï¿½Å‹ï¿½*ï¿½]ï¿½&ï¿½é‡€ï¿½8Yï¿½	ï¿½ï¿½ï¿½ï¿½Éµï¿½'ï¿½Ó¥ï¿½5ï¿½/ï¿½nï¿½Gï¿½ï¿½É¼7:MzUï¿½Êˆ)ï¿½ï¿½ï¿½k_ï¿½ï¿½ï¿½mï¿½[ï¿½Û±Ë¾...more

<iframe src="https://drive.google.com/file/d/1-8XhpOk1Iv7Du78rpOH1pf5tQto3nRdB/preview" width="640" height="480" allow="autoplay"></iframe>

https://github.com/TheAdam-Verse/tes/blob/main/Copy%20of%20MIDI%202.txt?raw=true

BNDLband

version: 0.2.3
files:
  - url: DAWG-Setup-0.2.3.exe
    sha512: INBSUFvGQo6v5UwjbRgeGQEdXuc4KjxZjmgPiGzNr19/z6mrmuyDUAydyaXHVc9s5C647iti+RrXw1yR+k6rhw==
    size: 47159793
    isAdminRightsRequired: true
path: DAWG-Setup-0.2.3.exe
sha512: INBSUFvGQo6v5UwjbRgeGQEdXuc4KjxZjmgPiGzNr19/z6mrmuyDUAydyaXHVc9s5C647iti+RrXw1yR+k6rhw==
releaseDate: '2020-03-05T14:15:13.004Z'

{
  "success": true,
  "skiplist": [
    {
      "zoom.us": 7200
    },
    {
      "meet.google.com": 7200
    },
    {
      "hangouts.clients6.google.com": 7200
    },
    {
      "clever.com": 7200
    },
    {
      "googleads.g.doubleclick.net": 86400
    },
    {
      "launchpad.classlink.com": 86400
    },
    {
      "canvas.apps.chrome": 86400
    },
    {
      "login.i-ready.com": 86400
    }
  ],
  "ttl": 3600,
  "selfharmlist": [
    "800-273-8255",
    "800 273 8255",
    "800-273-TALK",
    "800 273 TALK",
    "833 456 4566",
    "833-456-4566",
    "800-422-4453",
    "800 422 4453",
    "800-799-7233",
    "800 799 7233",
    "800-799-SAFE",
    "800 799 SAFE",
    "800-784-2433",
    "800 784 2433",
    "800-SUICIDE",
    "800 SUICIDE",
    "866-4-U-TREVOR",
    "866-488-7386",
    "866 488 7386",
    "800-4-A-Child",
    "808-2000-247",
    "808 2000 247",
    "suicide helpline",
    "suicide prevention helpline",
    "suicide hotline",
    "suicide prevention hotline",
    "988 suicide and crisis lifeline",
    "help is available"
  ],
  "vectorExpansionRules": {
    "instagram.com": [
      {
        "pattern": "*/web/comments/*/add*",
        "field": "requestBody|formData|comment_text!0",
        "content": "PLAIN",
        "context": "COMMENT"
      }
    ],
    "quora.com": [
      {
        "pattern": "*graphql/gql_POST*",
        "field": "variables|titlePlaintext",
        "content": "ENCODED",
        "context": "QUESTION"
      },
      {
        "pattern": "*graphql/gql_POST*",
        "field": "variables|content||sections!0|spans!0|text",
        "content": "DOUBLE_ENCODED",
        "context": "COMMENT"
      }
    ],
    "pinterest.co.uk": [
      {
        "pattern": "*resource/PinResource/create/*",
        "field": [
          "options|title",
          "options|description"
        ],
        "content": "JSON_STR",
        "data": "requestBody|formData|data",
        "context": "POST"
      }
    ],
    "pinterest.com": [
      {
        "pattern": "*resource/PinResource/create/*",
        "field": [
          "options|title",
          "options|description"
        ],
        "content": "JSON_STR",
        "data": "requestBody|formData|data",
        "context": "POST"
      }
    ],
    "in.pinterest.com": [
      {
        "pattern": "*resource/PinResource/create/*",
        "field": [
          "options|title",
          "options|description"
        ],
        "content": "JSON_STR",
        "data": "requestBody|formData|data",
        "context": "POST"
      }
    ],
    "tumblr.com": [
      {
        "pattern": "*/svc/post/update*",
        "field": [
          "post[one]",
          "post[two]",
          "post[tags]"
        ],
        "content": "ENCODED",
        "context": "POST"
      },
      {
        "pattern": "*/api/v2/blog/*/posts*",
        "field": [
          "content",
          "tags"
        ],
        "content": "ENCODED",
        "context": "POST"
      }
    ],
    "reddit.com": [
      {
        "pattern": "*api/comment*",
        "field": "richtext_json",
        "content": "ENCODED_STR",
        "context": "COMMENT"
      },
      {
        "pattern": "*api/submit*",
        "field": [
          "requestBody|formData|title!0",
          "requestBody|formData|richtext_json!0"
        ],
        "content": "PLAIN",
        "context": "POST"
      }
    ]
  },
  "bullyPhrases": [
    "U2FsdGVkX18OWYNqozHw5E7q+R9ZgL2CARA4M+mlI4v4G1G84uCl199T0FuT3lYD",
    "U2FsdGVkX1+H6a6wRIbNDxe1mcKmcv8hvY5/XzURPuV7PrR0n4g7BkUJpxnOTkhM",
    "U2FsdGVkX191ig2tlSoUdA/HYfnRI7zAt40RvDvHsQl4ExL0tJ38MyZ8bMWh8XLi",
    "U2FsdGVkX1+kk4pl9shqyN1B38LpBylywPxohK2iZun8xCSc22ds73v5sBsYS5DdhOFxZdU5cBraTGAfCt/Xqg==",
    "U2FsdGVkX1/IojW7InyMWatHrYfGe7XgclIP/gZbV6eQaYFr14GJkKURopanslkn",
    "U2FsdGVkX18TidyfNK7zzoaiM/s+/6O9NVNb896E5/04k+gjz659RDeKWy/rPrBl",
    "U2FsdGVkX1+RsyLsJxGc9wr/pR9ACxgloPb20O1Ygx0W9NM4THggyVpVblZtFzPs",
    "U2FsdGVkX1+b2Fv73ZG2srUF4bXb86/S0hUGk/HuqtE=",
    "U2FsdGVkX1/28/AGvMEEvQ8aAgc4tA2hmOJi/3hfeRQ=",
    "U2FsdGVkX18u9s2mW3qydY7uGDDox84XWRnjLk8KPT8=",
    "U2FsdGVkX18nsE8xBWZvyd7qFe5sDisEmXDCcRwrwD4I8fbZfcwrNHKHlYN6ttYE",
    "U2FsdGVkX1+9s3hBApfsQI/bZe/qaei7vWKg5UB3CXxt6zh9sjaCw+zg2dgCTLm/",
    "U2FsdGVkX18q3RLdhN7h1if+gjA2zm6h+YGr/UmhYSo=",
    "U2FsdGVkX1/sCNNWVC10RFKj+GoQGLn3Ju/kgjl11YKI7tN3vGMnHD7pZak5KXtg",
    "U2FsdGVkX1/ssJCxw2CaR3Dd+kLJ5BG8vDMrtCjPH9U=",
    "U2FsdGVkX1/Dj32djuBbqiuDjfHBVBTtkvxkPymYHBFnIpuajIpRqJeM5rdNu0Sn",
    "U2FsdGVkX19E9ayKlZ1G22rdIDlxEw3tMG6rq4SwU7Grfa5Cumvc3u58bsEWqqrq",
    "U2FsdGVkX1/rdE2D2DDmx+gLRE0DvbDZxAupE4CIKrD0/tnuxY2tCINQ0WzL/lD/",
    "U2FsdGVkX1/PHIRDxbbbgl5fxvOYM91ki4MjiNtQ04NzbVDDO9xNjqXTP4YBiorp",
    "U2FsdGVkX1+nHn3OAecgrawEb/BagjnVB485I4G1bEdX/wamccslgo3pzMowk4Ar",
    "U2FsdGVkX19u0YAZZ8xCdk0UCWJI27wMMm+bMbiZ/QE=",
    "U2FsdGVkX18zvKIAJ7mr0glyzq3IRUjyoYbQffGMwkyXizEGguc+hjpn3IuQnLhO",
    "U2FsdGVkX1+b/w6/Gd5gde7EKuK9soCLgsyYbxyxPYzDlMCVQkwWMMfh71GMrvk+2EM69NEA4FWag0qStdYIkw==",
    "U2FsdGVkX18sPuLSC6V2ne5ARTc2nfjTYjovjuxUvGDCAaTbnbf6ylU/kI8+dY5D",
    "U2FsdGVkX1+ijR+40bllYlpAsG/nwnEsbaJtyDmi1c8=",
    "U2FsdGVkX1/qaT9sHu7s5VC0oziqLgGDcQ/FdzE5YL4=",
    "U2FsdGVkX1+R+BHQUAztP8DSHgZ74TdQH8OLKkHEkT8=",
    "U2FsdGVkX1+D/+qnT/71PkaEk0ZHNTtYbugvj7tlWiw=",
    "U2FsdGVkX1/DzGVPX2q5dl6YANEohPHoGtfM9mIcqnXqG0NcpRk4Ta5JKX2Z4Hif",
    "U2FsdGVkX19ekvLzGdzqD83CJ404l31xSs477sc68zDFqQqAhe2JcknRb1gx/HiYfUKwLWRhfGphz0XiRseJig==",
    "U2FsdGVkX1/ofTxPpS5RScxj5uM/Vpf3IRlkGoFydqY=",
    "U2FsdGVkX1+hhWMLFdW+emdp2EwzTRB1UTWBAXZy4ILNeciK5nry7I4lKF8aKK9N",
    "U2FsdGVkX19okY9CkQPxtmRsBl8eBtD4dRWkaU6JcblieBVdCRJFNK6I74tsTzgn",
    "U2FsdGVkX19I/hoCyurv/rx/Yk5YXUs3GY774eEcIOaunvMDYiFQu/HnbVSwA712",
    "U2FsdGVkX1/gtFcwl+p8tlygjmWjksKYFkV9GfgOlBAY1KrGOgSWt6NNW+JB2Zzw",
    "U2FsdGVkX19Wa5hkvA8PoeBXw7hg8ZRAFffz6kh1ox31ATY19ZBWBFFYgX/O1LkR",
    "U2FsdGVkX19k58cdrmFHPf4VJGmvzuVzXWVxmcIw6Jgo7DI+Cg9RHE/Vim0mitVH",
    "U2FsdGVkX19H2GW08RgDdS73nqBOuVvhX1KLldFhjf2H6Zj4VzsfINyME8QPVvCY",
    "U2FsdGVkX18IX9ERG/RWKfybNF6vhi0Xu6KDF3MYops=",
    "U2FsdGVkX1/yeg/iCk4AVBjysIb43mjjerBVbXvoAKA=",
    "U2FsdGVkX18uU2d44v83fcSsaL/soGIaIponveNpDyg=",
    "U2FsdGVkX1+rz7HJhlopkrkrQXXfhYG6fykZ6C5rO+Hfw3XDVBMldGxkRBfr8yls",
    "U2FsdGVkX19nUlSRCs94Sp6ZgSyZ0Bw5g4KZ2xC77NHMuIN0UBbXr8Ja36io4Hq2",
    "U2FsdGVkX18wZg92/WTQ0OsSiJfnoXI2IkLuUeee3c8wErm2q+KULy3EmWEovQ/S",
    "U2FsdGVkX19Sx1nX2gMeC+pJ40VJ+evEk9M7WZf+GC8=",
    "U2FsdGVkX182ctcncihhKXTgpQb0MK4QbqIJpCIa63I=",
    "U2FsdGVkX1/1SNa9v+s/tLJx400CI6CwupNdhs4jRg8jC7nlMNmSelmZJNdKF84F",
    "U2FsdGVkX1+PH+EY0/LKSWlOVUo22LGm5NK7XaVYhDs=",
    "U2FsdGVkX1/BGwyLzP5Lv9/CrZYX9YWu3Rh0+H5jDE+ExZH7IlpDY8F4ewCx5lIJ",
    "U2FsdGVkX1/MbGdbKg+ayKRi5RVT/jJwBYMXz64CR0Q=",
    "U2FsdGVkX1/jy/F0pIqw1PUmYNYe7bTwJfN9OXjflQ2DDf/y+KAJrxrQThCAIkbV",
    "U2FsdGVkX1/d/gTAdGXS5Sp/w9XD4Ig9IZ3S52iRGXJRdxuIuluhuPTSHux2wl/s",
    "U2FsdGVkX1+1rwudR96P2KWRStu+rmsyFBDKeQfFdCIQDEv5gAczeDi0hJ1QqbUm",
    "U2FsdGVkX187QPIn18bOdHUlaFy25n4A8K6TIXW34BsrwBdMvp5oFWhH5Wz5JbYu",
    "U2FsdGVkX18/sqiY0EM8rygwYMXEQRMJCPAphqvBvyGJVSjYL/IpF2Ch14WYVC3T",
    "U2FsdGVkX1+ait2DaFibpqkFbIEaDxI0+h5Ou9LCHN3FztH4puwXzID4B2w/Lyd7",
    "U2FsdGVkX1+v9/l5O2oKt8AnR+YFUXwUnQeghQ+Dq2PzUguukIXsgE+lZrtTwZIX",
    "U2FsdGVkX1+DQkvz+v+Ow/TpVyluMGn3ccjlxIWLlAJepSYjxPRiPd8wgHMfz0Gq",
    "U2FsdGVkX18m1k9auldIH2tSQ1AWUL+bIoU8DDYK6l5j/G0cColWK4XxS8j05Dtb",
    "U2FsdGVkX1/JX6F0mZcDuuyAGiVVpro1J/XtAmif3vo=",
    "U2FsdGVkX18lY90WN6nHdWjuYZXJ75AnGNbLUCKg1l0=",
    "U2FsdGVkX19dhAi408BPjXGnZaasruBLTjCNZUjMzO7UPMmKcEi+fVu3WVXCnnFJ",
    "U2FsdGVkX19vJJ7BFmYxADoaRxrQRarsC6Y3vJReqraN6mPSX6moar3tP8QtJPvi",
    "U2FsdGVkX184fCEMwvgNAUUdf4BZ13Hn962XFFFkpxk=",
    "U2FsdGVkX185kGu2KhxOiwBSHAzRF+CXzkEumAXKfRI=",
    "U2FsdGVkX19b3XWsstSXs6Bi5dsgZZ3DzBaaY4OHSBg=",
    "U2FsdGVkX199dE5YywjNnDm84OEB9iEC3bUcNGIynyY=",
    "U2FsdGVkX18j71ZJn6tzeA60YvgjvwksIecrUqhEt9Q=",
    "U2FsdGVkX19h/OCL11/wne1G/bwKWE0wl1jfBytlDaw=",
    "U2FsdGVkX1/u072IrdxR6MXSdIFunzdXaK1y6AC50VE=",
    "U2FsdGVkX197n9G1Vtzp48FLxorJmJeqhftnLNuo17g=",
    "U2FsdGVkX18FA+fFIttSl1US8MQ1Cx5kk+i385NfsJg=",
    "U2FsdGVkX1+PvKI9jKBf/Sb0hE5SEDoem0JEjeYjXbQ=",
    "U2FsdGVkX19iyILItFkW+G43bpFDu39ze2etISEhrV4=",
    "U2FsdGVkX1/2jUFJB/eENhD+USOibywPQcoQD3CeiI8=",
    "U2FsdGVkX1/vMlohhwiDh19JbcT1bVbahEyeZezi86c=",
    "U2FsdGVkX1/GIcqEB9lwpxWF6gsd5r6V5n+4kketfBc=",
    "U2FsdGVkX1+572jChUmQVH5GcOEP/JYYJvZTi4JnkyA=",
    "U2FsdGVkX1+qPX9goZAxCVvPwA1KXw28Q7vCzh5v8II=",
    "U2FsdGVkX1+VbEr0TAv6v/HCODxRn+V/DlLe84ie/3A=",
    "U2FsdGVkX19dG6/huzLvod9AaYh5H1X3vzNAO5M7qOE=",
    "U2FsdGVkX1/IXBZkzGQslV5TES9GxDlcz0IlzVyohGs=",
    "U2FsdGVkX1+ua5ATM0TC5lgX2UlEwJ0oMmFPgVYSoW8=",
    "U2FsdGVkX1/DiwpjZTqypINkM6zRohHySlH9v6ss7xI=",
    "U2FsdGVkX1+FVCWj75a8na0ty+iauDd7QZaNRX3fMCM=",
    "U2FsdGVkX19IuvHmKtQzL4agdavosCPlcBrt3eQesTg=",
    "U2FsdGVkX18gZHFTUeoM1mPUSQfbpjdR9wNZa39MKgE=",
    "U2FsdGVkX1+XoR6wsS+wU0REA6RPNDpPJyjDbVt8QHE=",
    "U2FsdGVkX1/oIpxz8Uv3AMrel68d+0FaYMRrIN9SrsU=",
    "U2FsdGVkX1/2f7zMQh+Sxx7fDX/H1JoGRVyZaub6xfI=",
    "U2FsdGVkX1+7aM6WNFXRf+azgHlIwFDfHpeXH2dv4TU=",
    "U2FsdGVkX1+71+r4ukB8PKaQ2pT/MzerHpDQZQjVsm4=",
    "U2FsdGVkX1+KFv6QDH5IBDilPzXWQRr0PtFdW4cTTfg=",
    "U2FsdGVkX1/fnnhkfgFdM9VRl7SPF7rKGSlBLGRQEQI=",
    "U2FsdGVkX18xfkLuKhiS9KZqjKlOewWMw/HFqHA2Zdg=",
    "U2FsdGVkX18Kk4DDW/SwJ3f9hbWRgNBktaxDt0m5LS0=",
    "U2FsdGVkX1+0WH3LmaTZu2S1MPFamrr57JO1w5JdmqM=",
    "U2FsdGVkX1+JrqR58O9mHXbc6vQBbeuaHsWTUV4T3rQ=",
    "U2FsdGVkX1+PpZr/N+/vGkHbv2uufwwzgsPGKRHPClw=",
    "U2FsdGVkX1/wQBQNwYvrZEOd1teYKBun4kdpYC1z07E=",
    "U2FsdGVkX190iWuLBj//UBzC3Zym5tK38phroPVlmsY=",
    "U2FsdGVkX19YiznIIG+NfrZh+AnC6ndcLurVbqjxH0w=",
    "U2FsdGVkX18CaxIz5lIBbae57WCIt9+kh+zbNLa3E40=",
    "U2FsdGVkX19ir73QGl94ICq/nrwyduyAfXIm9OFYrZw=",
    "U2FsdGVkX19vivJoRmQMRIUk3O0n29wop7OYjUQB3Nc=",
    "U2FsdGVkX1/gVvE9/nkW3KAo7MeBv7SIE35xU2ISfjg=",
    "U2FsdGVkX182Jkx7Q4ZE+DupryGLFh7EaBO8UpKrslc=",
    "U2FsdGVkX1+HvtuwKLCOkv6MXPmiH00j/Tv6EulKVWc=",
    "U2FsdGVkX19/L5o/ciRrg2BEbZ6531TfABtrf52BO94=",
    "U2FsdGVkX1+n9alLgQEvw4DAMDLT1lzVRArMhndxdUU=",
    "U2FsdGVkX1/LMiTzUtezRSwQfGVuCcbDrqLum+KPe0I=",
    "U2FsdGVkX1+nE7VV7U2Knggnr6svHcvXc0gjRRY0WeM=",
    "U2FsdGVkX1+d6FpQxOup5pSGoQHXxSqjFn8F1Ch9p8E=",
    "U2FsdGVkX1+a24EyZbRqxcLm9RxcydRqkHj9pLAF15o=",
    "U2FsdGVkX18X7wzz6BySid10P2LqqAyitYXiu5cou+4=",
    "U2FsdGVkX1+a6CDe8fyUGKSqdxX2SX6kMaQtoYsQO04=",
    "U2FsdGVkX1+fngALE1Mz1OBvVpzB8hQWxBJRuzSxZW8=",
    "U2FsdGVkX1/9Dr3dslKjeOBlv6LJfqW9X6VGHyBDLgg=",
    "U2FsdGVkX1+P1Q5t/Q1sFRYXi1n5UmALJNhf5UpmzCs=",
    "U2FsdGVkX1+9DzXg2/9lx/U8T8OqFbjGugk+Epw7yLQ=",
    "U2FsdGVkX1+yC7SygqQjV1aN2esRYSDwdh+ZfubmcbU=",
    "U2FsdGVkX1+XtgLUOMDEAKjvCAhgEiorcWBrtT1CJUA=",
    "U2FsdGVkX19t+bJYZ28FBM1M3kHOMhwzMTMbzWVmXiA=",
    "U2FsdGVkX19fEBXpw/UQJ/vt7qy3gFF9alt6cCxgnBc=",
    "U2FsdGVkX19mQYPy54nkLzRYXzkb2BupISs4wOMybG0=",
    "U2FsdGVkX19X+ZuGvoxNm+2Og5sya8FaSS8w94xKbpg=",
    "U2FsdGVkX1+1BjpyE+kMNg5qJRaIhjfZeygW//xuCDyc/QKRWX+K3bZn/Uuzwi8G",
    "U2FsdGVkX1+Z+s8KP8NvbwCnkq0quMFxJ1frtFc0rlQ=",
    "U2FsdGVkX18399qHS7WJl0SiiR+8l4KHw8Keh3onQTg=",
    "U2FsdGVkX1/C3a4fqIbBuLUCALk/92d35eW3CWTP1lY=",
    "U2FsdGVkX18DNaxS4QP3dI6+soE8XCR6tAQJYQnm+YQ=",
    "U2FsdGVkX1/OX1d4R/A5DgGH8jqewGxwgpFIOR4Nsrw=",
    "U2FsdGVkX19/l7hAC2M+IqiQSn2wfeor4lpnjTaShmg=",
    "U2FsdGVkX19nOegMoR+bkwcETeup3LGHxwxyokWpvWA=",
    "U2FsdGVkX1/F34S0yLzNg5Wmz7xOYKPZZsJHJiZcabg=",
    "U2FsdGVkX18SaLyWwF7GDLJgACNd/hvrMqpcvjmrQNs=",
    "U2FsdGVkX18dyHekWHDz9uI7eg5Hjtj6X2+kKiqC1B8=",
    "U2FsdGVkX1+NLN8ZTIaGIIK6IMyJ0EqAu5z+GeVvDz0=",
    "U2FsdGVkX182LGS98sxSCHpHfsi5CpQejJm5YSUvSng=",
    "U2FsdGVkX19lTC+M3Q/BgCjzRWIadecXphNOD55o/24=",
    "U2FsdGVkX19bh4NLyWoL3sbGjdkjgmjiqKK0i7Pd0mY=",
    "U2FsdGVkX1/yengIIxX6e5dFkDm5NKpGRdt/oOsuLwM=",
    "U2FsdGVkX195+vKb8+xRbAESnrM97dBxKyDWSjnb+Mk=",
    "U2FsdGVkX1/P0bvkic0BWXoCY6ZfxVbdr9E9MUTznFE=",
    "U2FsdGVkX1/GR+5eNbcFnt8IbS3X+LyP0arykiMAh4Y=",
    "U2FsdGVkX19H9t2KZKMO16PoKuI7DEKgV9Ai97LoYCc=",
    "U2FsdGVkX19RxOcQjFjGGvv2XZV6udOdkSp5rYNcU10=",
    "U2FsdGVkX19+TzSC+EEmf+UYsasTzdEgxq8QiZCNjsw=",
    "U2FsdGVkX1+LkcRUIN5mFp1mNmnYG8LpvglqJJmlCyE=",
    "U2FsdGVkX1+YaYjSnPs/Sl/oi/k6ws4EhW/ZUXN5TgQ=",
    "U2FsdGVkX1/MtSvZn/dGlL7L+GsW8VT9VaGokYueJyU=",
    "U2FsdGVkX19t1vyiOS5p5ePmZwHkdDC3se96XXnQdvo=",
    "U2FsdGVkX180UAageBbWxduOYYAevGvLpdSYC+DrA6o=",
    "U2FsdGVkX18MfUvCKcD55fzC8fYPkVCcWcak6TxZ6iI=",
    "U2FsdGVkX1+9qVI2yy4nSeClAewOGlQ/nLrF38+H5lY=",
    "U2FsdGVkX19PtehwMqYaRfsZHbRLEQCNzE/N9dzA0Xg=",
    "U2FsdGVkX18NgZF0EE2vjZ7WRDBhPk8yKlSS6tOR5CkZAx1YjJsglnephGqs9frA",
    "U2FsdGVkX18kDwYNuvW9M4Bq+KduSdpHikxsVGdV9uLxntMsNU+K06tFgJhvjye0",
    "U2FsdGVkX1/PkxonDkfAGEECNog/60fAsgjXrcsM7JhWaiC4ezNVuf1MYQZFFWU3LRnCGYZ4VLCAwr7Yjx+3Vg==",
    "U2FsdGVkX1+r/g707SKkhbXxVCjvdAK6J+f4yGyq/OQ5js0cvbloDuq7FHFgHy6V",
    "U2FsdGVkX1+hhxOcbd3zZiRrTDSldlZg6kiIzZ0iS2ke9jwhpPnWAqiMbLBUHiJY",
    "U2FsdGVkX1/vjs46ooo0eFnG/jfT2v6EHooxfJpTm8lWtAXLrKqiUIRgIH/GSNWN",
    "U2FsdGVkX1/k4CiyH8pWWVSLolcGC+rfxL1qnB5BNsTlKqb1/3i1kAJSqagl/UzG",
    "U2FsdGVkX1+zFe3+iGY23FcSffjnM0lruaSdiNo4ra9oGUhJ9keKlExeLs2Opc8k",
    "U2FsdGVkX1/IadgrjWAQw6q3svCTGQqYr/pFROKB3qa1cIRiQX674OColHGxO2Be",
    "U2FsdGVkX1+ZYg/5DucirSBwKmXxXTdNHD5AphRX73g=",
    "U2FsdGVkX1/4EziPrBeH69695ySA8B3Tz21H/y7crQ8=",
    "U2FsdGVkX1+S3RwIPzKwNka83o+h84J5z4bdAdOmhtI=",
    "U2FsdGVkX180SFGTS0UseBHSJaC38bZF4rC6rCmBX/Q=",
    "U2FsdGVkX18WP2Z7ahmPk6VO1Le2b9LwKtCeULjrhls=",
    "U2FsdGVkX193UJWmcHic7RcLSwvInnHYOM18wcVrO/8=",
    "U2FsdGVkX19aq7G9UhG/gfu/DWzoOsNvANvw7ToeUk4=",
    "U2FsdGVkX1/EckygS5G0PZoVB+JqU9XKXBRCEldQre0=",
    "U2FsdGVkX183ruraMyal6zOE23Hw7q+wDepzaOu7DV4=",
    "U2FsdGVkX198S1qddz42YQHegjKF2NNRN5avaZE/UHs=",
    "U2FsdGVkX18hE7WXm1MNRnYC2euq93anWPeJE5AGFrM=",
    "U2FsdGVkX1+NL2FDnKYNcQRuJPJFVDNVwEA0AHxh6xM=",
    "U2FsdGVkX1+coOKeaeEnKvq2DMWoHQMao5tCe/5v7Wk=",
    "U2FsdGVkX1/AaWVt40uBqDGsAjavecdLUF88lacv/Mk="
  ],
  "wlBullyPhrases": [
    "U2FsdGVkX1/dBxHVCFaSyt/EWE+pfDWdo+9/5vqMwmo=",
    "U2FsdGVkX1/YXjiWwsS2jOOhiWvBBJm/PXMkSO7Rm30="
  ],
  "thinkTwiceSites": [
    "reddit.com",
    "tumblr.com",
    "pinterest.com",
    "in.pinterest.com",
    "pinterest.co.uk",
    "quora.com",
    "instagram.com",
    "facebook.com",
    "twitter.com",
    "mail.google.com",
    "chat.google.com",
    "tiktok.com",
    "youtube.com",
    "discord.com"
  ],
  "thirdPartySSO": [
    "sts.platform.rmunify.com",
    "smartlogin.realsmart.co.uk"
  ]
}

5 AI Music Apps You Can Use as a Melody Generator
AI Lyrics, Music, and Song Creation Apps | AUDOIR, LLC
Home - Amper Score
https://futureai.guru/technologies/brian-simulator-ii-open-source-agi-toolkit/
https://futureaiguru.com/BrainSimDownload.aspx
https://www.betabound.com/depix-private-beta/
https://github.com/hostedposted/BlooketHackGUI
https://github.com/hostedposted/jackbox
https://github.com/hostedposted/Blooket-Hacks
https://imageupscaler.com/ai-image-generator/


5 AI Music Apps You Can Use as a Melody Generator

AI Lyrics, Music, and Song Creation Apps | AUDOIR, LLC

Home - Amper Score

https://futureai.guru/technologies/brian-simulator-ii-open-source-agi-toolkit/

https://futureaiguru.com/BrainSimDownload.aspx

https://www.betabound.com/depix-private-beta/

https://github.com/hostedposted/BlooketHackGUI

https://github.com/hostedposted/jackbox

https://github.com/hostedposted/Blooket-Hacks

https://imageupscaler.com/ai-image-generator/


https://imageupscaler.com/ai-image-generator/


Doctor Who, God, Undertale, Megalovania, Genshin Impact, Sans, The Doctor, Aether, Fortnite, Battle Royale, FNBR, Fortnite Battle Royale, Zero, Point, Zero Point, Fortnite Zero Point, Nexus, Celestial, Galactic, Galaxy, GeÃ±o, Black Hole, Time Vortex, SuperNova, Super Nova, Rift, Rift Butterfly, Energy, Zero Point Energy, ZPE, Wormhole, Vortex, Cube, Cube Energy, Kevin, Kevin The Cube, Dark, Darkness, Dark Energy, Darkness Energy, The Last Reality, TLR, Zero Bloom, Bloom, Reality, Reality Tree, Chroma, Flame, Chroma Flame, Chroma Energy, Dark Matter, Matter, Irminsul, Irminsul Tree, Irminsul Branch, Irminsul Tree Branch, Irminsul Roots, Irminsul Root, Irminsul Tree Roots, Irminsul Tree Root, Ley Line, Ley, Music, Audio, MP3, mp3, WAV, wav, Image, Picture, JPG, Midi instrument, Midi File MID, mid, MIDI, midi, Sci-Fi, Science, Space, AI, Technology
Doctor Who, God, Undertale, Megalovania, Genshin Impact, Sans, The Doctor, Aether, Fortnite, Battle Royale, FNBR, Fortnite Battle Royale, Zero, Point, Zero Point, Fortnite Zero Point, Nexus, Celestial, Galactic, Galaxy, GeÃ±o, Black Hole, Time Vortex, SuperNova, Super Nova, Rift, Rift Butterfly, Energy, Zero Point Energy, ZPE, Wormhole, Vortex, Cube, Cube Energy, Kevin, Kevin The Cube, Dark, Darkness, Dark Energy, Darkness Energy, The Last Reality, TLR, Zero Bloom, Bloom, Reality, Reality Tree, Chroma, Flame, Chroma Flame, Chroma Energy, Dark Matter, Matter, Irminsul, Irminsul Tree, Irminsul Branch, Irminsul Tree Branch, Irminsul Roots, Irminsul Root, Irminsul Tree Roots, Irminsul Tree Root, Ley Line, Ley, Music, Audio, MP3, mp3, WAV, wav, Image, Picture, JPG, Midi instrument, Midi File MID, mid, MIDI, midi, Sci-Fi, Science, Space, AI, Technology.jpg

Doctor Who, God, Undertale, Megalovania, Genshin Impact, Sans, The Doctor, Aether, Fortnite, Battle Royale, FNBR, Fortnite Battle Royale, Zero, Point, Zero Point, Fortnite Zero Point, Nexus, Celestial, Galactic, Galaxy, GeÃ±o, Black Hole, Time Vortex, SuperNova, Super Nova, Rift, Rift Butterfly, Energy, Zero Point Energy, ZPE, Wormhole, Vortex, Cube, Cube Energy, Kevin, Kevin The Cube, Dark, Darkness, Dark Energy, Darkness Energy, The Last Reality, TLR, Zero Bloom, Bloom, Reality, Reality Tree, Chroma, Flame, Chroma Flame, Chroma Energy, Dark Matter, Matter, Irminsul, Irminsul Tree, Irminsul Branch, Irminsul Tree Branch, Irminsul Roots, Irminsul Root, Irminsul Tree Roots, Irminsul Tree Root, Ley Line, Ley, Music, Audio, MP3, mp3, WAV, wav, Image, Picture, JPG, Midi instrument, Midi File MID, mid, MIDI, midi, Sci-Fi, Science, Space, AI, Technology

Doctor Who, God, Undertale, Megalovania, Genshin Impact, Sans, The Doctor, Aether, Fortnite, Battle Royale, FNBR, Fortnite Battle Royale, Zero, Point, Zero Point, Fortnite Zero Point, Nexus, Celestial, Galactic, Galaxy, GeÃ±o, Black Hole, Time Vortex, SuperNova, Super Nova, Rift, Rift Butterfly, Energy, Zero Point Energy, ZPE, Wormhole, Vortex, Cube, Cube Energy, Kevin, Kevin The Cube, Dark, Darkness, Dark Energy, Darkness Energy, The Last Reality, TLR, Zero Bloom, Bloom, Reality, Reality Tree, Chroma, Flame, Chroma Flame, Chroma Energy, Dark Matter, Matter, Irminsul, Irminsul Tree, Irminsul Branch, Irminsul Tree Branch, Irminsul Roots, Irminsul Root, Irminsul Tree Roots, Irminsul Tree Root, Ley Line, Ley, Music, Audio, MP3, mp3, WAV, wav, Image, Picture, JPG, Midi instrument, Midi File MID, mid, MIDI, midi, Sci-Fi, Science, Space, AI, Technology.jpg

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated_github/src/xgboost_automated_github.py

---------------------------------------------------------------------------
DataApiError                              Traceback (most recent call last)
~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in &lt;module&gt;
     58 
     59 config = load_model_config()
---&gt; 60 xgb_path, xgb_obj = load_model(config)
     61 assert_model_md5(xgb_path)
     62 

~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in load_model(config)
     35     &quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;
     36     model_path = config[&quot;model_filepath&quot;]
---&gt; 37     model_file = client.file(model_path).getFile().name
     38     model_obj = joblib.load(model_file)
     39     return model_file, model_obj

~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py in getFile(self)
     35         exists, error = self.existsWithError()
     36         if not exists:
---&gt; 37             raise DataApiError(&#39;unable to get file {} - {}&#39;.format(self.path, error))
     38         # Make HTTP get request
     39         response = self.client.getHelper(self.url)

DataApiError: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated/src/xgboost_automated.py

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train, evaluate and test XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save XGBoost model to a file\n",
    "We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"model.pkl\", compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test our serving (Algorithm) code\n",
    "\n",
    "We will now test our algorithm code, ie. **`xgboost_automated_github.py`** script, by simply executing it with the `%run` macro. The script will begin its execution through our `if __name__ == \"__main__\"` line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "DataApiError",
     "evalue": "unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataApiError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 60\u001b[0;31m \u001b[0mxgb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0massert_model_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m&quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m&quot;model_filepath&quot;\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 37\u001b[0;31m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mmodel_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py\u001b[0m in \u001b[0;36mgetFile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mexists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexistsWithError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 37\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m&#39;unable to get file {} - {}&#39;\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Make HTTP get request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataApiError\u001b[0m: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required"
     ]
    }
   ],
   "source": [
    "%run xgboost_automated_github/src/xgboost_automated_github.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks before committing this notebook\n",
    "\n",
    "- Make sure `xgboost_automated_github.py` file is in good shape to be pushed to Algorithmia.\n",
    "\n",
    "- Make sure Algorithmia API Key is not committed and pushed in any file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


Sentiment Analysis with XGBoost on Algorithmia
1. Train, evaluate and test XGBoost model
In [ ]:
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib
Load the training data

Let's load our training data, take a look at a few rows and one of the review texts in detail.

In [ ]:
data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()
In [ ]:
data["reviewText"].iloc[1]
Preprocessing

Time to process our texts! Basically, we'll:

Remove the English stopwords
Remove punctuations
Drop unused columns
In [ ]:
import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)
In [ ]:
preprocess_reviews(data)
data.head()
Split our training and test sets
In [ ]:
rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)
Mini randomized search

Let's set up a very basic cross-validated randomized search over parameter settings.

In [ ]:
params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)
Pipeline to vectorize, transform and fit

Time to vectorize our data, transform it and then fit our model to it. To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us.

In [ ]:
model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)
Predict and calculate accuracy
In [ ]:
predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")
2. Save XGBoost model to a file

We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia.

In [ ]:
joblib.dump(model, "model.pkl", compress=True)
3. Test our serving (Algorithm) code

We will now test our algorithm code, ie. xgboost_automated_github.py script, by simply executing it with the %run macro. The script will begin its execution through our if __name__ == "__main__" line.

In [1]:
%run xgboost_automated_github/src/xgboost_automated_github.py
---------------------------------------------------------------------------
DataApiError                              Traceback (most recent call last)
~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in &lt;module&gt;
     58 
     59 config = load_model_config()
---&gt; 60 xgb_path, xgb_obj = load_model(config)
     61 assert_model_md5(xgb_path)
     62 

~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in load_model(config)
     35     &quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;
     36     model_path = config[&quot;model_filepath&quot;]
---&gt; 37     model_file = client.file(model_path).getFile().name
     38     model_obj = joblib.load(model_file)
     39     return model_file, model_obj

~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py in getFile(self)
     35         exists, error = self.existsWithError()
     36         if not exists:
---&gt; 37             raise DataApiError(&#39;unable to get file {} - {}&#39;.format(self.path, error))
     38         # Make HTTP get request
     39         response = self.client.getHelper(self.url)

DataApiError: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required
Final checks before committing this notebook

Make sure xgboost_automated_github.py file is in good shape to be pushed to Algorithmia.

Make sure Algorithmia API Key is not committed and pushed in any file!

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated_github/src/xgboost_automated_github.py

---------------------------------------------------------------------------
DataApiError                              Traceback (most recent call last)
~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in &lt;module&gt;
     58 
     59 config = load_model_config()
---&gt; 60 xgb_path, xgb_obj = load_model(config)
     61 assert_model_md5(xgb_path)
     62 

~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in load_model(config)
     35     &quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;
     36     model_path = config[&quot;model_filepath&quot;]
---&gt; 37     model_file = client.file(model_path).getFile().name
     38     model_obj = joblib.load(model_file)
     39     return model_file, model_obj

~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py in getFile(self)
     35         exists, error = self.existsWithError()
     36         if not exists:
---&gt; 37             raise DataApiError(&#39;unable to get file {} - {}&#39;.format(self.path, error))
     38         # Make HTTP get request
     39         response = self.client.getHelper(self.url)

DataApiError: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train, evaluate and test XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save XGBoost model to a file\n",
    "We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"model.pkl\", compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test our serving (Algorithm) code\n",
    "\n",
    "We will now test our algorithm code, ie. **`xgboost_automated.py`** script, by simply executing it with the `%run` macro. The script will begin its execution through our `if __name__ == \"__main__\"` line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run xgboost_automated/src/xgboost_automated.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks before committing this notebook\n",
    "\n",
    "- Make sure `xgboost_automated.py` file is in good shape to be pushed to Algorithmia.\n",
    "\n",
    "- Make sure Algorithmia API Key is not committed and pushed in any file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

Sentiment Analysis with XGBoost on Algorithmia
1. Train, evaluate and test XGBoost model
In [ ]:
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib
Load the training data

Let's load our training data, take a look at a few rows and one of the review texts in detail.

In [ ]:
data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()
In [ ]:
data["reviewText"].iloc[1]
Preprocessing

Time to process our texts! Basically, we'll:

Remove the English stopwords
Remove punctuations
Drop unused columns
In [ ]:
import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)
In [ ]:
preprocess_reviews(data)
data.head()
Split our training and test sets
In [ ]:
rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)
Mini randomized search

Let's set up a very basic cross-validated randomized search over parameter settings.

In [ ]:
params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)
Pipeline to vectorize, transform and fit

Time to vectorize our data, transform it and then fit our model to it. To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us.

In [ ]:
model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)
Predict and calculate accuracy
In [ ]:
predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")
2. Save XGBoost model to a file

We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia.

In [ ]:
joblib.dump(model, "model.pkl", compress=True)
3. Test our serving (Algorithm) code

We will now test our algorithm code, ie. xgboost_automated.py script, by simply executing it with the %run macro. The script will begin its execution through our if __name__ == "__main__" line.

In [ ]:
%run xgboost_automated/src/xgboost_automated.py
Final checks before committing this notebook

Make sure xgboost_automated.py file is in good shape to be pushed to Algorithmia.

Make sure Algorithmia API Key is not committed and pushed in any file!

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated/src/xgboost_automated.py

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated_github/src/xgboost_automated_github.py

---------------------------------------------------------------------------
DataApiError                              Traceback (most recent call last)
~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in &lt;module&gt;
     58 
     59 config = load_model_config()
---&gt; 60 xgb_path, xgb_obj = load_model(config)
     61 assert_model_md5(xgb_path)
     62 

~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in load_model(config)
     35     &quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;
     36     model_path = config[&quot;model_filepath&quot;]
---&gt; 37     model_file = client.file(model_path).getFile().name
     38     model_obj = joblib.load(model_file)
     39     return model_file, model_obj

~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py in getFile(self)
     35         exists, error = self.existsWithError()
     36         if not exists:
---&gt; 37             raise DataApiError(&#39;unable to get file {} - {}&#39;.format(self.path, error))
     38         # Make HTTP get request
     39         response = self.client.getHelper(self.url)

DataApiError: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated/src/xgboost_automated.py
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train, evaluate and test XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save XGBoost model to a file\n",
    "We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"model.pkl\", compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test our serving (Algorithm) code\n",
    "\n",
    "We will now test our algorithm code, ie. **`xgboost_automated_github.py`** script, by simply executing it with the `%run` macro. The script will begin its execution through our `if __name__ == \"__main__\"` line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "DataApiError",
     "evalue": "unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataApiError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 60\u001b[0;31m \u001b[0mxgb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0massert_model_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m&quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m&quot;model_filepath&quot;\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 37\u001b[0;31m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mmodel_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py\u001b[0m in \u001b[0;36mgetFile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mexists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexistsWithError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 37\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m&#39;unable to get file {} - {}&#39;\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Make HTTP get request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataApiError\u001b[0m: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required"
     ]
    }
   ],
   "source": [
    "%run xgboost_automated_github/src/xgboost_automated_github.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks before committing this notebook\n",
    "\n",
    "- Make sure `xgboost_automated_github.py` file is in good shape to be pushed to Algorithmia.\n",
    "\n",
    "- Make sure Algorithmia API Key is not committed and pushed in any file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


Sentiment Analysis with XGBoost on Algorithmia
1. Train, evaluate and test XGBoost model
In [ ]:
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib
Load the training data

Let's load our training data, take a look at a few rows and one of the review texts in detail.

In [ ]:
data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()
In [ ]:
data["reviewText"].iloc[1]
Preprocessing

Time to process our texts! Basically, we'll:

Remove the English stopwords
Remove punctuations
Drop unused columns
In [ ]:
import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)
In [ ]:
preprocess_reviews(data)
data.head()
Split our training and test sets
In [ ]:
rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)
Mini randomized search

Let's set up a very basic cross-validated randomized search over parameter settings.

In [ ]:
params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)
Pipeline to vectorize, transform and fit

Time to vectorize our data, transform it and then fit our model to it. To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us.

In [ ]:
model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)
Predict and calculate accuracy
In [ ]:
predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")
2. Save XGBoost model to a file

We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia.

In [ ]:
joblib.dump(model, "model.pkl", compress=True)
3. Test our serving (Algorithm) code

We will now test our algorithm code, ie. xgboost_automated_github.py script, by simply executing it with the %run macro. The script will begin its execution through our if __name__ == "__main__" line.

In [1]:
%run xgboost_automated_github/src/xgboost_automated_github.py
---------------------------------------------------------------------------
DataApiError                              Traceback (most recent call last)
~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in &lt;module&gt;
     58 
     59 config = load_model_config()
---&gt; 60 xgb_path, xgb_obj = load_model(config)
     61 assert_model_md5(xgb_path)
     62 

~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in load_model(config)
     35     &quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;
     36     model_path = config[&quot;model_filepath&quot;]
---&gt; 37     model_file = client.file(model_path).getFile().name
     38     model_obj = joblib.load(model_file)
     39     return model_file, model_obj

~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py in getFile(self)
     35         exists, error = self.existsWithError()
     36         if not exists:
---&gt; 37             raise DataApiError(&#39;unable to get file {} - {}&#39;.format(self.path, error))
     38         # Make HTTP get request
     39         response = self.client.getHelper(self.url)

DataApiError: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required
Final checks before committing this notebook

Make sure xgboost_automated_github.py file is in good shape to be pushed to Algorithmia.

Make sure Algorithmia API Key is not committed and pushed in any file!

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated_github/src/xgboost_automated_github.py

---------------------------------------------------------------------------
DataApiError                              Traceback (most recent call last)
~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in &lt;module&gt;
     58 
     59 config = load_model_config()
---&gt; 60 xgb_path, xgb_obj = load_model(config)
     61 assert_model_md5(xgb_path)
     62 

~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in load_model(config)
     35     &quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;
     36     model_path = config[&quot;model_filepath&quot;]
---&gt; 37     model_file = client.file(model_path).getFile().name
     38     model_obj = joblib.load(model_file)
     39     return model_file, model_obj

~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py in getFile(self)
     35         exists, error = self.existsWithError()
     36         if not exists:
---&gt; 37             raise DataApiError(&#39;unable to get file {} - {}&#39;.format(self.path, error))
     38         # Make HTTP get request
     39         response = self.client.getHelper(self.url)

DataApiError: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train, evaluate and test XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save XGBoost model to a file\n",
    "We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"model.pkl\", compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test our serving (Algorithm) code\n",
    "\n",
    "We will now test our algorithm code, ie. **`xgboost_automated.py`** script, by simply executing it with the `%run` macro. The script will begin its execution through our `if __name__ == \"__main__\"` line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run xgboost_automated/src/xgboost_automated.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks before committing this notebook\n",
    "\n",
    "- Make sure `xgboost_automated.py` file is in good shape to be pushed to Algorithmia.\n",
    "\n",
    "- Make sure Algorithmia API Key is not committed and pushed in any file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

Sentiment Analysis with XGBoost on Algorithmia
1. Train, evaluate and test XGBoost model
In [ ]:
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib
Load the training data

Let's load our training data, take a look at a few rows and one of the review texts in detail.

In [ ]:
data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()
In [ ]:
data["reviewText"].iloc[1]
Preprocessing

Time to process our texts! Basically, we'll:

Remove the English stopwords
Remove punctuations
Drop unused columns
In [ ]:
import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)
In [ ]:
preprocess_reviews(data)
data.head()
Split our training and test sets
In [ ]:
rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)
Mini randomized search

Let's set up a very basic cross-validated randomized search over parameter settings.

In [ ]:
params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)
Pipeline to vectorize, transform and fit

Time to vectorize our data, transform it and then fit our model to it. To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us.

In [ ]:
model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)
Predict and calculate accuracy
In [ ]:
predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")
2. Save XGBoost model to a file

We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia.

In [ ]:
joblib.dump(model, "model.pkl", compress=True)
3. Test our serving (Algorithm) code

We will now test our algorithm code, ie. xgboost_automated.py script, by simply executing it with the %run macro. The script will begin its execution through our if __name__ == "__main__" line.

In [ ]:
%run xgboost_automated/src/xgboost_automated.py
Final checks before committing this notebook

Make sure xgboost_automated.py file is in good shape to be pushed to Algorithmia.

Make sure Algorithmia API Key is not committed and pushed in any file!

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated/src/xgboost_automated.py

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated_github/src/xgboost_automated_github.py

---------------------------------------------------------------------------
DataApiError                              Traceback (most recent call last)
~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in &lt;module&gt;
     58 
     59 config = load_model_config()
---&gt; 60 xgb_path, xgb_obj = load_model(config)
     61 assert_model_md5(xgb_path)
     62 

~/repos/algorithmia_ci/demo_autodeploy_algo_on_github/xgboost_automated_github.py in load_model(config)
     35     &quot;&quot;&quot;Loads the model object from the file at model_filepath key in config dict&quot;&quot;&quot;
     36     model_path = config[&quot;model_filepath&quot;]
---&gt; 37     model_file = client.file(model_path).getFile().name
     38     model_obj = joblib.load(model_file)
     39     return model_file, model_obj

~/opt/anaconda3/lib/python3.8/site-packages/Algorithmia/datafile.py in getFile(self)
     35         exists, error = self.existsWithError()
     36         if not exists:
---&gt; 37             raise DataApiError(&#39;unable to get file {} - {}&#39;.format(self.path, error))
     38         # Make HTTP get request
     39         response = self.client.getHelper(self.url)

DataApiError: unable to get file asli/xgboost_automated_github/model_1efcc5e311207268e450d6018debe7012f182e71.pkl - authorization required

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

from string import punctuation
from nltk.corpus import stopwords

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import joblib

data = pd.read_csv("./data/amazon_musical_reviews/Musical_instruments_reviews.csv")
data.head()

data["reviewText"].iloc[1]

import nltk
nltk.download('stopwords')

def threshold_ratings(data):
    def threshold_overall_rating(rating):
        return 0 if int(rating)<=3 else 1
    data["overall"] = data["overall"].apply(threshold_overall_rating)

def remove_stopwords_punctuation(data):
    data["review"] = data["reviewText"] + data["summary"]

    puncs = list(punctuation)
    stops = stopwords.words("english")

    def remove_stopwords_in_str(input_str):
        filtered = [char for char in str(input_str).split() if char not in stops]
        return ' '.join(filtered)

    def remove_punc_in_str(input_str):
        filtered = [char for char in input_str if char not in puncs]
        return ''.join(filtered)

    def remove_stopwords_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_stopwords_in_str(input_series[i]))
        return text_clean

    def remove_punc_in_series(input_series):
        text_clean = []
        for i in range(len(input_series)):
            text_clean.append(remove_punc_in_str(input_series[i]))
        return text_clean

    data["review"] = remove_stopwords_in_series(data["review"].str.lower())
    data["review"] = remove_punc_in_series(data["review"].str.lower())

def drop_unused_colums(data):
    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', "reviewText", "summary"], axis=1, inplace=True)

def preprocess_reviews(data):
    remove_stopwords_punctuation(data)
    threshold_ratings(data)
    drop_unused_colums(data)

preprocess_reviews(data)
data.head()

rand_seed = 42
X = data["review"]
y = data["overall"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)

params = {"max_depth": range(9,12), "min_child_weight": range(5,8)}
rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)

model  = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('model', rand_search_cv)
])
model.fit(X_train, y_train)

predictions = model.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {round(acc * 100, 2)}")

joblib.dump(model, "model.pkl", compress=True)

%run xgboost_automated/src/xgboost_automated.py

<iframe width="638" height="479" src="https://www.youtube.com/embed/W-0waOsd8Ho" title="Zero Point Sans(final form)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><iframe width="638" height="479" src="https://www.youtube.com/embed/W-0waOsd8Ho" title="Zero Point Sans(final form)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

PK
ï¿½ï¿½T" MIDI.band/Caches.nosync/_cacheInfoUT
ï¿½ï¿½bï¿½ï¿½bfcuxï¿½ï¿½Uï¿½Qkï¿½0ï¿½ï¿½ï¿½Hï¿½o][ï¿½%Mï¿½IA6a:Ø£$aï¿½Y
&ï¿½ï¿½ï¿½/ï¿½{ï¿½ï¿½ï¿½ï¿½Î¡ï¿½ï¿½kï¿½ï¿½ï¿½dï¿½q8ï¿½(\cï¿½9ï¿½nï¿½8ï¿½ï¿½Iï¿½ï¿½,ï¿½+ï¿½ï¿½ï¿½ï¿½ï¿½Lï¿½Yï¿½ï¿½ï¿½Taï¿½ï¿½kQTï¿½ï¿½kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½8gï¿½ï¿½<ï¿½ï¿½Bï¿½rï¿½.ï¿½ï¿½jï¿½ï¿½Ü­ï¿½2ï¿½ï¿½r
ï¿½ï¿½ï¿½ï¿½ï¿½Uï¿½t,xï¿½ï¿½ï¿½ï¿½d+/Ú¾ï¿½}ï¿½Zç¡¦)ï¿½ï¿½ï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½&ï¿½NOï¿½ï¿½hï¿½ï¿½-ï¿½ï¿½$ï¿½f)ï¿½7Lï¿½$ï¿½v1ï¿½?8ï¿½pï¿½Sï¿½mgï¿½PK ï¿½Jï¿½PK
ï¿½ï¿½Tï¿½ï¿½	 MIDI.band/projectDataUT
ï¿½ï¿½bï¿½ï¿½bfcuxï¿½ï¿½ï¿½ï¿½vï¿½È–(ï¿½\ï¿½+ï¿½İ£ï¿½|jï¿½Ö†;ï¿½ï¿½3"ï¿½$ 	ï¿½ï¿½F46_cFï¿½	pî•«ï¿½Jï¿½ï¿½e4ï¿½ï¿½"ï¿½ï¿½ï¿½{Zï¿½;Î¾ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6Û¸ï¿½ï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½nï¿½tï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ğ·®‰ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½ï¿½ï¿½ï¿½ï¿½+?ï¿½ï¿½ï¿½m+ï¿½ï¿½ï¿½ï¿½3^_ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½Aï¿½cwï¿½ï¿½İ®ï¿½ï¿½ï¿½kï¿½kï¿½ï¿½}ï¿½mï¿½ï¿½ ï¿½ï¿½ï¿½'Ã°ï¿½gï¿½C>ï¿½.ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½jvï¿½ï¿½ï¿½ï¿½ï¿½ãŸ¯ï¿½'ï¿½tï¿½"ï¿½ï¿½Oï¿½ßšï¿½gS~ï¿½m'Ë™{ï¿½ï¿½`ã¯¯1ï¿½?ï¿½ï¿½7ß¾ï¿½ï¿½ï¿½Xlï¿½ï¿½wï¿½ï¿½ï¿½ï¿½]ï¿½ï¿½ï¿½ï¿½3\ï¿½z;Yï¿½3oï¿½5ï¿½fï¿½ï¿½fï¿½wwï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½?ï¿½ï¿½ï¿½L;ï¿½ï¿½7>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½fï¿½ï¿½ï¿½?ï¿½ï¿½|ï¿½~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½tï¿½ï¿½ï¿½ï¿½Ûï¿½Gï¿½y/'ï¿½Û¯ZSï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½8ï¿½dï¿½'ï¿½M)}ï¿½ï¿½&ï¿½Qï¿½ï¿½ï¿½Ô­jØ£ï¿½Qï¿½~4
ï¿½ï¿½.ï¿½ï¿½ï¿½ï¿½
#ï¿½ï¿½ï¿½ï¿½ï¿½)!ï¿½Cï¿½e ï¿½Cï¿½.Ã„ï¿½0B^ï¿½ï¿½8ï¿½î³°ï¿½ï¿½ï¿½848)ï¿½}ï¿½kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½{nï¿½ï¿½Fpflï¿½ï¿½lï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½qï¿½Eï¿½M>ï¿½iï¿½ï¿½X:ï¿½ï¿½ï¿½)w3ï¿½ï¿½ï¿½3;Or{ï¿½Ü†ï¿½ï¿½dhï¿½dï¿½ï¿½ï¿½/2ï¿½s;F?ï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½Qï¿½ï¿½ï¿½Hï¿½ï¿½Oï¿½ ï¿½ï¿½zï¿½ï¿½#8rï¿½+|;N2
ï¿½ï¿½gï¿½ï¿½Ohï¿½ï¿½ï¿½&ï¿½Wï¿½+ï¿½cp5Nï¿½ï¿½ï¿½Iï¿½ï¿½hGï¿½Pï¿½ï¿½ï¿½;}ï¿½nï¿½Ã¯vï¿½ï¿½ï¿½Sï¿½8@b.ï¿½$ï¿½ï¿½yï¿½uï¿½'Ö€-ï¿½ï¿½Xï¿½rvï¿½ï¿½ï¿½Oï¿½=ï¿½ï¿½*{{Cï¿½ï¿½ï¿½ï¿½#ï¿½^ï¿½ï¿½ï¿½ï¿½~ï¿½xï¿½K@ï¿½ï¿½ï¿½.ï¿½ï¿½ï¿½Bï¿½oï¿½ï¿½'Dp*ï¿½Qï¿½yTjï¿½ï¿½cï¿½ï¿½!-uï¿½Û²ï¿½ï¿½V/0#ï¿½hï¿½ï¿½!ï¿½#ï¿½Â…ï¿½ï¿½Z
ï¿½h}ï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½L.<ï¿½ï¿½)Wï¿½ï¿½ï¿½ï¿½ï¿½iï¿½ï¿½=A<'ï¿½ï¿½ï¿½ï¿½
Öœï¿½ï¿½ï¿½ï¿½9ï¿½
ï¿½ï¿½ï¿½cï¿½ï¿½Aï¿½ï¿½w|]ï¿½#ï¿½}ï¿½ï¿½ï¿½ÏŠï¿½Vfï¿½twtï¿½4ï¿½ï¿½ï¿½^ï¿½ï¿½ï¿½tCï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½/ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½WWï¿½Qï¿½ï¿½|jï¿½xï¿½l<%ï¿½aï¿½ï¿½%ï¿½ï¿½-p"5ï¿½ï¿½Fï¿½Nb?Æ­?4Şï¿½/ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½ï¿½ï¿½1wï¿½\ï¿½ï¿½ï¿½
ï¿½ï¿½_/ï¿½ï¿½4mÏ¶ï¿½ï¿½D^1ï¿½X<ï¿½ï¿½ï¿½?0O|Yï¿½Nï¿½l^ï¿½ï¿½ï¿½Sï¿½ï¿½P;1+ï¿½(HLLï¿½CØšlzï¿½ï¿½tï¿½GwFUDï¿½à¦’_ï¿½Hï¿½
Iï¿½oï¿½Mï¿½È£ï¿½ï¿½Wï¿½ï¿½ï¿½;ï¿½q@ï¿½$^nlï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bï¿½A8rz-
&Eï¿½IH*yï¿½/gï¿½ï¿½ï¿½ï¿½ï¿½<ï¿½oï¿½ï¿½Kï¿½ï¿½ï¿½ï¿½ï¿½HLï¿½V(zfï¿½Q~"ï¿½]ï¿½ï¿½F(ï¿½qï¿½GYVï¿½'ï¿½]:]ç«¥Å¹ï¿½NI	LW@ï¿½ï¿½Z\2+uï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½tï¿½kJqz3Lï¿½<ï¿½ï¿½ï¿½TU_uï¿½ï¿½_ï¿½gï¿½ï¿½Siï¿½G}<

ï¿½ï¿½ï¿½``()ï¿½ï¿½dï¿½Zï¿½b|ï¿½=-ï¿½=EYvï¿½4ï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ò²ï¿½ï¿½ï¿½ï¿½Gï¿½Bï¿½ï¿½ï¿½ï¿½5ï¿½{~~Hï¿½Í«Opï¿½ï¿½ï¿½ĞŸï¿½&kqï¿½gHZï¿½ï¿½ï¿½ï¿½ï¿½Kï¿½;ï¿½ï¿½ï¿½pï¿½ï¿½ï¿½Yï¿½{8ï¿½ï¿½{ï¿½ï¿½(ï¿½x8ï¿½pï¿½ï¿½Cã¨ï¿½ï¿½ï¿½\Kï¿½ï¿½ï¿½ï¿½ï¿½Hï¿½'Sï¿½Je5ï¿½ï¿½ï¿½ï¿½f4ï¿½ï¿½E9Y!ï¿½ï¿½ï¿½ï¿½m\ï¿½
ï¿½Ç×¼ï¿½ï¿½KjFï¿½ï¿½x}ï¿½pï¿½ï¿½ï¿½tï¿½Çï¿½ï¿½=yEï¿½|.ï¿½fxï¿½_ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½cÂ‹ï¿½ï¿½q\ï¿½ï¿½ï¿½`tNï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1^ï¿½ï¿½ï¿½ï¿½#ï¿½sï¿½ï¿½Ç©ï¿½ï¿½,ï¿½ï¿½,rï¿½QMï¿½>Iï¿½ï¿½ï¿½|=ï¿½~ï¿½ï¿½ï¿½jï¿½ï¿½!ï¿½ï¿½tï¿½ï¿½ï¿½gIVï¿½Pwï¿½ï¿½ï¿½Mlë‚¤ï¿½q~rQ?+ï¿½/aï¿½ï¿½ï¿½Ù¬ï¿½ï¿½Cï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dCCï¿½ï¿½pJ"g\ï¿½'ï¿½ï¿½Qï¿½=Yï¿½
ï¿½Tï¿½ï¿½Ø®ï¿½Ü„z
[4ï¿½ï¿½VDï¿½ï¿½vï¿½ï¿½ï¿½Ç¶ï¿½ï¿½dï¿½]ï¿½Kï¿½ï¿½/ï¿½ï¿½Aï¿½ï¿½ï¿½ï¿½pì©¯ï¿½ï¿½ï¿½V}wï¿½ï¿½ï¿½EW4ï¿½Iï¿½ï¿½Î¿;dï¿½Aï¿½Ãƒ,-ï¿½
ï¿½ï¿½ï¿½ï¿½]^ï¿½ï¿½ï¿½Gxï¿½ulï¿½ï¿½ï¿½ï¿½ï¿½Jï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(-Ryï¿½,ï¿½â²¼ï¿½ï¿½Ô¸dï¿½ï¿½ï¿½ï¿½kï¿½ï¿½ï¿½#ï¿½Oï¿½7ï¿½ï¿½Gï¿½'QUï¿½ï¿½ï¿½ï¿½Eï¿½:ï¿½Vï¿½ï¿½ï¿½ï¿½ï¿½@ï¿½ï¿½Yï¿½6ï¿½vÙ§ï¿½:ï¿½.ï¿½ï¿½ï¿½`&8ï¿½Q:ï¿½!
ï¿½ï¿½ï¿½,3O)ï¿½ï¿½_	Pï¿½ï¿½ï¿½Wï¿½ï¿½vï¿½ï¿½ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½xï¿½ï¿½ï¿½0ï¿½81ï¿½ï¿½T'ï¿½ï¿½ï¿½:ï¿½ï¿½=!tï¿½ï¿½ï¿½ï¿½$ï¿½	ï¿½ï¿½Oï¿½?9:ï¿½ï¿½eï¿½ï¿½ï¿½O,ï¿½
H|Rrï¿½C8ï¿½ï¿½Úªï¿½ï¿½ï¿½ï¿½ï¿½ï¿½y.ï¿½SIï¿½ï¿½ï¿½ï¿½uï¿½hï¿½ï¿½!Cï¿½ï¿½Wvï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½7.ï¿½pl:n!ï¿½ï¿½vï¿½ï¿½Xï¿½&
ï¿½ï¿½ï¿½xk+ï¿½"ï¿½ï¿½ï¿½=ï¿½ï¿½ï¿½Q^-ï¿½Jï¿½ï¿½ï¿½/×²ï¿½ï¿½ÕµD~%k>ï¿½8Yï¿½1ï¿½ï¿½?ï¿½_ï¿½=ï¿½rï¿½ï¿½dï¿½Ke~ï¿½rï¿½6<Bï¿½ï¿½=ï¿½ï¿½sï¿½Rï¿½Â‰ï¿½ï¿½_ï¿½-ï¿½[Vï¿½ï¿½'Õ¬ï¿½ï¿½ï¿½=Wï¿½ï¿½ï¿½ï¿½ï¿½WNÜ´ï¿½ï¿½ï¿½ï¿½!ï¿½ï¿½ï¿½ï¿½ï¿½}ï¿½-ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½à¸«ï¿½Uï¿½ï¿½Oï¿½Ğ¢7ï¿½ï¿½ï¿½^ï¿½ï¿½Mï¿½Y:^Vï¿½Ò‘rtï¿½8ï¿½ï¿½ï¿½ï¿½<ï¿½umï¿½<!xPï¿½oï¿½ï¿½Kï¿½ï¿½ï¿½uï¿½ï¿½0Cï¿½ï¿½Zï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½Pï¿½vï¿½?ï¿½yï¿½ï¿½ï¿½-ï¿½ï¿½Gï¿½ï¿½ï¿½7(\>ï¿½ï¿½{gHtï¿½ï¿½Ö­Oï¿½ï¿½ï¿½1"%ï¿½!W:ï¿½ï¿½}"ï¿½ï¿½ï¿½Pï¿½ï¿½ï¿½3ï¿½qï¿½ï¿½hï¿½ï¿½Hï¿½rï¿½ï¿½>ï¿½ï¿½ï¿½!>*ï¿½^'ï¿½ï¿½q"ï¿½ï¿½&ï¿½dvï¿½1ï¿½ï¿½Vï¿½ï¿½ï¿½?ï¿½×´ï¿½ï¿½`rÆƒï¿½M&ï¿½Sï¿½P9ï¿½Nï¿½ï¿½x98ï¿½ï¿½ï¿½ï¿½[}ï¿½\{Vï¿½|
ï¿½Wï¿½ï¿½ï¿½RİŸï¿½	ï¿½lï¿½ï¿½ï¿½CØ›O$9ï¿½nbTï¿½ï¿½ï¿½ï¿½ï¿½9W8ï¿½^ï¿½O×¦gVï¿½ï¿½ï¿½nï¿½ï¿½ ï¿½!ÔŸq"E2[8ï¿½ï¿½ï¿½~ï¿½ï¿½ï¿½ï¿½Uï¿½b{Jï¿½ÔSï¿½ï¿½ï¿½ï¿½&ï¿½Rï¿½ï¿½ï¿½ï¿½ï¿½4ï¿½Wï¿½ï¿½Lï¿½=gï¿½}&Gï¿½ï¿½>\ï¿½8ï¿½ï¿½%Zï¿½ï¿½i;ï¿½k[Ü«Z;ï¿½;gEï¿½ï¿½ï¿½6ï¿½Wyï¿½x}9ï¿½ï¿½1^<6ï¿½ï¿½ï¿½i1-ï¿½	í‰¯/ï¿½kï¿½#oï¿½ï¿½>ï¿½ï¿½{TÕÖ½LO$ï¿½ï¿½6&ï¿½ï¿½ï¿½ï¿½U\ï¿½/_ï¿½ï¿½Sï¿½/9Cï¿½ï¿½ï¿½y\ï¿½cï¿½ï¿½ï¿½guoï¿½ï¿½ï¿½]ï¿½QDï¿½ï¿½ï¿½Kï¿½İ‹ï¿½ï¿½ï¿½
ï¿½ï¿½dï¿½ï¿½ï¿½ï¿½/Cï¿½]ï¿½Fï¿½ï¿½{Vï¿½ï¿½ï¿½É4ï¿½ï¿½R\ï¿½OeGï¿½Qï¿½ï¿½ÈRï¿½ï¿½ï¿½eï¿½<ï¿½ï¿½$.ï¿½#^ï¿½<Z[ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½Tï¿½ï¿½Eï¿½Úºï¿½ï¿½WFï¿½qï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Lï¿½yï¿½ï¿½ï¿½qq_ï¿½ï¿½ÉŒï¿½ï¿½?ï¿½wï¿½ï¿½ï¿½ï¿½ï¿½D\ï¿½ï¿½/ï¿½Xï¿½ï¿½Mï¿½ï¿½Jï¿½ï¿½?ï¿½Mï¿½9:bï¿½ï¿½;ï¿½ï¿½gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?Kï¿½ï¿½ï¿½2ï¿½Wï¿½ï¿½}ï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½;ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(ï¿½ï¿½.ï¿½mY~>ï¿½ï¿½ï¿½+lï¿½Dï¿½.ï¿½ï¿½~ï¿½?ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½Dï¿½Sï¿½[&ï¿½	ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½ï¿½ï¿½sï¿½ï¿½ï¿½y?ï¿½ï¿½ï¿½s9ï¿½t<ï¿½ï¿½Ã•ï¿½ï¿½X2ï¿½ï¿½ï¿½ï¿½~ï¿½ï¿½ï¿½os(ï¿½ï¿½ï¿½xï¿½ï¿½_ï¿½ï¿½ï¿½ï¿½ï¿½İŸwï¿½ï¿½ï¿½ï¿½ï¿½h<ï¿½ï¿½;ï¿½ï¿½]ï¿½%gï¿½)Zï¿½ï¿½xLï¿½ï¿½+ï¿½ï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½ï¿½ï¿½ï¿½gï¿½'zï¿½ï¿½>+ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½>^)ï¿½ï¿½ï¿½A*ï¿½
t*~Hï¿½ï¿½Şš3ï¿½ï¿½ï¿½ï¿½C!ï¿½ï¿½9ï¿½ï¿½ï¿½ï¿½1ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½y>ï¿½ï¿½ï¿½uï¿½Y;vï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½bï¿½sï¿½p|4ï¿½ï¿½ï¿½}nï¿½ï¿½ï¿½ï¿½Yï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½}ï¿½gxï¿½ï¿½gï¿½[>ï¿½{ï¿½ï¿½ï¿½Ó¼ï¿½_rï¿½+ï¿½ï¿½Oï¿½KWï¿½ï¿½ï¿½ï¿½ï¿½y"ï¿½ï¿½ï¿½ï¿½	>ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½YQŞ§ï¿½ï¿½Iï¿½?ï¿½ï¿½Y4Ş“ï¿½ï¿½ï¿½3ï¿½wAO]i{ï¿½Sï¿½'#ï¿½×‚?\ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½}ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½>ï¿½Iï¿½Iï¿½ï¿½Tï¿½~ï¿½ 3ï¿½-ï¿½ï¿½dï¿½dï¿½bGpï¿½ ï¿½ï¿½dSï¿½ßrï¿½ï¿½Dï¿½1ï¿½Jï¿½ï¿½2ï¿½lï¿½ï¿½@ï¿½ï¿½z&rï¿½(@ï¿½.ï¿½|ï¿½yBï¿½7)'~ï¿½Ï†ï¿½a	rï¿½{nï¿½ï¿½ï¿½25Tï¿½6Æ¶ï¿½=	p}"%ï¿½ï¿½ï¿½xRDï¿½sï¿½ï¿½qï¿½ï¿½Lï¿½\eï¿½Hï¿½Ç…pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Dpï¿½Å·Ø“ï¿½{ï¿½ï¿½ï¿½ï¿½0ï¿½ï¿½m4ï¿½?;ï¿½mï¿½N}K?ï¿½ï¿½ï¿½ï¿½PD1ï¿½ï¿½ï¿½iJï¿½|ï¿½nxZï¿½ï¿½+ï¿½ï¿½ï¿½ï¿½Nï¿½ï¿½Wï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½udï¿½
 ï¿½Gjï¿½ï¿½ï¿½ï¿½ï¿½ï¿½kHï¿½/o1L+ï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½np$8h]ï¿½ï¿½pï¿½_ï¿½ï¿½ï¿½Ifvxï¿½K{RJï¿½.ï¿½ï¿½xYï¿½~ftï¿½ï¿½ï¿½ï¿½
`ï¿½!ï¿½
ï¿½ ï¿½ï¿½#ï¿½ï¿½]ï¿½(ï¿½ï¿½ï¿½ï¿½ï¿½_ï¿½Òºï¿½*ï¿½Kï¿½ï¿½ï¿½KGï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½wÓ§ï¿½ï¿½^;ï¿½ï¿½8ï¿½?ï¿½ï¿½ï¿½ï¿½wZï¿½ï¿½ï¿½eï¿½itï¿½ï¿½bï¿½[ï¿½3ï¿½ï¿½ï¿½'ï¿½S$ï¿½ï¿½nï¿½odyï¿½ï¿½ï¿½ï¿½Zï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½@È£ï¿½oUï¿½ZVï¿½ï¿½ï¿½ï¿½qï¿½	ï¿½{ï¿½ï¿½ï¿½}ï¿½ï¿½Oï¿½>ï¿½wï¿½ï¿½|ï¿½([3~kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½nï¿½ï¿½Q|ï¿½ï¿½/okÉ„oï¿½ï¿½z:ï¿½,ï¿½oï¿½#!aÌ·ï¿½yË»ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ç«–ï¿½Õ›ï¿½=ï¿½n\Óï¿½ro4ï¿½/ï¿½7ï¿½ï¿½ï¿½Ïï¿½wï¿½ï¿½ï¿½ï¿½ï¿½Ü§qï¿½ï¿½ï¿½2ï¿½ï¿½2ï¿½ï¿½ï¿½ï¿½ï¿½Ñ›wï¿½pï¿½ï¿½ï¿½-ï¿½İ³ï¿½ï¿½;:ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½%ï¿½+ï¿½ï¿½ï¿½k:$ï¿½ï¿½rOps5Gï¿½ï¿½ï¿½ï¿½ï¿½97ï¿½ï¿½ï¿½=ï¿½GLï¿½ï¿½ï¿½[ï¿½gya&ï¿½wï¿½|Kï¿½Öï¿½ï¿½}ï¿½ï¿½,ï¿½Sx_ï¿½ï¿½u.\ï¿½ï¿½]bï¿½
qï¿½ï¿½ï¿½5ï¿½ï¿½ï¿½ï¿½Dï¿½>ï¿½_ï¿½F@ï¿½=	(ï¿½s]Lï¿½
wï¿½ï¿½;Jï¿½ï¿½ï¿½Jï¿½+ï¿½ï¿½ï¿½ï¿½tï¿½]=ï¿½ï¿½}&Wæ“£ï¿½ï¿½'ï¿½ï¿½tÆï¿½_ï¿½ï¿½ï¿½ï¿½3ï¿½ß—@ï¿½ï¿½ï¿½Ï‰3}ï¿½Î»ï¿½D!XJï¿½nB$9ï¿½ï¿½ï¿½ï¿½=ï¿½[!ï¿½ï¿½İ{Jï¿½ï¿½ï¿½/K7ï¿½_s-/ï¿½ï¿½ï¿½yp)ï¿½\ï¿½21ï¿½ï¿½x4ï¿½[ï¿½7ï¿½ï¿½ï¿½/21ï¿½ï¿½0ï¿½ï¿½8/{%1^4uï¿½sï¿½qï¿½<|rï¿½$ï¿½ï¿½ï¿½x_IB3}ï¿½Hï¿½ï¿½;ï¿½lu?ï¿½ï¿½kï¿½>ï¿½ï¿½Hxï¿½ï¿½;Yï¿½@>ï¿½ aï¿½ï¿½â•›|Sï¿½ï¿½ï¿½Pï¿½ZXï¿½Zï¿½ï¿½?ï¿½ï¿½nï¿½ï¿½Î¹Rï¿½Dï¿½ï¿½ï¿½JVIï¿½ï¿½oBï¿½ï¿½ï¿½Zï¿½}Hï¿½ï¿½Kï¿½Gï¿½iï¿½Pï¿½^Oï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½8]ï¿½Gï¿½ï¿½c,ï¿½@ï¿½ï¿½cï¿½ï¿½<ï¿½×…_~ï¿½cï¿½ï¿½ï¿½ï¿½yï¿½ï¿½ï¿½oï¿½]+Ô—ï¿½gï¿½NXï¿½4aï¿½Wï¿½cï¿½ï¿½?ï¿½wï¿½ï¿½yOï¿½\MRs#Şºï¿½ï¿½2ï¿½ï¿½ßŒï¿½Bï¿½ï¿½@ï¿½sCï¿½,ï¿½ï¿½ï¿½Bï¿½Lï¿½"d}ï¿½ï¿½}a"ï¿½,?Oï¿½9ï¿½,
ï¿½)iï¿½ï¿½m2ï¿½yBï¿½/'ï¿½Ó‘ï¿½ï¿½ï¿½JFï¿½×©ï¿½4ï¿½ï¿½H
Tï¿½ï¿½ï¿½54C?ï¿½İ„u$kï¿½ï¿½Wï¿½İŒ×‘ï¿½cx}ï¿½1ï¿½Pï¿½Mï¿½ï¿½7ï¿½3Ryï¿½0ï¿½ï¿½@Gï¿½ï¿½Ò¤Rï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½dï¿½Pï¿½qï¿½>ï¿½ï¿½ï¿½
ï¿½]ï¿½ï¿½'Uï¿½3+bnï¿½ï¿½Åï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Xï¿½34aï¿½Ä¯ï¿½ï¿½ï¿½ï¿½bï¿½ï¿½wï¿½ï¿½ï¿½ï¿½Vraï¿½Nï¿½Ş‘ï¿½ï¿½<ï¿½Rï¿½ ï¿½Rï¿½ï¿½5ZÃï¿½ï¿½ï¿½[\ï¿½|\ï¿½YvEï¿½ï¿½ï¿½ï¿½c0ï¿½ï¿½3ï¿½5ï¿½:rï¿½k#Q?ï¿½ï¿½Jï¿½ï¿½nwï¿½"ï¿½{Ò–ï¿½+ï¿½+6ï¿½8ï¿½ï¿½ï¿½\%ï¿½>kï¿½'ï¿½p'ï¿½ï¿½ï¿½ï¿½ï¿½;ï¿½0ï¿½"ï¿½ï¿½ï¿½9B7ï¿½_Ùp<4Nï¿½76ï¿½ï¿½ï¿½ï¿½Zï¿½'ï¿½ï¿½?ï¿½>\gï¿½Û©Xï¿½"ï¿½ï¿½ï¿½ï¿½-ï¿½ï¿½Gï¿½ï¿½ï¿½oÑŠWo%ï¿½ï¿½ï¿½Vdpï¿½]Trï¿½ï¿½ï¿½ï¿½yï¿½ï¿½Bï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½24ï¿½zï¿½vï¿½ï¿½/ï¿½ï¿½'0ï¿½4	ï¿½Ffï¿½ï¿½cß±ï¿½ï¿½ï¿½/iï¿½ï¿½ï¿½ï¿½1}eï¿½Õ½7ï¿½zzï¿½ï¿½ï¿½ï¿½<?>ï¿½#ï¿½1ï¿½ï¿½#ï¿½#ï¿½G|2ï¿½Y{yï¿½ï¿½
e<ï¿½ï¿½~ï¿½ï¿½}qï¿½Nï¿½|'ï¿½8ï¿½>ï¿½ï¿½ï¿½?,ï¿½Yï¿½ï¿½x=ï¿½[j;R?_ï¿½;vï¿½sG8ï¿½ï¿½\y7Xï¿½ï¿½ï¿½xLyï¿½A^ï¿½ï¿½[ï¿½ï¿½|ï¿½@~}}ï¿½_ï¿½ï¿½@7]xï¿½ï¿½ï¿½gï¿½s
â€ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ ï¿½Üï¿½ï¿½mï¿½ï¿½ñ°‡£ï¿½ï¿½Bï¿½ï¿½p8h]ï¿½eï¿½ï¿½Yï¿½ï¿½Rï¿½gï¿½.ï¿½t_]ï¿½_ï¿½, ï¿½fï¿½ï¿½ï¿½ï¿½ï¿½[ï¿½AJ$Z'ï¿½)By7ï¿½ï¿½Hï¿½S:W$C
â€%#ï¿½ï¿½ï¿½ï¿½ß¨ï¿½ï¿½ï¿½qcï¿½ï¿½ï¿½ï¿½:pï¿½Zï¿½cxï¿½.qXï¿½ï¿½ï¿½tï¿½ï¿½È¶Jï¿½cï¿½"ï¿½|ï¿½6ï¿½ï¿½0ï¿½ï¿½ï¿½ï¿½]ï¿½ï¿½>ï¿½!ï¿½gï¿½ï¿½ï¿½ï¿½Ø¥ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½NR{ï¿½ï¿½ï¿½gMHï¿½]ï¿½ï¿½ï¿½ï¿½bzbï¿½ï¿½ï¿½>Ô…=	?ï¿½ï¿½Cï¿½ï¿½ï¿½yfï¿½ï¿½Ù¢ï¿½Qï¿½ï¿½ï¿½t=ï¿½á¼¢ï¿½ï¿½Bï¿½ï¿½	ï¿½ï¿½ï¿½zï¿½;ï¿½ï¿½ï¿½=bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½gï¿½ï¿½ï¿½pï¿½ï¿½Tï¿½Ï‘Mï¿½vï¿½ï¿½ï¿½
ï¿½É³iï¿½ï¿½yï¿½ï¿½3xï¿½ï¿½4>ï¿½ï¿½"â£»ï¿½ï¿½ï¿½ï¿½ï¿½aOï¿½ï¿½9ï¿½Ibï¿½iï¿½ï¿½6ï¿½ï¿½oï¿½TEï¿½ï¿½~QØ›ï¿½fkn#ï¿½ï¿½ï¿½ï¿½ï¿½×·ë—©ï¿½|ï¿½?ï¿½:ï¿½ï¿½hï¿½ï¿½ï¿½3ï¿½ï¿½}×ƒsï¿½ï¿½ï¿½qï¿½ï¿½ï¿½ï¿½Eï¿½
7z'ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½î‰¡aï¿½yï¿½ï¿½Wï¿½qï¿½ï¿½ï¿½ï¿½"mï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½`
ï¿½Å•ï¿½ï¿½ï¿½pï¿½qfï¿½Jï¿½ï¿½Xçª±<ï¿½ï¿½!ï¿½u{ï¿½T:ï¿½ï¿½ï¿½f]J$7ï¿½,${vï¿½{ï¿½ï¿½Åï¿½yï¿½r5^<ï¿½ï¿½Å¥dï¿½Wï¿½ï¿½s?9c9/ï¿½ï¿½-ï¿½ï¿½ï¿½(ï¿½\yï¿½ï¿½:\!ï¿½7ï¿½7ï¿½ï¿½Q-ï¿½ï¿½)Ğ—ï¿½ï¿½ï¿½ï¿½oï¿½nï¿½ï¿½ï¿½ï¿½ï¿½ÌŸO?_ï¿½=ï¿½[ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½ï¿½^ï¿½#=ï¿½)ï¿½>pï¿½ï¿½8ï¿½_ï¿½ï¿½7ï¿½ï¿½ï¿½ÑHï¿½Éœï¿½nï¿½ï¿½wï¿½Ì…ï¿½"ï¿½cï¿½8Aï¿½{ï¿½iï¿½ï¿½ï¿½0'ï¿½ï¿½ï¿½<ojï¿½
bï¿½kOï¿½7ï¿½pï¿½ï¿½v)/^aï¿½x
ï¿½'o>ï¿½[Gs3ï¿½\Wï¿½8ï¿½fï¿½ï¿½ï¿½sï¿½ï¿½ï¿½uï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"ï¿½ï¿½8ï¿½]ï¿½ï¿½qï¿½cï¿½YRï¿½ï¿½4l2ï¿½<ï¿½ï¿½zcrï¿½ï¿½ï¿½ï¿½ï¿½\8ï¿½ï¿½ï¿½{0Yï¿½ï¿½lï¿½+cs>Oï¿½uï¿½æ¤’ï¿½]0ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½ï¿½fï¿½ï¿½qï¿½Z|Ï¤:]ï¿½ï¿½mvY'ï¿½Oï¿½ï¿½ï¿½ï¿½ï¿½Pï¿½ï¿½ï¿½ï¿½3ï¿½mï¿½3ï¿½gï¿½ï¿½Mï¿½ï¿½aï¿½o
ï¿½Cï¿½ï¿½é¥Ÿï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?`}aï¿½%ï¿½`G6ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿½^3ï¿½s""ï¿½1Ë·"ï¿½G
ï¿½ï¿½ï¿½ï¿½6ï¿½DÓ¹Nï¿½ï¿½Iï¿½[ï¿½ï¿½ï¿½ï¿½ï¿½fï¿½ŞÇ‡ï¿½ï¿½Kï¿½7XR
ï¿½ï¿½jï¿½ï¿½Sï¿½8,ï¿½×œ:cï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½>ï¿½,ï¿½ï¿½Tï¿½#ï¿½pï¿½ï¿½),ï¿½ï¿½{ï¿½'ï¿½ï¿½ï¿½ï¿½6+ï¿½ï¿½Qï¿½ï¿½ï¿½~ï¿½zï¿½ï¿½ï¿½ï¿½xï¿½Ì“ï¿½ï¿½ï¿½ï¿½<Bï¿½$sï¿½Wï¿½^Fï¿½ï¿½ï¿½ï¿½Lï¿½arï¿½Ìï¿½3gï¿½ï¿½vï¿½3Xï¿½g\ï¿½gï¿½sï¿½ï¿½{Fï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½i'~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dï¿½eï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½9iï¿½ï¿½ï¿½ï¿½ï¿½ï¿½g\ï¿½ï¿½ï¿½ï¿½rJï¿½q&Ï¸ï¿½NGï¿½rï¿½ï¿½6ï¿½9KMX'/ï¿½ï¿½+Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"ï¿½ï¿½ï¿½Ï¯ï¿½Rp	H8ï¿½ï¿½^Dï¿½ï¿½ï¿½ï¿½ï¿½mL
â€×—|ï¿½ï¿½ï¿½ï¿½oï¿½Aï¿½_q`}ß£ï¿½xï¿½R>
ï¿½gï¿½n
ï¿½5/\pï¿½ï¿½Úˆï¿½zpï¿½ï¿½1ï¿½Zï¿½`ï¿½Zï¿½ï¿½hï¿½zï¿½ï¿½zï¿½7=ï¿½Ö·G?ï¿½e|ï¿½ï¿½ï¿½	,ï¿½É¼ï¿½ï¿½ï¿½yï¿½ï¿½5?K×—ï¿½ï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½=Ø”=ï¿½ï¿½ï¿½{6ï¿½ï¿½>Kï¿½@Oï¿½Vï¿½ï¿½ï¿½Iï¿½Jï¿½ï¿½Oï¿½2ï¿½ï¿½yï¿½ï¿½ï¿½_ï¿½ï¿½Sypï¿½6LNï¿½i:t3ï¿½Kï¿½ï¿½rÓ¶JNÛ§ï¿½^fï¿½ï¿½k{$ï¿½ï¿½Pï¿½ï¿½Zï¿½ï¿½ï¿½cEï¿½ï¿½ï¿½ï¿½p)ï¿½ï¿½ï¿½@>e[ï¿½ï¿½Mï¿½ï¿½ï¿½ï¿½ï¿½!Bh7xï¿½ï¿½a/ï¿½'ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ØƒVï¿½ï¿½ï¿½@>ï¿½"ï¿½ï¿½ï¿½=ï¿½_ï¿½ï¿½X=n44<zï¿½Mï¿½2ï¿½ï¿½ï¿½<?sï¿½ï¿½cï¿½\Rï¿½ï¿½ï¿½PÛ¡c{ï¿½mï¿½ï¿½Kï¿½_ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½`ï¿½ï¿½fï¿½ï¿½ï¿½ï¿½ï¿½ï¿½iï¿½>ï¿½_ï¿½wï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½;ï¿½ï¿½2Nï¿½ï¿½ï¿½ï¿½5OVzï¿½ sï¿½ï¿½	\~ï¿½ï¿½Ó’3ï¿½i+rï¿½xï¿½ï¿½oUzï¿½ï¿½ï¿½gï¿½#Nï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½ï¿½ï¿½'ï¿½`Ï±3ï¿½ï¿½ï¿½7ï¿½OKï¿½Mï¿½m	ï¿½&rï¿½ï¿½uï¿½ï¿½ï¿½ï¿½nï¿½ï¿½J^Kï¿½2ï¿½ï¿½e?{ï¿½ï¿½ï¿½xOï¿½ï¿½ï¿½ï¿½!Lï¿½ï¿½ï¿½ï¿½ï¿½Y_Vï¿½ï¿½kFï¿½ï¿½ï¿½ï¿½qUï¿½Iï¿½ï¿½Kï¿½ï¿½4Gï¿½(ï¿½{ï¿½ï¿½,Ø€ï¿½a<ï¿½{;ï¿½Sï¿½ï¿½ï¿½ï¿½ï¿½ï¿½&|ï¿½ï¿½ï¿½Rï¿½ï¿½Fï¿½-ï¿½zï¿½ï¿½ï¿½ï¿½9ï¿½#ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½x&ï¿½Î¨ZPzï¿½ï¿½ï¿½fï¿½aXï¿½j'ï¿½ï¿½ï¿½ï¿½tØ¨ï¿½*oï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½$yÎ§Ç£ï¿½ï¿½ï¿½t<Dï¿½ï¿½EQgï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½sï¿½vï¿½ï¿½mï¿½ï¿½_ï¿½ï¿½Ò—ï¿½ï¿½ï¿½Oï¿½=ï¿½ï¿½×¯ï¿½F{Zhï¿½oï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½B\Yï¿½ï¿½Aï¿½9ï¿½GÅ›è†ï¿½ï¿½ï¿½_ï¿½ï¿½ï¿½Ü“\sï¿½3ï¿½ï¿½ï¿½Uï¿½ï¿½Pï¿½xï¿½/ï¿½=ï¿½ï¿½%ï¿½\ï¿½ZFï¿½3ï¿½ï¿½%ï¿½~ï¿½ï¿½3Mï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½5ï¿½dï¿½:ï¿½ï¿½Tï¿½ï¿½ï¿½[ï¿½ï¿½qï¿½+ï¿½@ï¿½<yï¿½ï¿½ï¿½kï¿½ï¿½ï¿½7i[ï¿½ï¿½ï¿½ï¿½ï¿½3yï¿½,ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½5Xï¿½ï¿½ï¿½ï¿½~ï¿½kï¿½ï¿½ï¿½ï¿½Ê£ï¿½ï¿½ï¿½\=ï¿½ï¿½O&E_ï¿½ï¿½ï¿½hLï¿½ï¿½Õ§(ï¿½zDï¿½ï¿½Jï¿½{ï¿½É§ï¿½ï¿½ï¿½ï¿½G4Î£/ï¿½ï¿½hï¿½ï¿½ï¿½ï¿½ï¿½Ñ•ï¿½/ï¿½ï¿½g/Fï¿½ï¿½_Jï¿½Xï¿½ï¿½Tï¿½sQï¿½ï¿½Ş£ï¿½ï¿½Eï¿½Kï¿½ï¿½kï¿½Sï¿½0ï¿½~\Hï¿½<ï¿½Ú£ï¿½Æ©ï¿½oBï¿½P/ï¿½ï¿½ï¿½R$ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½#ï¿½ï¿½'ï¿½ï¿½xï¿½ï¿½ï¿½ï¿½kï¿½ï¿½ï¿½Bï¿½"_ï¿½7>ï¿½ó¡¦Oï¿½ï¿½7ï¿½?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Æ©|ï¿½}Nï¿½ï¿½3zï¿½ï¿½ï¿½Ì¡qï¿½ï¿½[ï¿½ï¿½Oï¿½ï¿½ï¿½<ï¿½hï¿½ï¿½Wï¿½
ï¿½ï¿½Ó·ï¿½|ï¿½ï¿½.ï¿½1ï¿½ï¿½ï¿½pÖŸï¿½ï¿½ï¿½3ZCï¿½ï¿½ï¿½ï¿½ï¿½ï¿½p?ï¿½ï¿½}ï¿½yï¿½É½ï¿½ï¿½p
ï¿½Oï¿½éˆ¾ï¿½hlï¿½szFï¿½ï¿½zğ±ï¿½ï¿½1_ï¿½ï¿½|ï¿½ï¿½4Oï¿½=]7ï¿½:ï¿½qï¿½gé–¾4~ï¿½Yï¿½#ï¿½ï¿½ï¿½ï¿½tï¿½?)ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½È·Gy2ï¿½ï¿½ï¿½Iï¿½ï¿½ï¿½ï¿½Uï¿½ï¿½\Nï¿½ï¿½Dï¿½ï¿½hL3hï¿½2Ú‡ï¿½ï¿½tD_ï¿½<ï¿½T"ï¿½9ï¿½ï¿½ï¿½iï¿½ï¿½Kï¿½qï¿½gï¿½Xï¿½cï¿½ï¿½ï¿½ï¿½Êoï¿½xNï¿½Û§ï¿½ttï¿½'ï¿½ï¿½Ş‘ï¿½$?Aï¿½ï¿½ï¿½ï¿½d~ï¿½ï¿½ï¿½ï¿½Oï¿½Oï¿½È°ï¿½ï¿½>ï¿½kï¿½ï¿½ï¿½ï¿½ZŞµï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Eï¿½GOï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½#_+ï¿½Kå ’ï¿½ï¿½eï¿½Z>+ï¿½i|ï¿½ï¿½8ï¿½6ï¿½ï¿½7èŒ¶Vvï¿½y4ï¿½]ï¿½}Sp?ï¿½nï¿½Cï¿½ï¿½ï¿½ï¿½\Lcï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'11ï¿½'Hï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bï¿½ï¿½>9uï¿½ï¿½cï¿½ï¿½ï¿½Ó¾Vï¿½hï¿½zï¿½ï¿½ï¿½ï¿½ï¿½@_6^ï¿½ï¿½Oï¿½ï¿½1ï¿½~Gï¿½Kï¿½ï¿½qï¿½0Kï¿½î§¾Vvï¿½ï¿½z:ï¿½ï¿½ï¿½ï¿½ï¿½~ï¿½k1ï¿½ï¿½3ï¿½\ï¿½ï¿½ï¿½Iï¿½ÛŸï¿½Zï¿½ï¿½'ï¿½ï¿½ï¿½ï¿½hLï¿½ï¿½ï¿½o*~ï¿½ï¿½ï¿½Eï¿½/ï¿½cï¿½>ï¿½ï¿½hï¿½ï¿½ï¿½Z4}Kï¿½	ï¿½e8ï¿½/?ï¿½Bï¿½%ï¿½ï¿½A<ï¿½<ï¿½^Û¿ge8É¿ï¿½ï¿½ï¿½ï¿½ï¿½p1ï¿½i
ï¿½)Nï¿½Uï¿½ï¿½tÒ³ï¿½}Q>ï¿½aL,ï¿½ï¿½ï¿½C_Kï¿½ï¿½ï¿½ï¿½Vï¿½ï¿½ï¿½Xï¿½ï¿½Jñœ•ï¿½dï¿½?ï¿½"8ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Srï¿½ï¿½ï¿½ï¿½Cï¿½ï¿½Eï¿½ï¿½ï¿½ï¿½ï¿½ï¿½UHï¿½ï¿½ï¿½7ï¿½ï¿½+;\\ï¿½}ï¿½ï¿½ï¿½ï¿½@ï¿½=ï¿½lqï¿½~aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½xNï¿½ï¿½ï¿½@ï¿½ï¿½Lï¿½ï¿½ï¿½ï¿½Wï¿½ï¿½w,ï¿½ï¿½ï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½oï¿½?Qï¿½ï¿½HOï¿½tMï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½ï¿½Tï¿½ï¿½uï¿½?ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½cï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½/ï¿½?ï¿½hï¿½ï¿½ï¿½Xï¿½ï¿½ï¿½ï¿½ï¿½ï¿½311;ï¿½ï¿½Ï¨?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ yï¿½oï¿½?ï¿½ï¿½5ï¿½ï¿½ï¿½@ï¿½hï¿½ï¿½ï¿½mï¿½8ï¿½Ç¿X8'{e~{ï¿½!ï¿½pï¿½ï¿½ï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 7ï¿½ï¿½suï¿½hOï¿½ï¿½ï¿½yï¿½ï¿½Xï¿½bï¿½ï¿½ï¿½7ï¿½ï¿½ï¿½_Rï¿½ï¿½ï¿½Ù±xFï¿½ï¿½ï¿½L^ï¿½Oï¿½?ï¿½É½ï¿½ï¿½ï¿½ï¿½ï¿½uï¿½ï¿½Qsï¿½pnï¿½×‰ï¿½ï¿½ï¿½ï¿½Yï¿½ï¿½ï¿½ï¿½ï¿½É½ï¿½ï¿½ï¿½ï¿½×‰ï¿½ï¿½Uï¿½ï¿½6<<ï¿½á¸¾ï¿½ï¿½ï¿½ç¢¼ï¿½oï¿½?ï¿½ï¿½Mï¿½v`ï¿½oï¿½?Hï¿½~Xï¿½ï¿½:q4ï¿½_P8ï¿½$ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Tï¿½ï¿½uï¿½ï¿½ï¿½Sï¿½ï¿½;ï¿½ï¿½?ï¿½ï¿½uï¿½?P8ï¿½ï¿½Zï¿½ï¿½ï¿½0ï¿½ï¿½ï¿½oï¿½?ï¿½ï¿½ï¿½Ä¿ï¿½ï¿½ =ï¿½ï¿½æŸ²ï¿½ï¿½Q^ï¿½Oï¿½?Lï¿½'ï¿½ï¿½ï¿½Nï¿½Ä¿ï¿½ï¿½@ï¿½Òï¿½?ï¿½ï¿½:q8ï¿½?Zï¿½ï¿½ï¿½;ï¿½ï¿½ï¿½	ï¿½ï¿½ï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½_'ï¿½ï¿½eï¿½A~'ï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½?\ï¿½ï¿½Zï¿½ï¿½ï¿½!ï¿½ï¿½ï¿½ï¿½Fï¿½;ï¿½rï¿½wï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½gï¿½ZBï¿½aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½uï¿½?ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½:ï¿½ï¿½?\ï¿½ï¿½ï¿½Qï¿½ï¿½#ï¿½ï¿½wï¿½ï¿½ï¿½ï¿½?ï¿½ï¿½ï¿½@gï¿½Hï¿½ï¿½7Fï¿½ï¿½r/ï¿½ï¿½'ï¿½ï¿½ä‰¿ï¿½ï¿½ï¿½ï¿½ï¿½[ï¿½Jï¿½ß­?pï¿½uï¿½hï¿½bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½yrï¿½oï¿½?pï¿½ubï¿½'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ñ±rh,ï¿½ï¿½ï¿½pï¿½ï¿½kï¿½ï¿½ï¿½$r_1[X0~/bï¿½ï¿½ï¿½7ï¿½8ï¿½ï¿½Aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dï¿½Lï¿½qï¿½Wqï¿½ï¿½ï¿½ï¿½ï¿½N6ï¿½<ï¿½ï¿½gpï¿½ï¿½ï¿½ï¿½Mï¿½Oï¿½3ï¿½ï¿½ï¿½?@*2Cß‚ï¿½ï¿½ï¿½ï¿½ï¿½zSï¿½tï¿½ï¿½Qï¿½ï¿½;ï¿½Cï¿½ï¿½ï¿½yï¿½ï¿½zï¿½pï¿½qï¿½hï¿½7ï¿½Cï¿½ï¿½ï¿½xIÈ©?pï¿½_ï¿½2ï¿½ï¿½@ï¿½Ş›ï¿½ï¿½ï¿½_|ï¿½Ngï¿½ï¿½6ï¿½'ï¿½geÖ›_Héµ¼ï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½è›‘ï¿½'y:ï¿½ï¿½ï¿½ì¥¿ï¿½ï¿½ï¿½ï¿½%ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½t
/ï¿½+Zoï¿½ï¿½0ï¿½ï¿½lï¿½=ï¿½ï¿½yï¿½}?ï¿½M-9ë½Šï¿½ï¿½g ï¿½??ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½;4ï¿½[ï¿½ï¿½ï¿½!oï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½Fï¿½#ï¿½,?ï¿½Tï¿½gmï¿½Rï¿½ï¿½Wï¿½ï¿½>f]ï¿½%-ï¿½ï¿½xÉ¬ï¿½/ï¿½a.mï¿½ï¿½|yî…‹ï¿½Hï¿½ï¿½|ï¿½Ì”ï¿½ï¿½hï¿½Ú‡V2~ï¿½ï¿½ï¿½Fï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½3ï¿½"ï¿½_cï¿½ï¿½ï¿½A}ï¿½I|ï¿½ï¿½Íˆï¿½Rï¿½I}ï¿½Kï¿½ï¿½F~ï¿½ï¿½:ï¿½#ï¿½Cï¿½ï¿½ï¿½/Y|Ú¿ï¿½ï¿½8ï¿½&ï¿½Ã©ï¿½-jï¿½D.ï¿½ï¿½3Oï¿½Rï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½ï¿½>oFQJï¿½j`zï¿½ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½Ip&bUï¿½9qgO:v}ï¿½^[57ï¿½}ï¿½ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿½ï¿½å« ï¿½"$ï¿½pï¿½wJPï¿½Gï¿½1ï¿½ï¿½ï¿½Rï¿½ï¿½ï¿½A><eï¿½u?ï¿½eï¿½ï¿½U7ï¿½\!ï¿½ï¿½lï¿½0ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½Ux=[ï¿½bï¿½ï¿½ï¿½ï¿½ERï¿½ï¿½.ï¿½ï¿½ï¿½ï¿½aszï¿½"ï¿½ï¿½Itï¿½sï¿½ï¿½ï¿½.|.Q>KŞ§ï¿½ï¿½ï¿½.ï¿½ï¿½8gï¿½'ï¿½ï¿½oï¿½\?2uï¿½ï¿½%
	8ï¿½ï¿½ï¿½'ï¿½ï¿½Hï¿½|	ï¿½ï¿½H{ï¿½ï¿½`ï¿½Yï¿½{ï¿½'ï¿½ï¿½ï¿½a7ï¿½Â¸ï¿½ï¿½ï¿½}tjQpï¿½ï¿½/zï¿½ï¿½ï¿½YFï¿½ï¿½ï¿½ï¿½ï¿½=[ï¿½ï¿½=Qï¿½pï¿½ï¿½ï¿½Ï Myï¿½	ï¿½bï¿½ï¿½Í“ï¿½oŞ†ï¿½ï¿½3ï¿½ß¼ï¿½ï¿½ONyï¿½Mï¿½ï¿½ï¿½h}ï¿½ï¿½&|Mï¿½Lï¿½ï¿½ï¿½$
ï¿½ÂŠï¿½ï¿½t
ï¿½0f#ï¿½g`ï¿½Hï¿½lkdï¿½Dvï¿½wï¿½>#æ“±'ï¿½Kï¿½ï¿½=/Eï¿½%ï¿½ï¿½/ï¿½ï¿½ï¿½ï¿½7Jï¿½ï¿½$<ï¿½ï¿½ï¿½4ï¿½xï¿½zï¿½=ï¿½ï¿½ï¿½=ï¿½ï¿½Yï¿½ï¿½+ï¿½ï¿½ï¿½pï¿½zï¿½@ï¿½Bï¿½ï¿½ï¿½ï¿½ï¿½Ä†ï¿½]p)8ï¿½'ï¿½3ï¿½{ï¿½Ä·$k1ï¿½ï¿½ï¿½\bï¿½<+ï¿½+6ã¹Šnï¿½ï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½ï¿½{ï¿½cAï¿½)mï¿½{ï¿½;%ï¿½ï¿½ï¿½Frï¿½9~va=ï¿½9Ì•j"ï¿½ï¿½ï¿½ï¿½<ï¿½ï¿½(ï¿½ï¿½=ï¿½ï¿½>É½ï¿½Eï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½ï¿½_pï¿½ï¿½ï¿½cï¿½ï¿½kgspï¿½Y8ï¿½ï¿½ï¿½ï¿½9ï¿½?uï¿½ï¿½Sp^ï¿½*.ï¿½ï¿½Î¨4ï¿½ï¿½ï¿½nU?ï¿½ï¿½Wï¿½]&7oj%pï¿½ï¿½uï¿½`Z<bï¿½ï¿½ï¿½ï¿½x`Zï¿½ï¿½ï¿½fï¿½ï¿½ï¿½ï¿½%ï¿½ï¿½ï¿½\ sUï¿½[ï¿½#ï¿½ï¿½ï¿½ï¿½ï¿½5[H]ZUï¿½ï¿½ï¿½Krï¿½>ï¿½ï¿½ï¿½Iï¿½s{ï¿½ï¿½CpÙ«0Ê’@ï¿½ï¿½ï¿½u:ï¿½ï¿½oEï¿½ï¿½_ï¿½ï¿½2ï¿½ï¿½C3pR=ï¿½tï¿½ï¿½r|Æ¢ï¿½ï¿½'fï¿½eï¿½ï¿½>É³sï¿½ï¿½ï¿½_Q'`cï¿½\4ï¿½ppï¿½ï¿½(/Yï¿½ï¿½)ï¿½ï¿½
![ï¿½6ï¿½ï¿½+ï¿½!ï¿½0}O={ï¿½/ï¿½
ï¿½fLï¿½ï¿½ï¿½ï¿½ï¿½FİˆOï¿½:Cï¿½ï¿½ï¿½4ß‘ï¿½ï¿½wï¿½ï¿½ï¿½Fï¿½ï¿½*|Fï¿½`ï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½>Cï¿½ÜGfï¿½ï¿½ï¿½J:.ï¿½_ï¿½ï¿½cï¿½ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1g	ï¿½<ï¿½ï¿½g1ï¿½cÃ„ï¿½ï¿½$Ë±&ï¿½T.>ï¿½ï¿½ï¿½\ï¿½~Iï¿½5ï¿½ï¿½ï¿½ï¿½Oï¿½È³ï¿½ï¿½ï¿½'kï¿½ï¿½ï¿½_ï¿½ï¿½Îº9wï¿½ï¿½rï¿½ï¿½x7ï¿½ï¿½.ï¿½ï¿½yï¿½ï¿½Ñ°sï¿½
ï¿½qï¿½z*ï¿½@Egï¿½F5ï¿½&ï¿½ï¿½lï¿½~<ï¿½KÏ›ï¿½Qyï¿½L	ï¿½pï¿½sTyï¿½}7ï¿½ï¿½Gï¿½$ï¿½ï¿½ï¿½|7][gï¿½ï¿½&Uï¿½Rs1^[Ë©ï¿½ï¿½/Qï¿½ï¿½ï¿½0ï¿½R}ï¿½ï¿½=ï¿½ï¿½ï¿½xHï¿½sï¿½Ú­Æ¶^ï¿½Mxï¿½^ï¿½Cï¿½Qï¿½ï¿½ï¿½*ï¿½3ï¿½ï¿½ï¿½Ûµï¿½}ï¿½_;wï¿½ï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½Gï¿½ï¿½vlï¿½&Rï¿½ï¿½ï¿½{"s*ï¿½ï¿½ï¿½ï¿½yNï¿½ï¿½+ï¿½ï¿½ï¿½~kKï¿½ï¿½Æ³ï¿½7ß½ï¿½-Uï¿½ï¿½'ï¿½eï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½Ì«ï¿½>ï¿½ï¿½ï¿½Í†ï¿½xï¿½kß“ï¿½ï¿½9"srï¿½ï¿½Ê¡ï¿½ï¿½ï¿½	=e bï¿½Oï¿½ï¿½Wï¿½#"lï¿½Oï¿½ï¿½2ï¿½qj}ï¿½ï¿½ï¿½Ü«}(ï¿½ï¿½PjPï¿½@*ï¿½ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½8ï¿½@ï¿½\ï¿½ï¿½5ï¿½ï¿½ï¿½ï¿½Gï¿½ï¿½ß®lï¿½ï¿½UKÍ•Yï¿½JSï¿½ï¿½Uï¿½ï¿½'ï¿½{ï¿½hØ£yï¿½Iï¿½ï¿½
ï¿½ï¿½Dï¿½FY7kï¿½0ï¿½ï¿½3J'Tï¿½3S=Oï¿½ï¿½ï¿½hï¿½ï¿½Nï¿½ï¿½İ·{3
ï¿½oTï¿½ï¿½Èİ?ï¿½>3ï¿½ï¿½Ş”ï¿½ï¿½ï¿½"bï¿½ï¿½ï¿½ï¿½Å‹ï¿½*ï¿½]ï¿½&ï¿½é‡€ï¿½8Yï¿½	ï¿½ï¿½ï¿½ï¿½Éµï¿½'ï¿½Ó¥ï¿½5ï¿½/ï¿½nï¿½Gï¿½ï¿½É¼7:MzUï¿½Êˆ)ï¿½ï¿½ï¿½k_ï¿½ï¿½ï¿½mï¿½[ï¿½Û±Ë¾...more

cawXom-mykxov-qempa2
glpat-srrSzoS7a5GfnTr-eQea
88e805C9d4dEdD4cc934A8B1AC05Ef52e452cB19AA60ccC0DAdE8AAB6C0b7CC0841E9D0
cawXommykxov$qempa2
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
miv3iexmlkiao5zra6avqsxpjjz3tqbgo3qsag5v4xiuv3bc45ja
938eb8a0-bad9-435a-a5b9-443cfbde0dfd
9e3520d8-acb1-4037-a2e9-fff7301b7655
938eb8a0-bad9-435a-a5b9-443cfbde0dfd


cawXom-mykxov-qempa2

glpat-srrSzoS7a5GfnTr-eQea

88e805C9d4dEdD4cc934A8B1AC05Ef52e452cB19AA60ccC0DAdE8AAB6C0b7CC0841E9D0

cawXommykxov$qempa2

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

miv3iexmlkiao5zra6avqsxpjjz3tqbgo3qsag5v4xiuv3bc45ja

938eb8a0-bad9-435a-a5b9-443cfbde0dfd

9e3520d8-acb1-4037-a2e9-fff7301b7655

938eb8a0-bad9-435a-a5b9-443cfbde0dfd

